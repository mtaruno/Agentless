agentless:
  .DS_Store: {}
fl:
  FL.py:
    classes:
    - end_line: 40
      methods:
      - end_line: 36
        name: __init__
        start_line: 33
        text:
        - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
        - '        self.structure = structure'
        - '        self.instance_id = instance_id'
        - '        self.problem_statement = problem_statement'
      - end_line: 40
        name: localize
        start_line: 39
        text:
        - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list,
          any]:'
        - '        pass'
      name: FL
      start_line: 32
      text:
      - 'class FL(ABC):'
      - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
      - '        self.structure = structure'
      - '        self.instance_id = instance_id'
      - '        self.problem_statement = problem_statement'
      - ''
      - '    @abstractmethod'
      - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
      - '        pass'
    - end_line: 618
      methods:
      - end_line: 229
        name: __init__
        start_line: 227
        text:
        - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
        - '        super().__init__(instance_id, structure, problem_statement)'
        - '        self.max_tokens = 300'
      - end_line: 232
        name: _parse_model_return_lines
        start_line: 231
        text:
        - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
        - '        return content.strip().split("\n")'
      - end_line: 315
        name: localize
        start_line: 234
        text:
        - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list,
          any]:'
        - ''
        - '        found_files = []'
        - ''
        - '        # lazy import, not sure if this is actually better?'
        - '        from agentless.util.api_requests import ('
        - '            create_chatgpt_config,'
        - '            num_tokens_from_messages,'
        - '            request_chatgpt_engine,'
        - '        )'
        - ''
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            request_codegeex_engine'
        - '        )'
        - ''
        - '        message = self.obtain_relevant_files_prompt.format('
        - '            problem_statement=self.problem_statement,'
        - '            structure=show_project_structure(self.structure).strip(),'
        - '        ).strip()'
        - '        print(f"prompting with message:\n{message}")'
        - '        print("=" * 80)'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "codegeex-4"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        # config = create_chatgpt_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=0,'
        - '        #     batch_size=1,'
        - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        # )'
        - ''
        - ''
        - '        # ret = request_chatgpt_engine(config)'
        - ''
        - '        config = create_codegeex_config('
        - '        message=message,'
        - '        max_tokens=self.max_tokens,'
        - '        temperature=0,'
        - '        )'
        - ''
        - '        ret = request_codegeex_engine(config)'
        - '        raw_output = extract_code(ret.choices[0].message.content)'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - '        model_found_files = self._parse_model_return_lines(raw_output)'
        - ''
        - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
        - '            self.structure'
        - '        )'
        - ''
        - '        for file_content in files:'
        - '            file = file_content[0]'
        - '            if file in model_found_files:'
        - '                found_files.append(file)'
        - ''
        - '        # sort based on order of appearance in model_found_files'
        - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
        - ''
        - '        print(raw_output)'
        - ''
        - '        return ('
        - '            found_files,'
        - '            {"raw_output_files": raw_output},'
        - '            traj,'
        - '        )'
      - end_line: 410
        name: localize_function_for_files
        start_line: 317
        text:
        - '    def localize_function_for_files('
        - '        self, file_names, mock=False'
        - '    ) -> tuple[list, dict, dict]:'
        - '        # from agentless.util.api_requests import ('
        - '        #     create_chatgpt_config,'
        - '        #     num_tokens_from_messages,'
        - '        #     request_chatgpt_engine,'
        - '        # )'
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            num_tokens_from_messages,'
        - '            request_codegeex_engine'
        - '        )'
        - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
        - '            self.structure'
        - '        )'
        - ''
        - '        max_num_files = len(file_names)'
        - '        while 1:'
        - '            # added small fix to prevent too many tokens'
        - '            contents = []'
        - '            for file_name in file_names[:max_num_files]:'
        - '                for file_content in files:'
        - '                    if file_content[0] == file_name:'
        - '                        content = "\n".join(file_content[1])'
        - '                        file_content = line_wrap_content(content)'
        - '                        contents.append('
        - '                            self.file_content_template.format('
        - '                                file_name=file_name, file_content=file_content'
        - '                            )'
        - '                        )'
        - '                        break'
        - '                else:'
        - '                    raise ValueError(f"File {file_name} does not exist.")'
        - ''
        - '            file_contents = "".join(contents)'
        - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
          < 128000:'
        - '                break'
        - '            else:'
        - '                max_num_files -= 1'
        - ''
        - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
        - '            problem_statement=self.problem_statement,'
        - '            file_contents=file_contents,'
        - '        ).strip()'
        - '        print(f"prompting with message:\n{message}")'
        - '        print("=" * 80)'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                # "usage": {'
        - '                #     "prompt_tokens": num_tokens_from_messages('
        - '                #         message, "gpt-4o-2024-05-13"'
        - '                #     ),'
        - '                # },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        # config = create_chatgpt_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=0,'
        - '        #     batch_size=1,'
        - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        # )'
        - ''
        - '        config = create_codegeex_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=0,'
        - '        )'
        - '        '
        - '        # ret = request_chatgpt_engine(config)'
        - '        ret = request_codegeex_engine(config)'
        - ''
        - '        raw_output = extract_code(ret.choices[0].message.content)'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - ''
        - '        model_found_locs = extract_code_blocks(raw_output)'
        - '        model_found_locs_separated = extract_locs_for_files('
        - '            model_found_locs, file_names'
        - '        )'
        - ''
        - '        print(raw_output)'
        - ''
        - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
          traj'
      - end_line: 504
        name: localize_function_from_compressed_files
        start_line: 412
        text:
        - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
        - '        # from agentless.util.api_requests import ('
        - '        #     create_chatgpt_config,'
        - '        #     num_tokens_from_messages,'
        - '        #     request_chatgpt_engine,'
        - '        # )'
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            num_tokens_from_messages,'
        - '            request_codegeex_engine'
        - '        )'
        - ''
        - '        file_contents = get_repo_files(self.structure, file_names)'
        - '        compressed_file_contents = {'
        - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
        - '        }'
        - ''
        - '        with open("localize_related.txt", "a") as f: '
        - '            f.write("Compressed file contents:\n")'
        - '            f.write(f"{compressed_file_contents}" + "\n"*15) # compressed
          file contents are the skeleton'
        - ''
        - '        contents = ['
        - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
        - '            for fn, code in compressed_file_contents.items()'
        - '        ]'
        - '        file_contents = "".join(contents)'
        - '        template = ('
        - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
        - '        )'
        - '        message = template.format('
        - '            problem_statement=self.problem_statement, file_contents=file_contents'
        - '        )'
        - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
        - '        logging.info(f"prompting with message:\n{message}")'
        - '        logging.info("=" * 80)'
        - ''
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "codegeex-4"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        # config = create_chatgpt_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=0,'
        - '        #     batch_size=1,'
        - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        # )'
        - '        # ret = request_chatgpt_engine(config)'
        - '        # raw_output = ret.choices[0].message.content'
        - ''
        - '        config = create_codegeex_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=0,'
        - '        )'
        - '        ret = request_codegeex_engine(config)'
        - '        raw_output = extract_code(ret.choices[0].message.content)'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - ''
        - '        model_found_locs = extract_code_blocks(raw_output)'
        - ''
        - '        with open("localize_related.txt", "a") as f:'
        - '            f.write("Model found locs\n")'
        - '            f.write(f"{model_found_locs}" + "\n"*15)'
        - ''
        - '        model_found_locs_separated = extract_locs_for_files('
        - '            model_found_locs, file_names'
        - '        )'
        - ''
        - '        logging.info(f"==== raw output ====")'
        - '        logging.info(raw_output)'
        - '        logging.info("=" * 80)'
        - '        logging.info(f"==== extracted locs ====")'
        - '        for loc in model_found_locs_separated:'
        - '            logging.info(loc)'
        - '        logging.info("=" * 80)'
        - ''
        - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
          traj'
      - end_line: 618
        name: localize_line_from_coarse_function_locs
        start_line: 506
        text:
        - '    def localize_line_from_coarse_function_locs('
        - '        self,'
        - '        file_names,'
        - '        coarse_locs,'
        - '        context_window: int,'
        - '        add_space: bool,'
        - '        sticky_scroll: bool,'
        - '        no_line_number: bool,'
        - '        temperature: float = 0.0,'
        - '        num_samples: int = 1,'
        - '        mock=False,'
        - '    ):'
        - '        from agentless.util.api_requests import ('
        - '            create_chatgpt_config,'
        - '            num_tokens_from_messages,'
        - '            request_chatgpt_engine,'
        - '        )'
        - ''
        - '        file_contents = get_repo_files(self.structure, file_names)'
        - '        topn_content, file_loc_intervals = construct_topn_file_context('
        - '            coarse_locs,'
        - '            file_names,'
        - '            file_contents,'
        - '            self.structure,'
        - '            context_window=context_window,'
        - '            loc_interval=True,'
        - '            add_space=add_space,'
        - '            sticky_scroll=sticky_scroll,'
        - '            no_line_number=no_line_number,'
        - '        )'
        - '        if no_line_number:'
        - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
        - '        else:'
        - '            template = self.obtain_relevant_code_combine_top_n_prompt'
        - '        message = template.format('
        - '            problem_statement=self.problem_statement, file_contents=topn_content'
        - '        )'
        - '        logging.info(f"prompting with message:\n{message}")'
        - '        logging.info("=" * 80)'
        - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "codegeex-4"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - '        # config = create_chatgpt_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=temperature,'
        - '        #     batch_size=num_samples,'
        - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        # )'
        - '        config = create_codegeex_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=temperature,'
        - '            model="codegeex-4",'
        - '        )'
        - ''
        - '        # ret = request_chatgpt_engine(config)'
        - '        ret = request_codegeex_engine(config)'
        - ''
        - '        raw_outputs = [extract_code(choice.message.content) for choice
          in ret.choices]'
        - ''
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_outputs,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - '        model_found_locs_separated_in_samples = []'
        - '        for raw_output in raw_outputs:'
        - '            model_found_locs = extract_code_blocks(raw_output)'
        - '            model_found_locs_separated = extract_locs_for_files('
        - '                model_found_locs, file_names'
        - '            )'
        - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
        - ''
        - '            logging.info(f"==== raw output ====")'
        - '            logging.info(raw_output)'
        - '            logging.info("=" * 80)'
        - '            print(raw_output)'
        - '            print("=" * 80)'
        - '            logging.info(f"==== extracted locs ====")'
        - '            for loc in model_found_locs_separated:'
        - '                logging.info(loc)'
        - '            logging.info("=" * 80)'
        - '        logging.info("==== Input coarse_locs")'
        - '        coarse_info = ""'
        - '        for fn, found_locs in coarse_locs.items():'
        - '            coarse_info += f"### {fn}\n"'
        - '            if isinstance(found_locs, str):'
        - '                coarse_info += found_locs + "\n"'
        - '            else:'
        - '                coarse_info += "\n".join(found_locs) + "\n"'
        - '        logging.info("\n" + coarse_info)'
        - '        if len(model_found_locs_separated_in_samples) == 1:'
        - '            model_found_locs_separated_in_samples = ('
        - '                model_found_locs_separated_in_samples[0]'
        - '            )'
        - ''
        - '        return ('
        - '            model_found_locs_separated_in_samples,'
        - '            {"raw_output_loc": raw_outputs},'
        - '            traj,'
        - '        )'
      name: LLMFL
      start_line: 43
      text:
      - 'class LLMFL(FL):'
      - '    obtain_relevant_files_prompt = """'
      - Please look through the following GitHub problem description and Repository
        structure and provide a list of files that one would need to edit to fix the
        problem.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - ''
      - '### Repository Structure ###'
      - '{structure}'
      - ''
      - '###'
      - ''
      - 'Task:'
      - Please only provide the full path and return at most 5 files. The returned
        files should be separated by new lines ordered by most to least important
        and wrapped with ```
      - 'Pay attention that the response should be on the same format as the following
        example:'
      - '```'
      - file1.py
      - file2.py
      - '```'
      - ''
      - Return the files single response. Do not return any other information.
      - '"""'
      - ''
      - '    obtain_relevant_code_prompt = """'
      - Please look through the following GitHub problem description and file and
        provide a set of locations that one would need to edit to fix the problem.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - ''
      - '###PATH:{file_name}'
      - '{file_content}'
      - ''
      - '###'
      - 'Task:'
      - 'Please provide either the class, the function name or line numbers that need
        to be edited. Note that if you include a class, you do NOT need to list its
        specific methods. You can include either the entire class or don''t include
        the class name and instead include specific methods in the class. Pay attention
        that the response should be on the same format as the following examples:'
      - ''
      - '### Example 1:'
      - '```'
      - 'class: MyClass'
      - '```'
      - '### Example 2:'
      - '```'
      - 'function: my_function'
      - '```'
      - '### Example 3:'
      - '```'
      - 'line: 10'
      - 'line: 24'
      - '```'
      - Return just the location(s) in a single response if need. Do not return any
        other information.
      - ''
      - '"""'
      - '    file_content_template = """'
      - '###PATH:{file_name}'
      - '{file_content}'
      - '"""'
      - '    file_content_in_block_template = """'
      - '###PATH: {file_name}'
      - '{file_content}'
      - '"""'
      - '    obtain_relevant_code_combine_top_n_prompt = """'
      - Please review the following GitHub problem description and relevant files,
        and provide a set of locations that need to be edited to fix the issue.
      - The locations can be specified as class names, function or method names, or
        exact line numbers that require modification.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - 'Task:'
      - 'Please provide the complete set of locations as either a class name, a function
        name, or line. Note that you need to start with the full path to the file
        when you want to include the line and class. Note that if you include a class,
        you do NOT need to list its specific methods. You can include either the entire
        class or don''t include the class name and instead include specific methods
        in the class. Pay attention that the response should be on the same format
        as the following example:'
      - '```'
      - 'path: path1/file1.py'
      - 'line: 10'
      - 'class: MyClass1'
      - 'line: 51'
      - ''
      - 'path: path2/file2.py'
      - 'function: MyClass2.my_method'
      - 'line: 12'
      - ''
      - 'path: path3/file3.py'
      - 'function: my_function'
      - 'line: 24'
      - 'line: 156'
      - '```'
      - Return just the location(s) in a single response if need. Do not return any
        other information.
      - '"""'
      - '    obtain_relevant_code_combine_top_n_no_line_number_prompt = """'
      - Please review the following GitHub problem description and relevant files,
        and provide a set of locations that need to be edited to fix the issue.
      - The locations can be specified as class, method, or function names that require
        modification.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - 'Task:'
      - 'Please provide the complete set of locations as either a class name, a function
        name, or a variable name. Note that for each function or class listed, it
        needs to be under a file (give the full path to the file ). Note that if you
        include a class, you do NOT need to list its specific methods. You can include
        either the entire class or don''t include the class name and instead include
        specific methods in the class. Pay attention that the response should be on
        the same format as the following example:'
      - '```'
      - 'path: path1/file1.py'
      - 'function: my_function1'
      - 'class: MyClass1'
      - ''
      - 'path: path2/file2.py'
      - 'function: MyClass2.my_method'
      - 'class: MyClass3'
      - ''
      - 'path: path3/file3.py'
      - 'function: my_function2'
      - '```'
      - ''
      - Return multiple files and locations in a single response if need. Do not return
        any other information.
      - '"""'
      - '    obtain_relevant_functions_from_compressed_files_prompt = """'
      - Please look through the following GitHub problem description and the skeleton
        of relevant files.
      - Provide a thorough set of locations that need inspection or editing to fix
        the problem, including directly related areas as well as any potentially related
        functions and classes.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - 'Task:'
      - 'Please provide the complete set of locations as either a class name, a function
        name, or a variable name. Note that for each function or class listed, it
        needs to be under a file (give the full path to the file ). Note that if you
        include a class, you do NOT need to list its specific methods. You can include
        either the entire class or don''t include the class name and instead include
        specific methods in the class. Pay attention that the response should be on
        the same format as the following example:'
      - '```'
      - 'path: path1/file1.py'
      - 'class: MyClass1'
      - ''
      - 'path: path2/file2.py'
      - 'function: MyClass2.my_method'
      - ''
      - 'path: path3/file3.py'
      - 'function: my_function'
      - '```'
      - ''
      - Return multiple files in a single response if need. Do not return any other
        information.
      - '"""'
      - '    obtain_relevant_functions_and_vars_from_compressed_files_prompt_more
        = """'
      - Please look through the following GitHub Problem Description and the Skeleton
        of Relevant Files.
      - Identify all locations that need inspection or editing to fix the problem,
        including directly related areas as well as any potentially related global
        variables, functions, and classes.
      - For each location you provide, either give the name of the class, the name
        of a method in a class, the name of a function, or the name of a global variable.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '### Skeleton of Relevant Files ###'
      - '{file_contents}'
      - ''
      - '###'
      - 'Task:'
      - 'Please provide the complete set of locations as either a class name, a function
        name, or a variable name. Note that if you include a class, you do NOT need
        to list its specific methods. You can include either the entire class or don''t
        include the class name and instead include specific methods in the class.
        Pay attention that the response should be on the same format as the following
        example:'
      - '```'
      - 'path: path1/file1.py'
      - 'function: my_function_1'
      - 'class: MyClass1'
      - 'function: MyClass2.my_method'
      - ''
      - 'path: path2/file2.py'
      - 'variable: my_var'
      - 'function: MyClass3.my_method'
      - ''
      - 'path: path3/file3.py'
      - 'function: my_function_2'
      - 'function: my_function_3'
      - 'function: MyClass4.my_method_1'
      - 'class: MyClass5'
      - '```'
      - Return multiple files and locations in a single response if need. Do not return
        any other information.
      - '"""'
      - ''
      - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
      - '        super().__init__(instance_id, structure, problem_statement)'
      - '        self.max_tokens = 300'
      - ''
      - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
      - '        return content.strip().split("\n")'
      - ''
      - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
      - ''
      - '        found_files = []'
      - ''
      - '        # lazy import, not sure if this is actually better?'
      - '        from agentless.util.api_requests import ('
      - '            create_chatgpt_config,'
      - '            num_tokens_from_messages,'
      - '            request_chatgpt_engine,'
      - '        )'
      - ''
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            request_codegeex_engine'
      - '        )'
      - ''
      - '        message = self.obtain_relevant_files_prompt.format('
      - '            problem_statement=self.problem_statement,'
      - '            structure=show_project_structure(self.structure).strip(),'
      - '        ).strip()'
      - '        print(f"prompting with message:\n{message}")'
      - '        print("=" * 80)'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "codegeex-4"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        # config = create_chatgpt_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=0,'
      - '        #     batch_size=1,'
      - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        # )'
      - ''
      - ''
      - '        # ret = request_chatgpt_engine(config)'
      - ''
      - '        config = create_codegeex_config('
      - '        message=message,'
      - '        max_tokens=self.max_tokens,'
      - '        temperature=0,'
      - '        )'
      - ''
      - '        ret = request_codegeex_engine(config)'
      - '        raw_output = extract_code(ret.choices[0].message.content)'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - '        model_found_files = self._parse_model_return_lines(raw_output)'
      - ''
      - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
      - '            self.structure'
      - '        )'
      - ''
      - '        for file_content in files:'
      - '            file = file_content[0]'
      - '            if file in model_found_files:'
      - '                found_files.append(file)'
      - ''
      - '        # sort based on order of appearance in model_found_files'
      - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
      - ''
      - '        print(raw_output)'
      - ''
      - '        return ('
      - '            found_files,'
      - '            {"raw_output_files": raw_output},'
      - '            traj,'
      - '        )'
      - ''
      - '    def localize_function_for_files('
      - '        self, file_names, mock=False'
      - '    ) -> tuple[list, dict, dict]:'
      - '        # from agentless.util.api_requests import ('
      - '        #     create_chatgpt_config,'
      - '        #     num_tokens_from_messages,'
      - '        #     request_chatgpt_engine,'
      - '        # )'
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            num_tokens_from_messages,'
      - '            request_codegeex_engine'
      - '        )'
      - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
      - '            self.structure'
      - '        )'
      - ''
      - '        max_num_files = len(file_names)'
      - '        while 1:'
      - '            # added small fix to prevent too many tokens'
      - '            contents = []'
      - '            for file_name in file_names[:max_num_files]:'
      - '                for file_content in files:'
      - '                    if file_content[0] == file_name:'
      - '                        content = "\n".join(file_content[1])'
      - '                        file_content = line_wrap_content(content)'
      - '                        contents.append('
      - '                            self.file_content_template.format('
      - '                                file_name=file_name, file_content=file_content'
      - '                            )'
      - '                        )'
      - '                        break'
      - '                else:'
      - '                    raise ValueError(f"File {file_name} does not exist.")'
      - ''
      - '            file_contents = "".join(contents)'
      - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
        < 128000:'
      - '                break'
      - '            else:'
      - '                max_num_files -= 1'
      - ''
      - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
      - '            problem_statement=self.problem_statement,'
      - '            file_contents=file_contents,'
      - '        ).strip()'
      - '        print(f"prompting with message:\n{message}")'
      - '        print("=" * 80)'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                # "usage": {'
      - '                #     "prompt_tokens": num_tokens_from_messages('
      - '                #         message, "gpt-4o-2024-05-13"'
      - '                #     ),'
      - '                # },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        # config = create_chatgpt_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=0,'
      - '        #     batch_size=1,'
      - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        # )'
      - ''
      - '        config = create_codegeex_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=0,'
      - '        )'
      - '        '
      - '        # ret = request_chatgpt_engine(config)'
      - '        ret = request_codegeex_engine(config)'
      - ''
      - '        raw_output = extract_code(ret.choices[0].message.content)'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - ''
      - '        model_found_locs = extract_code_blocks(raw_output)'
      - '        model_found_locs_separated = extract_locs_for_files('
      - '            model_found_locs, file_names'
      - '        )'
      - ''
      - '        print(raw_output)'
      - ''
      - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
        traj'
      - ''
      - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
      - '        # from agentless.util.api_requests import ('
      - '        #     create_chatgpt_config,'
      - '        #     num_tokens_from_messages,'
      - '        #     request_chatgpt_engine,'
      - '        # )'
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            num_tokens_from_messages,'
      - '            request_codegeex_engine'
      - '        )'
      - ''
      - '        file_contents = get_repo_files(self.structure, file_names)'
      - '        compressed_file_contents = {'
      - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
      - '        }'
      - ''
      - '        with open("localize_related.txt", "a") as f: '
      - '            f.write("Compressed file contents:\n")'
      - '            f.write(f"{compressed_file_contents}" + "\n"*15) # compressed
        file contents are the skeleton'
      - ''
      - '        contents = ['
      - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
      - '            for fn, code in compressed_file_contents.items()'
      - '        ]'
      - '        file_contents = "".join(contents)'
      - '        template = ('
      - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
      - '        )'
      - '        message = template.format('
      - '            problem_statement=self.problem_statement, file_contents=file_contents'
      - '        )'
      - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
      - '        logging.info(f"prompting with message:\n{message}")'
      - '        logging.info("=" * 80)'
      - ''
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "codegeex-4"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        # config = create_chatgpt_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=0,'
      - '        #     batch_size=1,'
      - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        # )'
      - '        # ret = request_chatgpt_engine(config)'
      - '        # raw_output = ret.choices[0].message.content'
      - ''
      - '        config = create_codegeex_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=0,'
      - '        )'
      - '        ret = request_codegeex_engine(config)'
      - '        raw_output = extract_code(ret.choices[0].message.content)'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - ''
      - '        model_found_locs = extract_code_blocks(raw_output)'
      - ''
      - '        with open("localize_related.txt", "a") as f:'
      - '            f.write("Model found locs\n")'
      - '            f.write(f"{model_found_locs}" + "\n"*15)'
      - ''
      - '        model_found_locs_separated = extract_locs_for_files('
      - '            model_found_locs, file_names'
      - '        )'
      - ''
      - '        logging.info(f"==== raw output ====")'
      - '        logging.info(raw_output)'
      - '        logging.info("=" * 80)'
      - '        logging.info(f"==== extracted locs ====")'
      - '        for loc in model_found_locs_separated:'
      - '            logging.info(loc)'
      - '        logging.info("=" * 80)'
      - ''
      - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
        traj'
      - ''
      - '    def localize_line_from_coarse_function_locs('
      - '        self,'
      - '        file_names,'
      - '        coarse_locs,'
      - '        context_window: int,'
      - '        add_space: bool,'
      - '        sticky_scroll: bool,'
      - '        no_line_number: bool,'
      - '        temperature: float = 0.0,'
      - '        num_samples: int = 1,'
      - '        mock=False,'
      - '    ):'
      - '        from agentless.util.api_requests import ('
      - '            create_chatgpt_config,'
      - '            num_tokens_from_messages,'
      - '            request_chatgpt_engine,'
      - '        )'
      - ''
      - '        file_contents = get_repo_files(self.structure, file_names)'
      - '        topn_content, file_loc_intervals = construct_topn_file_context('
      - '            coarse_locs,'
      - '            file_names,'
      - '            file_contents,'
      - '            self.structure,'
      - '            context_window=context_window,'
      - '            loc_interval=True,'
      - '            add_space=add_space,'
      - '            sticky_scroll=sticky_scroll,'
      - '            no_line_number=no_line_number,'
      - '        )'
      - '        if no_line_number:'
      - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
      - '        else:'
      - '            template = self.obtain_relevant_code_combine_top_n_prompt'
      - '        message = template.format('
      - '            problem_statement=self.problem_statement, file_contents=topn_content'
      - '        )'
      - '        logging.info(f"prompting with message:\n{message}")'
      - '        logging.info("=" * 80)'
      - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "codegeex-4"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - '        # config = create_chatgpt_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=temperature,'
      - '        #     batch_size=num_samples,'
      - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        # )'
      - '        config = create_codegeex_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=temperature,'
      - '            model="codegeex-4",'
      - '        )'
      - ''
      - '        # ret = request_chatgpt_engine(config)'
      - '        ret = request_codegeex_engine(config)'
      - ''
      - '        raw_outputs = [extract_code(choice.message.content) for choice in
        ret.choices]'
      - ''
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_outputs,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - '        model_found_locs_separated_in_samples = []'
      - '        for raw_output in raw_outputs:'
      - '            model_found_locs = extract_code_blocks(raw_output)'
      - '            model_found_locs_separated = extract_locs_for_files('
      - '                model_found_locs, file_names'
      - '            )'
      - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
      - ''
      - '            logging.info(f"==== raw output ====")'
      - '            logging.info(raw_output)'
      - '            logging.info("=" * 80)'
      - '            print(raw_output)'
      - '            print("=" * 80)'
      - '            logging.info(f"==== extracted locs ====")'
      - '            for loc in model_found_locs_separated:'
      - '                logging.info(loc)'
      - '            logging.info("=" * 80)'
      - '        logging.info("==== Input coarse_locs")'
      - '        coarse_info = ""'
      - '        for fn, found_locs in coarse_locs.items():'
      - '            coarse_info += f"### {fn}\n"'
      - '            if isinstance(found_locs, str):'
      - '                coarse_info += found_locs + "\n"'
      - '            else:'
      - '                coarse_info += "\n".join(found_locs) + "\n"'
      - '        logging.info("\n" + coarse_info)'
      - '        if len(model_found_locs_separated_in_samples) == 1:'
      - '            model_found_locs_separated_in_samples = ('
      - '                model_found_locs_separated_in_samples[0]'
      - '            )'
      - ''
      - '        return ('
      - '            model_found_locs_separated_in_samples,'
      - '            {"raw_output_loc": raw_outputs},'
      - '            traj,'
      - '        )'
    functions:
    - end_line: 30
      name: extract_code
      start_line: 22
      text:
      - 'def extract_code(code):'
      - '    if "```" in code:'
      - '        pattern = r''```.*?\n(.*?)```'''
      - '        matches = re.findall(pattern, code, re.DOTALL)'
      - '        extracted_code = '''''
      - '        for match in matches:'
      - '            extracted_code += ''```\n'' + match + ''```\n'''
      - '        return extracted_code.strip()'
      - '    return code'
    text:
    - import logging
    - from abc import ABC, abstractmethod
    - ''
    - from agentless.repair.repair import construct_topn_file_context
    - from agentless.util.compress_file import get_skeleton
    - from agentless.util.postprocess_data import extract_code_blocks, extract_locs_for_files
    - from agentless.util.preprocess_data import (
    - '    get_full_file_paths_and_classes_and_functions,'
    - '    get_repo_files,'
    - '    line_wrap_content,'
    - '    show_project_structure,'
    - )
    - from agentless.util.api_requests import (
    - "\tcreate_codegeex_config,"
    - "\tnum_tokens_from_messages,"
    - "\trequest_codegeex_engine"
    - )
    - import re
    - ''
    - ''
    - ''
    - 'def extract_code(code):'
    - '    if "```" in code:'
    - '        pattern = r''```.*?\n(.*?)```'''
    - '        matches = re.findall(pattern, code, re.DOTALL)'
    - '        extracted_code = '''''
    - '        for match in matches:'
    - '            extracted_code += ''```\n'' + match + ''```\n'''
    - '        return extracted_code.strip()'
    - '    return code'
    - ''
    - 'class FL(ABC):'
    - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
    - '        self.structure = structure'
    - '        self.instance_id = instance_id'
    - '        self.problem_statement = problem_statement'
    - ''
    - '    @abstractmethod'
    - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
    - '        pass'
    - ''
    - ''
    - 'class LLMFL(FL):'
    - '    obtain_relevant_files_prompt = """'
    - Please look through the following GitHub problem description and Repository
      structure and provide a list of files that one would need to edit to fix the
      problem.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - ''
    - '### Repository Structure ###'
    - '{structure}'
    - ''
    - '###'
    - ''
    - 'Task:'
    - Please only provide the full path and return at most 5 files. The returned files
      should be separated by new lines ordered by most to least important and wrapped
      with ```
    - 'Pay attention that the response should be on the same format as the following
      example:'
    - '```'
    - file1.py
    - file2.py
    - '```'
    - ''
    - Return the files single response. Do not return any other information.
    - '"""'
    - ''
    - '    obtain_relevant_code_prompt = """'
    - Please look through the following GitHub problem description and file and provide
      a set of locations that one would need to edit to fix the problem.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - ''
    - '###PATH:{file_name}'
    - '{file_content}'
    - ''
    - '###'
    - 'Task:'
    - 'Please provide either the class, the function name or line numbers that need
      to be edited. Note that if you include a class, you do NOT need to list its
      specific methods. You can include either the entire class or don''t include
      the class name and instead include specific methods in the class. Pay attention
      that the response should be on the same format as the following examples:'
    - ''
    - '### Example 1:'
    - '```'
    - 'class: MyClass'
    - '```'
    - '### Example 2:'
    - '```'
    - 'function: my_function'
    - '```'
    - '### Example 3:'
    - '```'
    - 'line: 10'
    - 'line: 24'
    - '```'
    - Return just the location(s) in a single response if need. Do not return any
      other information.
    - ''
    - '"""'
    - '    file_content_template = """'
    - '###PATH:{file_name}'
    - '{file_content}'
    - '"""'
    - '    file_content_in_block_template = """'
    - '###PATH: {file_name}'
    - '{file_content}'
    - '"""'
    - '    obtain_relevant_code_combine_top_n_prompt = """'
    - Please review the following GitHub problem description and relevant files, and
      provide a set of locations that need to be edited to fix the issue.
    - The locations can be specified as class names, function or method names, or
      exact line numbers that require modification.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - 'Task:'
    - 'Please provide the complete set of locations as either a class name, a function
      name, or line. Note that you need to start with the full path to the file when
      you want to include the line and class. Note that if you include a class, you
      do NOT need to list its specific methods. You can include either the entire
      class or don''t include the class name and instead include specific methods
      in the class. Pay attention that the response should be on the same format as
      the following example:'
    - '```'
    - 'path: path1/file1.py'
    - 'line: 10'
    - 'class: MyClass1'
    - 'line: 51'
    - ''
    - 'path: path2/file2.py'
    - 'function: MyClass2.my_method'
    - 'line: 12'
    - ''
    - 'path: path3/file3.py'
    - 'function: my_function'
    - 'line: 24'
    - 'line: 156'
    - '```'
    - Return just the location(s) in a single response if need. Do not return any
      other information.
    - '"""'
    - '    obtain_relevant_code_combine_top_n_no_line_number_prompt = """'
    - Please review the following GitHub problem description and relevant files, and
      provide a set of locations that need to be edited to fix the issue.
    - The locations can be specified as class, method, or function names that require
      modification.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - 'Task:'
    - 'Please provide the complete set of locations as either a class name, a function
      name, or a variable name. Note that for each function or class listed, it needs
      to be under a file (give the full path to the file ). Note that if you include
      a class, you do NOT need to list its specific methods. You can include either
      the entire class or don''t include the class name and instead include specific
      methods in the class. Pay attention that the response should be on the same
      format as the following example:'
    - '```'
    - 'path: path1/file1.py'
    - 'function: my_function1'
    - 'class: MyClass1'
    - ''
    - 'path: path2/file2.py'
    - 'function: MyClass2.my_method'
    - 'class: MyClass3'
    - ''
    - 'path: path3/file3.py'
    - 'function: my_function2'
    - '```'
    - ''
    - Return multiple files and locations in a single response if need. Do not return
      any other information.
    - '"""'
    - '    obtain_relevant_functions_from_compressed_files_prompt = """'
    - Please look through the following GitHub problem description and the skeleton
      of relevant files.
    - Provide a thorough set of locations that need inspection or editing to fix the
      problem, including directly related areas as well as any potentially related
      functions and classes.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - 'Task:'
    - 'Please provide the complete set of locations as either a class name, a function
      name, or a variable name. Note that for each function or class listed, it needs
      to be under a file (give the full path to the file ). Note that if you include
      a class, you do NOT need to list its specific methods. You can include either
      the entire class or don''t include the class name and instead include specific
      methods in the class. Pay attention that the response should be on the same
      format as the following example:'
    - '```'
    - 'path: path1/file1.py'
    - 'class: MyClass1'
    - ''
    - 'path: path2/file2.py'
    - 'function: MyClass2.my_method'
    - ''
    - 'path: path3/file3.py'
    - 'function: my_function'
    - '```'
    - ''
    - Return multiple files in a single response if need. Do not return any other
      information.
    - '"""'
    - '    obtain_relevant_functions_and_vars_from_compressed_files_prompt_more =
      """'
    - Please look through the following GitHub Problem Description and the Skeleton
      of Relevant Files.
    - Identify all locations that need inspection or editing to fix the problem, including
      directly related areas as well as any potentially related global variables,
      functions, and classes.
    - For each location you provide, either give the name of the class, the name of
      a method in a class, the name of a function, or the name of a global variable.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '### Skeleton of Relevant Files ###'
    - '{file_contents}'
    - ''
    - '###'
    - 'Task:'
    - 'Please provide the complete set of locations as either a class name, a function
      name, or a variable name. Note that if you include a class, you do NOT need
      to list its specific methods. You can include either the entire class or don''t
      include the class name and instead include specific methods in the class. Pay
      attention that the response should be on the same format as the following example:'
    - '```'
    - 'path: path1/file1.py'
    - 'function: my_function_1'
    - 'class: MyClass1'
    - 'function: MyClass2.my_method'
    - ''
    - 'path: path2/file2.py'
    - 'variable: my_var'
    - 'function: MyClass3.my_method'
    - ''
    - 'path: path3/file3.py'
    - 'function: my_function_2'
    - 'function: my_function_3'
    - 'function: MyClass4.my_method_1'
    - 'class: MyClass5'
    - '```'
    - Return multiple files and locations in a single response if need. Do not return
      any other information.
    - '"""'
    - ''
    - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
    - '        super().__init__(instance_id, structure, problem_statement)'
    - '        self.max_tokens = 300'
    - ''
    - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
    - '        return content.strip().split("\n")'
    - ''
    - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
    - ''
    - '        found_files = []'
    - ''
    - '        # lazy import, not sure if this is actually better?'
    - '        from agentless.util.api_requests import ('
    - '            create_chatgpt_config,'
    - '            num_tokens_from_messages,'
    - '            request_chatgpt_engine,'
    - '        )'
    - ''
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            request_codegeex_engine'
    - '        )'
    - ''
    - '        message = self.obtain_relevant_files_prompt.format('
    - '            problem_statement=self.problem_statement,'
    - '            structure=show_project_structure(self.structure).strip(),'
    - '        ).strip()'
    - '        print(f"prompting with message:\n{message}")'
    - '        print("=" * 80)'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "codegeex-4"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        # config = create_chatgpt_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=0,'
    - '        #     batch_size=1,'
    - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        # )'
    - ''
    - ''
    - '        # ret = request_chatgpt_engine(config)'
    - ''
    - '        config = create_codegeex_config('
    - '        message=message,'
    - '        max_tokens=self.max_tokens,'
    - '        temperature=0,'
    - '        )'
    - ''
    - '        ret = request_codegeex_engine(config)'
    - '        raw_output = extract_code(ret.choices[0].message.content)'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - '        model_found_files = self._parse_model_return_lines(raw_output)'
    - ''
    - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
    - '            self.structure'
    - '        )'
    - ''
    - '        for file_content in files:'
    - '            file = file_content[0]'
    - '            if file in model_found_files:'
    - '                found_files.append(file)'
    - ''
    - '        # sort based on order of appearance in model_found_files'
    - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
    - ''
    - '        print(raw_output)'
    - ''
    - '        return ('
    - '            found_files,'
    - '            {"raw_output_files": raw_output},'
    - '            traj,'
    - '        )'
    - ''
    - '    def localize_function_for_files('
    - '        self, file_names, mock=False'
    - '    ) -> tuple[list, dict, dict]:'
    - '        # from agentless.util.api_requests import ('
    - '        #     create_chatgpt_config,'
    - '        #     num_tokens_from_messages,'
    - '        #     request_chatgpt_engine,'
    - '        # )'
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            num_tokens_from_messages,'
    - '            request_codegeex_engine'
    - '        )'
    - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
    - '            self.structure'
    - '        )'
    - ''
    - '        max_num_files = len(file_names)'
    - '        while 1:'
    - '            # added small fix to prevent too many tokens'
    - '            contents = []'
    - '            for file_name in file_names[:max_num_files]:'
    - '                for file_content in files:'
    - '                    if file_content[0] == file_name:'
    - '                        content = "\n".join(file_content[1])'
    - '                        file_content = line_wrap_content(content)'
    - '                        contents.append('
    - '                            self.file_content_template.format('
    - '                                file_name=file_name, file_content=file_content'
    - '                            )'
    - '                        )'
    - '                        break'
    - '                else:'
    - '                    raise ValueError(f"File {file_name} does not exist.")'
    - ''
    - '            file_contents = "".join(contents)'
    - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
      < 128000:'
    - '                break'
    - '            else:'
    - '                max_num_files -= 1'
    - ''
    - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
    - '            problem_statement=self.problem_statement,'
    - '            file_contents=file_contents,'
    - '        ).strip()'
    - '        print(f"prompting with message:\n{message}")'
    - '        print("=" * 80)'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                # "usage": {'
    - '                #     "prompt_tokens": num_tokens_from_messages('
    - '                #         message, "gpt-4o-2024-05-13"'
    - '                #     ),'
    - '                # },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        # config = create_chatgpt_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=0,'
    - '        #     batch_size=1,'
    - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        # )'
    - ''
    - '        config = create_codegeex_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=0,'
    - '        )'
    - '        '
    - '        # ret = request_chatgpt_engine(config)'
    - '        ret = request_codegeex_engine(config)'
    - ''
    - '        raw_output = extract_code(ret.choices[0].message.content)'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - ''
    - '        model_found_locs = extract_code_blocks(raw_output)'
    - '        model_found_locs_separated = extract_locs_for_files('
    - '            model_found_locs, file_names'
    - '        )'
    - ''
    - '        print(raw_output)'
    - ''
    - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
      traj'
    - ''
    - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
    - '        # from agentless.util.api_requests import ('
    - '        #     create_chatgpt_config,'
    - '        #     num_tokens_from_messages,'
    - '        #     request_chatgpt_engine,'
    - '        # )'
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            num_tokens_from_messages,'
    - '            request_codegeex_engine'
    - '        )'
    - ''
    - '        file_contents = get_repo_files(self.structure, file_names)'
    - '        compressed_file_contents = {'
    - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
    - '        }'
    - ''
    - '        with open("localize_related.txt", "a") as f: '
    - '            f.write("Compressed file contents:\n")'
    - '            f.write(f"{compressed_file_contents}" + "\n"*15) # compressed file
      contents are the skeleton'
    - ''
    - '        contents = ['
    - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
    - '            for fn, code in compressed_file_contents.items()'
    - '        ]'
    - '        file_contents = "".join(contents)'
    - '        template = ('
    - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
    - '        )'
    - '        message = template.format('
    - '            problem_statement=self.problem_statement, file_contents=file_contents'
    - '        )'
    - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
    - '        logging.info(f"prompting with message:\n{message}")'
    - '        logging.info("=" * 80)'
    - ''
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "codegeex-4"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        # config = create_chatgpt_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=0,'
    - '        #     batch_size=1,'
    - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        # )'
    - '        # ret = request_chatgpt_engine(config)'
    - '        # raw_output = ret.choices[0].message.content'
    - ''
    - '        config = create_codegeex_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=0,'
    - '        )'
    - '        ret = request_codegeex_engine(config)'
    - '        raw_output = extract_code(ret.choices[0].message.content)'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - ''
    - '        model_found_locs = extract_code_blocks(raw_output)'
    - ''
    - '        with open("localize_related.txt", "a") as f:'
    - '            f.write("Model found locs\n")'
    - '            f.write(f"{model_found_locs}" + "\n"*15)'
    - ''
    - '        model_found_locs_separated = extract_locs_for_files('
    - '            model_found_locs, file_names'
    - '        )'
    - ''
    - '        logging.info(f"==== raw output ====")'
    - '        logging.info(raw_output)'
    - '        logging.info("=" * 80)'
    - '        logging.info(f"==== extracted locs ====")'
    - '        for loc in model_found_locs_separated:'
    - '            logging.info(loc)'
    - '        logging.info("=" * 80)'
    - ''
    - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
      traj'
    - ''
    - '    def localize_line_from_coarse_function_locs('
    - '        self,'
    - '        file_names,'
    - '        coarse_locs,'
    - '        context_window: int,'
    - '        add_space: bool,'
    - '        sticky_scroll: bool,'
    - '        no_line_number: bool,'
    - '        temperature: float = 0.0,'
    - '        num_samples: int = 1,'
    - '        mock=False,'
    - '    ):'
    - '        from agentless.util.api_requests import ('
    - '            create_chatgpt_config,'
    - '            num_tokens_from_messages,'
    - '            request_chatgpt_engine,'
    - '        )'
    - ''
    - '        file_contents = get_repo_files(self.structure, file_names)'
    - '        topn_content, file_loc_intervals = construct_topn_file_context('
    - '            coarse_locs,'
    - '            file_names,'
    - '            file_contents,'
    - '            self.structure,'
    - '            context_window=context_window,'
    - '            loc_interval=True,'
    - '            add_space=add_space,'
    - '            sticky_scroll=sticky_scroll,'
    - '            no_line_number=no_line_number,'
    - '        )'
    - '        if no_line_number:'
    - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
    - '        else:'
    - '            template = self.obtain_relevant_code_combine_top_n_prompt'
    - '        message = template.format('
    - '            problem_statement=self.problem_statement, file_contents=topn_content'
    - '        )'
    - '        logging.info(f"prompting with message:\n{message}")'
    - '        logging.info("=" * 80)'
    - '        assert num_tokens_from_messages(message, "codegeex-4") < 128000'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "codegeex-4"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - '        # config = create_chatgpt_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=temperature,'
    - '        #     batch_size=num_samples,'
    - '        #     model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        # )'
    - '        config = create_codegeex_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=temperature,'
    - '            model="codegeex-4",'
    - '        )'
    - ''
    - '        # ret = request_chatgpt_engine(config)'
    - '        ret = request_codegeex_engine(config)'
    - ''
    - '        raw_outputs = [extract_code(choice.message.content) for choice in ret.choices]'
    - ''
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_outputs,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - '        model_found_locs_separated_in_samples = []'
    - '        for raw_output in raw_outputs:'
    - '            model_found_locs = extract_code_blocks(raw_output)'
    - '            model_found_locs_separated = extract_locs_for_files('
    - '                model_found_locs, file_names'
    - '            )'
    - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
    - ''
    - '            logging.info(f"==== raw output ====")'
    - '            logging.info(raw_output)'
    - '            logging.info("=" * 80)'
    - '            print(raw_output)'
    - '            print("=" * 80)'
    - '            logging.info(f"==== extracted locs ====")'
    - '            for loc in model_found_locs_separated:'
    - '                logging.info(loc)'
    - '            logging.info("=" * 80)'
    - '        logging.info("==== Input coarse_locs")'
    - '        coarse_info = ""'
    - '        for fn, found_locs in coarse_locs.items():'
    - '            coarse_info += f"### {fn}\n"'
    - '            if isinstance(found_locs, str):'
    - '                coarse_info += found_locs + "\n"'
    - '            else:'
    - '                coarse_info += "\n".join(found_locs) + "\n"'
    - '        logging.info("\n" + coarse_info)'
    - '        if len(model_found_locs_separated_in_samples) == 1:'
    - '            model_found_locs_separated_in_samples = ('
    - '                model_found_locs_separated_in_samples[0]'
    - '            )'
    - ''
    - '        return ('
    - '            model_found_locs_separated_in_samples,'
    - '            {"raw_output_loc": raw_outputs},'
    - '            traj,'
    - '        )'
  FL_gpt.py:
    classes:
    - end_line: 29
      methods:
      - end_line: 25
        name: __init__
        start_line: 22
        text:
        - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
        - '        self.structure = structure'
        - '        self.instance_id = instance_id'
        - '        self.problem_statement = problem_statement'
      - end_line: 29
        name: localize
        start_line: 28
        text:
        - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list,
          any]:'
        - '        pass'
      name: FL
      start_line: 21
      text:
      - 'class FL(ABC):'
      - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
      - '        self.structure = structure'
      - '        self.instance_id = instance_id'
      - '        self.problem_statement = problem_statement'
      - ''
      - '    @abstractmethod'
      - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
      - '        pass'
    - end_line: 600
      methods:
      - end_line: 222
        name: __init__
        start_line: 220
        text:
        - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
        - '        super().__init__(instance_id, structure, problem_statement)'
        - '        self.max_tokens = 300'
      - end_line: 225
        name: _parse_model_return_lines
        start_line: 224
        text:
        - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
        - '        return content.strip().split("\n")'
      - end_line: 308
        name: localize
        start_line: 227
        text:
        - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list,
          any]:'
        - ''
        - '        found_files = []'
        - ''
        - '        # lazy import, not sure if this is actually better?'
        - '        from agentless.util.api_requests import ('
        - '            create_chatgpt_config,'
        - '            num_tokens_from_messages,'
        - '            request_chatgpt_engine,'
        - '        )'
        - ''
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            request_codegeex_engine'
        - '        )'
        - ''
        - '        message = self.obtain_relevant_files_prompt.format('
        - '            problem_statement=self.problem_statement,'
        - '            structure=show_project_structure(self.structure).strip(),'
        - '        ).strip()'
        - '        print(f"prompting with message:\n{message}")'
        - '        print("=" * 80)'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "gpt-4o-2024-05-13"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        config = create_chatgpt_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=0,'
        - '            batch_size=1,'
        - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        )'
        - ''
        - ''
        - '        ret = request_chatgpt_engine(config)'
        - ''
        - '        # config = create_codegeex_config('
        - '        # message=message,'
        - '        # max_tokens=self.max_tokens,'
        - '        # temperature=0,'
        - '        # )'
        - ''
        - '        # ret = request_codegeex_engine(config)'
        - '        raw_output = ret.choices[0].message.content'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - '        model_found_files = self._parse_model_return_lines(raw_output)'
        - '        '
        - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
        - '            self.structure'
        - '        )'
        - ''
        - '        for file_content in files:'
        - '            file = file_content[0]'
        - '            if file in model_found_files:'
        - '                found_files.append(file)'
        - ''
        - '        # sort based on order of appearance in model_found_files'
        - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
        - ''
        - '        print(raw_output)'
        - ''
        - '        return ('
        - '            found_files,'
        - '            {"raw_output_files": raw_output},'
        - '            traj,'
        - '        )'
      - end_line: 403
        name: localize_function_for_files
        start_line: 310
        text:
        - '    def localize_function_for_files('
        - '        self, file_names, mock=False'
        - '    ) -> tuple[list, dict, dict]:'
        - '        # from agentless.util.api_requests import ('
        - '        #     create_chatgpt_config,'
        - '        #     num_tokens_from_messages,'
        - '        #     request_chatgpt_engine,'
        - '        # )'
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            num_tokens_from_messages,'
        - '            request_codegeex_engine'
        - '        )'
        - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
        - '            self.structure'
        - '        )'
        - ''
        - '        max_num_files = len(file_names)'
        - '        while 1:'
        - '            # added small fix to prevent too many tokens'
        - '            contents = []'
        - '            for file_name in file_names[:max_num_files]:'
        - '                for file_content in files:'
        - '                    if file_content[0] == file_name:'
        - '                        content = "\n".join(file_content[1])'
        - '                        file_content = line_wrap_content(content)'
        - '                        contents.append('
        - '                            self.file_content_template.format('
        - '                                file_name=file_name, file_content=file_content'
        - '                            )'
        - '                        )'
        - '                        break'
        - '                else:'
        - '                    raise ValueError(f"File {file_name} does not exist.")'
        - ''
        - '            file_contents = "".join(contents)'
        - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
          < 128000:'
        - '                break'
        - '            else:'
        - '                max_num_files -= 1'
        - ''
        - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
        - '            problem_statement=self.problem_statement,'
        - '            file_contents=file_contents,'
        - '        ).strip()'
        - '        print(f"prompting with message:\n{message}")'
        - '        print("=" * 80)'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                # "usage": {'
        - '                #     "prompt_tokens": num_tokens_from_messages('
        - '                #         message, "gpt-4o-2024-05-13"'
        - '                #     ),'
        - '                # },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        config = create_chatgpt_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=0,'
        - '            batch_size=1,'
        - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        )'
        - ''
        - '        # config = create_codegeex_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=0,'
        - '        # )'
        - '        '
        - '        ret = request_chatgpt_engine(config)'
        - '        # ret = request_codegeex_engine(config)'
        - ''
        - '        raw_output = ret.choices[0].message.content'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - ''
        - '        model_found_locs = extract_code_blocks(raw_output)'
        - '        model_found_locs_separated = extract_locs_for_files('
        - '            model_found_locs, file_names'
        - '        )'
        - ''
        - '        print(raw_output)'
        - ''
        - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
          traj'
      - end_line: 487
        name: localize_function_from_compressed_files
        start_line: 405
        text:
        - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
        - '        from agentless.util.api_requests import ('
        - '            create_chatgpt_config,'
        - '            num_tokens_from_messages,'
        - '            request_chatgpt_engine,'
        - '        )'
        - ''
        - '        from agentless.util.api_requests import ('
        - '            create_codegeex_config,'
        - '            num_tokens_from_messages,'
        - '            request_codegeex_engine'
        - '        )'
        - ''
        - '        file_contents = get_repo_files(self.structure, file_names)'
        - '        compressed_file_contents = {'
        - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
        - '        }'
        - '        contents = ['
        - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
        - '            for fn, code in compressed_file_contents.items()'
        - '        ]'
        - '        file_contents = "".join(contents)'
        - '        template = ('
        - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
        - '        )'
        - '        message = template.format('
        - '            problem_statement=self.problem_statement, file_contents=file_contents'
        - '        )'
        - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") <
          128000'
        - '        logging.info(f"prompting with message:\n{message}")'
        - '        logging.info("=" * 80)'
        - ''
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "gpt-4o-2024-05-13"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - ''
        - '        config = create_chatgpt_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=0,'
        - '            batch_size=1,'
        - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        )'
        - '        ret = request_chatgpt_engine(config)'
        - '        raw_output = ret.choices[0].message.content'
        - ''
        - '        # config = create_codegeex_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=0,'
        - '        # )'
        - '        # ret = request_codegeex_engine(config)'
        - '        # raw_output = ret.choices[0].message.content'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_output,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - ''
        - '        model_found_locs = extract_code_blocks(raw_output)'
        - '        model_found_locs_separated = extract_locs_for_files('
        - '            model_found_locs, file_names'
        - '        )'
        - ''
        - '        logging.info(f"==== raw output ====")'
        - '        logging.info(raw_output)'
        - '        logging.info("=" * 80)'
        - '        logging.info(f"==== extracted locs ====")'
        - '        for loc in model_found_locs_separated:'
        - '            logging.info(loc)'
        - '        logging.info("=" * 80)'
        - ''
        - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
          traj'
      - end_line: 600
        name: localize_line_from_coarse_function_locs
        start_line: 489
        text:
        - '    def localize_line_from_coarse_function_locs('
        - '        self,'
        - '        file_names,'
        - '        coarse_locs,'
        - '        context_window: int,'
        - '        add_space: bool,'
        - '        sticky_scroll: bool,'
        - '        no_line_number: bool,'
        - '        temperature: float = 0.0,'
        - '        num_samples: int = 1,'
        - '        mock=False,'
        - '    ):'
        - '        from agentless.util.api_requests import ('
        - '            create_chatgpt_config,'
        - '            num_tokens_from_messages,'
        - '            request_chatgpt_engine,'
        - '        )'
        - ''
        - '        file_contents = get_repo_files(self.structure, file_names)'
        - '        topn_content, file_loc_intervals = construct_topn_file_context('
        - '            coarse_locs,'
        - '            file_names,'
        - '            file_contents,'
        - '            self.structure,'
        - '            context_window=context_window,'
        - '            loc_interval=True,'
        - '            add_space=add_space,'
        - '            sticky_scroll=sticky_scroll,'
        - '            no_line_number=no_line_number,'
        - '        )'
        - '        if no_line_number:'
        - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
        - '        else:'
        - '            template = self.obtain_relevant_code_combine_top_n_prompt'
        - '        message = template.format('
        - '            problem_statement=self.problem_statement, file_contents=topn_content'
        - '        )'
        - '        logging.info(f"prompting with message:\n{message}")'
        - '        logging.info("=" * 80)'
        - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") <
          128000'
        - '        if mock:'
        - '            traj = {'
        - '                "prompt": message,'
        - '                "usage": {'
        - '                    "prompt_tokens": num_tokens_from_messages('
        - '                        message, "gpt-4o-2024-05-13"'
        - '                    ),'
        - '                },'
        - '            }'
        - '            return [], {"raw_output_loc": ""}, traj'
        - '        config = create_chatgpt_config('
        - '            message=message,'
        - '            max_tokens=self.max_tokens,'
        - '            temperature=temperature,'
        - '            batch_size=num_samples,'
        - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
        - '        )'
        - '        # config = create_codegeex_config('
        - '        #     message=message,'
        - '        #     max_tokens=self.max_tokens,'
        - '        #     temperature=temperature,'
        - '        #     model="codegeex-4",'
        - '        # )'
        - ''
        - '        ret = request_chatgpt_engine(config)'
        - '        # ret = request_codegeex_engine(config)'
        - ''
        - '        raw_outputs = [choice.message.content for choice in ret.choices]'
        - '        traj = {'
        - '            "prompt": message,'
        - '            "response": raw_outputs,'
        - '            "usage": {'
        - '                "prompt_tokens": ret.usage.prompt_tokens,'
        - '                "completion_tokens": ret.usage.completion_tokens,'
        - '            },'
        - '        }'
        - '        model_found_locs_separated_in_samples = []'
        - '        for raw_output in raw_outputs:'
        - '            model_found_locs = extract_code_blocks(raw_output)'
        - '            model_found_locs_separated = extract_locs_for_files('
        - '                model_found_locs, file_names'
        - '            )'
        - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
        - ''
        - '            logging.info(f"==== raw output ====")'
        - '            logging.info(raw_output)'
        - '            logging.info("=" * 80)'
        - '            print(raw_output)'
        - '            print("=" * 80)'
        - '            logging.info(f"==== extracted locs ====")'
        - '            for loc in model_found_locs_separated:'
        - '                logging.info(loc)'
        - '            logging.info("=" * 80)'
        - '        logging.info("==== Input coarse_locs")'
        - '        coarse_info = ""'
        - '        for fn, found_locs in coarse_locs.items():'
        - '            coarse_info += f"### {fn}\n"'
        - '            if isinstance(found_locs, str):'
        - '                coarse_info += found_locs + "\n"'
        - '            else:'
        - '                coarse_info += "\n".join(found_locs) + "\n"'
        - '        logging.info("\n" + coarse_info)'
        - '        if len(model_found_locs_separated_in_samples) == 1:'
        - '            model_found_locs_separated_in_samples = ('
        - '                model_found_locs_separated_in_samples[0]'
        - '            )'
        - ''
        - '        return ('
        - '            model_found_locs_separated_in_samples,'
        - '            {"raw_output_loc": raw_outputs},'
        - '            traj,'
        - '        )'
      name: LLMFL
      start_line: 32
      text:
      - 'class LLMFL(FL):'
      - '    obtain_relevant_files_prompt = """'
      - Please look through the following GitHub problem description and Repository
        structure and provide a list of files that one would need to edit to fix the
        problem.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - ''
      - '### Repository Structure ###'
      - '{structure}'
      - ''
      - '###'
      - ''
      - Please only provide the full path and return at most 5 files.
      - The returned files should be separated by new lines ordered by most to least
        important and wrapped with ```
      - 'For example:'
      - '```'
      - file1.py
      - file2.py
      - '```'
      - '"""'
      - ''
      - '    obtain_relevant_code_prompt = """'
      - Please look through the following GitHub problem description and file and
        provide a set of locations that one would need to edit to fix the problem.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - ''
      - '### File: {file_name} ###'
      - '{file_content}'
      - ''
      - '###'
      - ''
      - Please provide either the class, the function name or line numbers that need
        to be edited.
      - '### Example 1:'
      - '```'
      - 'class: MyClass'
      - '```'
      - '### Example 2:'
      - '```'
      - 'function: my_function'
      - '```'
      - '### Example 3:'
      - '```'
      - 'line: 10'
      - 'line: 24'
      - '```'
      - ''
      - Return just the location(s)
      - '"""'
      - '    file_content_template = """'
      - '### File: {file_name} ###'
      - '{file_content}'
      - '"""'
      - '    file_content_in_block_template = """'
      - '### File: {file_name} ###'
      - '```python'
      - '{file_content}'
      - '```'
      - '"""'
      - '    obtain_relevant_code_combine_top_n_prompt = """'
      - Please review the following GitHub problem description and relevant files,
        and provide a set of locations that need to be edited to fix the issue.
      - The locations can be specified as class names, function or method names, or
        exact line numbers that require modification.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - Please provide the class name, function or method name, or the exact line
        numbers that need to be edited.
      - '### Examples:'
      - '```'
      - full_path1/file1.py
      - 'line: 10'
      - 'class: MyClass1'
      - 'line: 51'
      - ''
      - full_path2/file2.py
      - 'function: MyClass2.my_method'
      - 'line: 12'
      - ''
      - full_path3/file3.py
      - 'function: my_function'
      - 'line: 24'
      - 'line: 156'
      - '```'
      - ''
      - Return just the location(s)
      - '"""'
      - '    obtain_relevant_code_combine_top_n_no_line_number_prompt = """'
      - Please review the following GitHub problem description and relevant files,
        and provide a set of locations that need to be edited to fix the issue.
      - The locations can be specified as class, method, or function names that require
        modification.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - Please provide the class, method, or function names that need to be edited.
      - '### Examples:'
      - '```'
      - full_path1/file1.py
      - 'function: my_function1'
      - 'class: MyClass1'
      - ''
      - full_path2/file2.py
      - 'function: MyClass2.my_method'
      - 'class: MyClass3'
      - ''
      - full_path3/file3.py
      - 'function: my_function2'
      - '```'
      - ''
      - Return just the location(s)
      - '"""'
      - '    obtain_relevant_functions_from_compressed_files_prompt = """'
      - Please look through the following GitHub problem description and the skeleton
        of relevant files.
      - Provide a thorough set of locations that need inspection or editing to fix
        the problem, including directly related areas as well as any potentially related
        functions and classes.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - Please provide locations as either the class or the function name.
      - '### Examples:'
      - '```'
      - full_path1/file1.py
      - 'class: MyClass1'
      - ''
      - full_path2/file2.py
      - 'function: MyClass2.my_method'
      - ''
      - full_path3/file3.py
      - 'function: my_function'
      - '```'
      - ''
      - Return just the location(s)
      - '"""'
      - '    obtain_relevant_functions_and_vars_from_compressed_files_prompt_more
        = """'
      - Please look through the following GitHub Problem Description and the Skeleton
        of Relevant Files.
      - Identify all locations that need inspection or editing to fix the problem,
        including directly related areas as well as any potentially related global
        variables, functions, and classes.
      - For each location you provide, either give the name of the class, the name
        of a method in a class, the name of a function, or the name of a global variable.
      - ''
      - '### GitHub Problem Description ###'
      - '{problem_statement}'
      - ''
      - '### Skeleton of Relevant Files ###'
      - '{file_contents}'
      - ''
      - '###'
      - ''
      - Please provide the complete set of locations as either a class name, a function
        name, or a variable name.
      - Note that if you include a class, you do not need to list its specific methods.
      - You can include either the entire class or don't include the class name and
        instead include specific methods in the class.
      - '### Examples:'
      - '```'
      - full_path1/file1.py
      - 'function: my_function_1'
      - 'class: MyClass1'
      - 'function: MyClass2.my_method'
      - ''
      - full_path2/file2.py
      - 'variable: my_var'
      - 'function: MyClass3.my_method'
      - ''
      - full_path3/file3.py
      - 'function: my_function_2'
      - 'function: my_function_3'
      - 'function: MyClass4.my_method_1'
      - 'class: MyClass5'
      - '```'
      - ''
      - Return just the locations.
      - '"""'
      - ''
      - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
      - '        super().__init__(instance_id, structure, problem_statement)'
      - '        self.max_tokens = 300'
      - ''
      - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
      - '        return content.strip().split("\n")'
      - ''
      - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
      - ''
      - '        found_files = []'
      - ''
      - '        # lazy import, not sure if this is actually better?'
      - '        from agentless.util.api_requests import ('
      - '            create_chatgpt_config,'
      - '            num_tokens_from_messages,'
      - '            request_chatgpt_engine,'
      - '        )'
      - ''
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            request_codegeex_engine'
      - '        )'
      - ''
      - '        message = self.obtain_relevant_files_prompt.format('
      - '            problem_statement=self.problem_statement,'
      - '            structure=show_project_structure(self.structure).strip(),'
      - '        ).strip()'
      - '        print(f"prompting with message:\n{message}")'
      - '        print("=" * 80)'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "gpt-4o-2024-05-13"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        config = create_chatgpt_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=0,'
      - '            batch_size=1,'
      - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        )'
      - ''
      - ''
      - '        ret = request_chatgpt_engine(config)'
      - ''
      - '        # config = create_codegeex_config('
      - '        # message=message,'
      - '        # max_tokens=self.max_tokens,'
      - '        # temperature=0,'
      - '        # )'
      - ''
      - '        # ret = request_codegeex_engine(config)'
      - '        raw_output = ret.choices[0].message.content'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - '        model_found_files = self._parse_model_return_lines(raw_output)'
      - '        '
      - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
      - '            self.structure'
      - '        )'
      - ''
      - '        for file_content in files:'
      - '            file = file_content[0]'
      - '            if file in model_found_files:'
      - '                found_files.append(file)'
      - ''
      - '        # sort based on order of appearance in model_found_files'
      - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
      - ''
      - '        print(raw_output)'
      - ''
      - '        return ('
      - '            found_files,'
      - '            {"raw_output_files": raw_output},'
      - '            traj,'
      - '        )'
      - ''
      - '    def localize_function_for_files('
      - '        self, file_names, mock=False'
      - '    ) -> tuple[list, dict, dict]:'
      - '        # from agentless.util.api_requests import ('
      - '        #     create_chatgpt_config,'
      - '        #     num_tokens_from_messages,'
      - '        #     request_chatgpt_engine,'
      - '        # )'
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            num_tokens_from_messages,'
      - '            request_codegeex_engine'
      - '        )'
      - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
      - '            self.structure'
      - '        )'
      - ''
      - '        max_num_files = len(file_names)'
      - '        while 1:'
      - '            # added small fix to prevent too many tokens'
      - '            contents = []'
      - '            for file_name in file_names[:max_num_files]:'
      - '                for file_content in files:'
      - '                    if file_content[0] == file_name:'
      - '                        content = "\n".join(file_content[1])'
      - '                        file_content = line_wrap_content(content)'
      - '                        contents.append('
      - '                            self.file_content_template.format('
      - '                                file_name=file_name, file_content=file_content'
      - '                            )'
      - '                        )'
      - '                        break'
      - '                else:'
      - '                    raise ValueError(f"File {file_name} does not exist.")'
      - ''
      - '            file_contents = "".join(contents)'
      - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
        < 128000:'
      - '                break'
      - '            else:'
      - '                max_num_files -= 1'
      - ''
      - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
      - '            problem_statement=self.problem_statement,'
      - '            file_contents=file_contents,'
      - '        ).strip()'
      - '        print(f"prompting with message:\n{message}")'
      - '        print("=" * 80)'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                # "usage": {'
      - '                #     "prompt_tokens": num_tokens_from_messages('
      - '                #         message, "gpt-4o-2024-05-13"'
      - '                #     ),'
      - '                # },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        config = create_chatgpt_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=0,'
      - '            batch_size=1,'
      - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        )'
      - ''
      - '        # config = create_codegeex_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=0,'
      - '        # )'
      - '        '
      - '        ret = request_chatgpt_engine(config)'
      - '        # ret = request_codegeex_engine(config)'
      - ''
      - '        raw_output = ret.choices[0].message.content'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - ''
      - '        model_found_locs = extract_code_blocks(raw_output)'
      - '        model_found_locs_separated = extract_locs_for_files('
      - '            model_found_locs, file_names'
      - '        )'
      - ''
      - '        print(raw_output)'
      - ''
      - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
        traj'
      - ''
      - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
      - '        from agentless.util.api_requests import ('
      - '            create_chatgpt_config,'
      - '            num_tokens_from_messages,'
      - '            request_chatgpt_engine,'
      - '        )'
      - ''
      - '        from agentless.util.api_requests import ('
      - '            create_codegeex_config,'
      - '            num_tokens_from_messages,'
      - '            request_codegeex_engine'
      - '        )'
      - ''
      - '        file_contents = get_repo_files(self.structure, file_names)'
      - '        compressed_file_contents = {'
      - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
      - '        }'
      - '        contents = ['
      - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
      - '            for fn, code in compressed_file_contents.items()'
      - '        ]'
      - '        file_contents = "".join(contents)'
      - '        template = ('
      - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
      - '        )'
      - '        message = template.format('
      - '            problem_statement=self.problem_statement, file_contents=file_contents'
      - '        )'
      - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") < 128000'
      - '        logging.info(f"prompting with message:\n{message}")'
      - '        logging.info("=" * 80)'
      - ''
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "gpt-4o-2024-05-13"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - ''
      - '        config = create_chatgpt_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=0,'
      - '            batch_size=1,'
      - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        )'
      - '        ret = request_chatgpt_engine(config)'
      - '        raw_output = ret.choices[0].message.content'
      - ''
      - '        # config = create_codegeex_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=0,'
      - '        # )'
      - '        # ret = request_codegeex_engine(config)'
      - '        # raw_output = ret.choices[0].message.content'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_output,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - ''
      - '        model_found_locs = extract_code_blocks(raw_output)'
      - '        model_found_locs_separated = extract_locs_for_files('
      - '            model_found_locs, file_names'
      - '        )'
      - ''
      - '        logging.info(f"==== raw output ====")'
      - '        logging.info(raw_output)'
      - '        logging.info("=" * 80)'
      - '        logging.info(f"==== extracted locs ====")'
      - '        for loc in model_found_locs_separated:'
      - '            logging.info(loc)'
      - '        logging.info("=" * 80)'
      - ''
      - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
        traj'
      - ''
      - '    def localize_line_from_coarse_function_locs('
      - '        self,'
      - '        file_names,'
      - '        coarse_locs,'
      - '        context_window: int,'
      - '        add_space: bool,'
      - '        sticky_scroll: bool,'
      - '        no_line_number: bool,'
      - '        temperature: float = 0.0,'
      - '        num_samples: int = 1,'
      - '        mock=False,'
      - '    ):'
      - '        from agentless.util.api_requests import ('
      - '            create_chatgpt_config,'
      - '            num_tokens_from_messages,'
      - '            request_chatgpt_engine,'
      - '        )'
      - ''
      - '        file_contents = get_repo_files(self.structure, file_names)'
      - '        topn_content, file_loc_intervals = construct_topn_file_context('
      - '            coarse_locs,'
      - '            file_names,'
      - '            file_contents,'
      - '            self.structure,'
      - '            context_window=context_window,'
      - '            loc_interval=True,'
      - '            add_space=add_space,'
      - '            sticky_scroll=sticky_scroll,'
      - '            no_line_number=no_line_number,'
      - '        )'
      - '        if no_line_number:'
      - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
      - '        else:'
      - '            template = self.obtain_relevant_code_combine_top_n_prompt'
      - '        message = template.format('
      - '            problem_statement=self.problem_statement, file_contents=topn_content'
      - '        )'
      - '        logging.info(f"prompting with message:\n{message}")'
      - '        logging.info("=" * 80)'
      - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") < 128000'
      - '        if mock:'
      - '            traj = {'
      - '                "prompt": message,'
      - '                "usage": {'
      - '                    "prompt_tokens": num_tokens_from_messages('
      - '                        message, "gpt-4o-2024-05-13"'
      - '                    ),'
      - '                },'
      - '            }'
      - '            return [], {"raw_output_loc": ""}, traj'
      - '        config = create_chatgpt_config('
      - '            message=message,'
      - '            max_tokens=self.max_tokens,'
      - '            temperature=temperature,'
      - '            batch_size=num_samples,'
      - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
      - '        )'
      - '        # config = create_codegeex_config('
      - '        #     message=message,'
      - '        #     max_tokens=self.max_tokens,'
      - '        #     temperature=temperature,'
      - '        #     model="codegeex-4",'
      - '        # )'
      - ''
      - '        ret = request_chatgpt_engine(config)'
      - '        # ret = request_codegeex_engine(config)'
      - ''
      - '        raw_outputs = [choice.message.content for choice in ret.choices]'
      - '        traj = {'
      - '            "prompt": message,'
      - '            "response": raw_outputs,'
      - '            "usage": {'
      - '                "prompt_tokens": ret.usage.prompt_tokens,'
      - '                "completion_tokens": ret.usage.completion_tokens,'
      - '            },'
      - '        }'
      - '        model_found_locs_separated_in_samples = []'
      - '        for raw_output in raw_outputs:'
      - '            model_found_locs = extract_code_blocks(raw_output)'
      - '            model_found_locs_separated = extract_locs_for_files('
      - '                model_found_locs, file_names'
      - '            )'
      - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
      - ''
      - '            logging.info(f"==== raw output ====")'
      - '            logging.info(raw_output)'
      - '            logging.info("=" * 80)'
      - '            print(raw_output)'
      - '            print("=" * 80)'
      - '            logging.info(f"==== extracted locs ====")'
      - '            for loc in model_found_locs_separated:'
      - '                logging.info(loc)'
      - '            logging.info("=" * 80)'
      - '        logging.info("==== Input coarse_locs")'
      - '        coarse_info = ""'
      - '        for fn, found_locs in coarse_locs.items():'
      - '            coarse_info += f"### {fn}\n"'
      - '            if isinstance(found_locs, str):'
      - '                coarse_info += found_locs + "\n"'
      - '            else:'
      - '                coarse_info += "\n".join(found_locs) + "\n"'
      - '        logging.info("\n" + coarse_info)'
      - '        if len(model_found_locs_separated_in_samples) == 1:'
      - '            model_found_locs_separated_in_samples = ('
      - '                model_found_locs_separated_in_samples[0]'
      - '            )'
      - ''
      - '        return ('
      - '            model_found_locs_separated_in_samples,'
      - '            {"raw_output_loc": raw_outputs},'
      - '            traj,'
      - '        )'
    functions: []
    text:
    - import logging
    - from abc import ABC, abstractmethod
    - ''
    - from agentless.repair.repair import construct_topn_file_context
    - from agentless.util.compress_file import get_skeleton
    - from agentless.util.postprocess_data import extract_code_blocks, extract_locs_for_files
    - from agentless.util.preprocess_data import (
    - '    get_full_file_paths_and_classes_and_functions,'
    - '    get_repo_files,'
    - '    line_wrap_content,'
    - '    show_project_structure,'
    - )
    - from agentless.util.api_requests import (
    - "\tcreate_codegeex_config,"
    - "\tnum_tokens_from_messages,"
    - "\trequest_codegeex_engine"
    - )
    - ''
    - ''
    - ''
    - 'class FL(ABC):'
    - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
    - '        self.structure = structure'
    - '        self.instance_id = instance_id'
    - '        self.problem_statement = problem_statement'
    - ''
    - '    @abstractmethod'
    - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
    - '        pass'
    - ''
    - ''
    - 'class LLMFL(FL):'
    - '    obtain_relevant_files_prompt = """'
    - Please look through the following GitHub problem description and Repository
      structure and provide a list of files that one would need to edit to fix the
      problem.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - ''
    - '### Repository Structure ###'
    - '{structure}'
    - ''
    - '###'
    - ''
    - Please only provide the full path and return at most 5 files.
    - The returned files should be separated by new lines ordered by most to least
      important and wrapped with ```
    - 'For example:'
    - '```'
    - file1.py
    - file2.py
    - '```'
    - '"""'
    - ''
    - '    obtain_relevant_code_prompt = """'
    - Please look through the following GitHub problem description and file and provide
      a set of locations that one would need to edit to fix the problem.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - ''
    - '### File: {file_name} ###'
    - '{file_content}'
    - ''
    - '###'
    - ''
    - Please provide either the class, the function name or line numbers that need
      to be edited.
    - '### Example 1:'
    - '```'
    - 'class: MyClass'
    - '```'
    - '### Example 2:'
    - '```'
    - 'function: my_function'
    - '```'
    - '### Example 3:'
    - '```'
    - 'line: 10'
    - 'line: 24'
    - '```'
    - ''
    - Return just the location(s)
    - '"""'
    - '    file_content_template = """'
    - '### File: {file_name} ###'
    - '{file_content}'
    - '"""'
    - '    file_content_in_block_template = """'
    - '### File: {file_name} ###'
    - '```python'
    - '{file_content}'
    - '```'
    - '"""'
    - '    obtain_relevant_code_combine_top_n_prompt = """'
    - Please review the following GitHub problem description and relevant files, and
      provide a set of locations that need to be edited to fix the issue.
    - The locations can be specified as class names, function or method names, or
      exact line numbers that require modification.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - Please provide the class name, function or method name, or the exact line numbers
      that need to be edited.
    - '### Examples:'
    - '```'
    - full_path1/file1.py
    - 'line: 10'
    - 'class: MyClass1'
    - 'line: 51'
    - ''
    - full_path2/file2.py
    - 'function: MyClass2.my_method'
    - 'line: 12'
    - ''
    - full_path3/file3.py
    - 'function: my_function'
    - 'line: 24'
    - 'line: 156'
    - '```'
    - ''
    - Return just the location(s)
    - '"""'
    - '    obtain_relevant_code_combine_top_n_no_line_number_prompt = """'
    - Please review the following GitHub problem description and relevant files, and
      provide a set of locations that need to be edited to fix the issue.
    - The locations can be specified as class, method, or function names that require
      modification.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - Please provide the class, method, or function names that need to be edited.
    - '### Examples:'
    - '```'
    - full_path1/file1.py
    - 'function: my_function1'
    - 'class: MyClass1'
    - ''
    - full_path2/file2.py
    - 'function: MyClass2.my_method'
    - 'class: MyClass3'
    - ''
    - full_path3/file3.py
    - 'function: my_function2'
    - '```'
    - ''
    - Return just the location(s)
    - '"""'
    - '    obtain_relevant_functions_from_compressed_files_prompt = """'
    - Please look through the following GitHub problem description and the skeleton
      of relevant files.
    - Provide a thorough set of locations that need inspection or editing to fix the
      problem, including directly related areas as well as any potentially related
      functions and classes.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - Please provide locations as either the class or the function name.
    - '### Examples:'
    - '```'
    - full_path1/file1.py
    - 'class: MyClass1'
    - ''
    - full_path2/file2.py
    - 'function: MyClass2.my_method'
    - ''
    - full_path3/file3.py
    - 'function: my_function'
    - '```'
    - ''
    - Return just the location(s)
    - '"""'
    - '    obtain_relevant_functions_and_vars_from_compressed_files_prompt_more =
      """'
    - Please look through the following GitHub Problem Description and the Skeleton
      of Relevant Files.
    - Identify all locations that need inspection or editing to fix the problem, including
      directly related areas as well as any potentially related global variables,
      functions, and classes.
    - For each location you provide, either give the name of the class, the name of
      a method in a class, the name of a function, or the name of a global variable.
    - ''
    - '### GitHub Problem Description ###'
    - '{problem_statement}'
    - ''
    - '### Skeleton of Relevant Files ###'
    - '{file_contents}'
    - ''
    - '###'
    - ''
    - Please provide the complete set of locations as either a class name, a function
      name, or a variable name.
    - Note that if you include a class, you do not need to list its specific methods.
    - You can include either the entire class or don't include the class name and
      instead include specific methods in the class.
    - '### Examples:'
    - '```'
    - full_path1/file1.py
    - 'function: my_function_1'
    - 'class: MyClass1'
    - 'function: MyClass2.my_method'
    - ''
    - full_path2/file2.py
    - 'variable: my_var'
    - 'function: MyClass3.my_method'
    - ''
    - full_path3/file3.py
    - 'function: my_function_2'
    - 'function: my_function_3'
    - 'function: MyClass4.my_method_1'
    - 'class: MyClass5'
    - '```'
    - ''
    - Return just the locations.
    - '"""'
    - ''
    - '    def __init__(self, instance_id, structure, problem_statement, **kwargs):'
    - '        super().__init__(instance_id, structure, problem_statement)'
    - '        self.max_tokens = 300'
    - ''
    - '    def _parse_model_return_lines(self, content: str) -> list[str]:'
    - '        return content.strip().split("\n")'
    - ''
    - '    def localize(self, top_n=1, mock=False) -> tuple[list, list, list, any]:'
    - ''
    - '        found_files = []'
    - ''
    - '        # lazy import, not sure if this is actually better?'
    - '        from agentless.util.api_requests import ('
    - '            create_chatgpt_config,'
    - '            num_tokens_from_messages,'
    - '            request_chatgpt_engine,'
    - '        )'
    - ''
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            request_codegeex_engine'
    - '        )'
    - ''
    - '        message = self.obtain_relevant_files_prompt.format('
    - '            problem_statement=self.problem_statement,'
    - '            structure=show_project_structure(self.structure).strip(),'
    - '        ).strip()'
    - '        print(f"prompting with message:\n{message}")'
    - '        print("=" * 80)'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "gpt-4o-2024-05-13"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        config = create_chatgpt_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=0,'
    - '            batch_size=1,'
    - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        )'
    - ''
    - ''
    - '        ret = request_chatgpt_engine(config)'
    - ''
    - '        # config = create_codegeex_config('
    - '        # message=message,'
    - '        # max_tokens=self.max_tokens,'
    - '        # temperature=0,'
    - '        # )'
    - ''
    - '        # ret = request_codegeex_engine(config)'
    - '        raw_output = ret.choices[0].message.content'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - '        model_found_files = self._parse_model_return_lines(raw_output)'
    - '        '
    - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
    - '            self.structure'
    - '        )'
    - ''
    - '        for file_content in files:'
    - '            file = file_content[0]'
    - '            if file in model_found_files:'
    - '                found_files.append(file)'
    - ''
    - '        # sort based on order of appearance in model_found_files'
    - '        found_files = sorted(found_files, key=lambda x: model_found_files.index(x))'
    - ''
    - '        print(raw_output)'
    - ''
    - '        return ('
    - '            found_files,'
    - '            {"raw_output_files": raw_output},'
    - '            traj,'
    - '        )'
    - ''
    - '    def localize_function_for_files('
    - '        self, file_names, mock=False'
    - '    ) -> tuple[list, dict, dict]:'
    - '        # from agentless.util.api_requests import ('
    - '        #     create_chatgpt_config,'
    - '        #     num_tokens_from_messages,'
    - '        #     request_chatgpt_engine,'
    - '        # )'
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            num_tokens_from_messages,'
    - '            request_codegeex_engine'
    - '        )'
    - '        files, classes, functions = get_full_file_paths_and_classes_and_functions('
    - '            self.structure'
    - '        )'
    - ''
    - '        max_num_files = len(file_names)'
    - '        while 1:'
    - '            # added small fix to prevent too many tokens'
    - '            contents = []'
    - '            for file_name in file_names[:max_num_files]:'
    - '                for file_content in files:'
    - '                    if file_content[0] == file_name:'
    - '                        content = "\n".join(file_content[1])'
    - '                        file_content = line_wrap_content(content)'
    - '                        contents.append('
    - '                            self.file_content_template.format('
    - '                                file_name=file_name, file_content=file_content'
    - '                            )'
    - '                        )'
    - '                        break'
    - '                else:'
    - '                    raise ValueError(f"File {file_name} does not exist.")'
    - ''
    - '            file_contents = "".join(contents)'
    - '            if num_tokens_from_messages(file_contents, "gpt-4o-2024-05-13")
      < 128000:'
    - '                break'
    - '            else:'
    - '                max_num_files -= 1'
    - ''
    - '        message = self.obtain_relevant_code_combine_top_n_prompt.format('
    - '            problem_statement=self.problem_statement,'
    - '            file_contents=file_contents,'
    - '        ).strip()'
    - '        print(f"prompting with message:\n{message}")'
    - '        print("=" * 80)'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                # "usage": {'
    - '                #     "prompt_tokens": num_tokens_from_messages('
    - '                #         message, "gpt-4o-2024-05-13"'
    - '                #     ),'
    - '                # },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        config = create_chatgpt_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=0,'
    - '            batch_size=1,'
    - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        )'
    - ''
    - '        # config = create_codegeex_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=0,'
    - '        # )'
    - '        '
    - '        ret = request_chatgpt_engine(config)'
    - '        # ret = request_codegeex_engine(config)'
    - ''
    - '        raw_output = ret.choices[0].message.content'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - ''
    - '        model_found_locs = extract_code_blocks(raw_output)'
    - '        model_found_locs_separated = extract_locs_for_files('
    - '            model_found_locs, file_names'
    - '        )'
    - ''
    - '        print(raw_output)'
    - ''
    - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
      traj'
    - ''
    - '    def localize_function_from_compressed_files(self, file_names, mock=False):'
    - '        from agentless.util.api_requests import ('
    - '            create_chatgpt_config,'
    - '            num_tokens_from_messages,'
    - '            request_chatgpt_engine,'
    - '        )'
    - ''
    - '        from agentless.util.api_requests import ('
    - '            create_codegeex_config,'
    - '            num_tokens_from_messages,'
    - '            request_codegeex_engine'
    - '        )'
    - ''
    - '        file_contents = get_repo_files(self.structure, file_names)'
    - '        compressed_file_contents = {'
    - '            fn: get_skeleton(code) for fn, code in file_contents.items()'
    - '        }'
    - '        contents = ['
    - '            self.file_content_in_block_template.format(file_name=fn, file_content=code)'
    - '            for fn, code in compressed_file_contents.items()'
    - '        ]'
    - '        file_contents = "".join(contents)'
    - '        template = ('
    - '            self.obtain_relevant_functions_and_vars_from_compressed_files_prompt_more'
    - '        )'
    - '        message = template.format('
    - '            problem_statement=self.problem_statement, file_contents=file_contents'
    - '        )'
    - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") < 128000'
    - '        logging.info(f"prompting with message:\n{message}")'
    - '        logging.info("=" * 80)'
    - ''
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "gpt-4o-2024-05-13"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - ''
    - '        config = create_chatgpt_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=0,'
    - '            batch_size=1,'
    - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        )'
    - '        ret = request_chatgpt_engine(config)'
    - '        raw_output = ret.choices[0].message.content'
    - ''
    - '        # config = create_codegeex_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=0,'
    - '        # )'
    - '        # ret = request_codegeex_engine(config)'
    - '        # raw_output = ret.choices[0].message.content'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_output,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - ''
    - '        model_found_locs = extract_code_blocks(raw_output)'
    - '        model_found_locs_separated = extract_locs_for_files('
    - '            model_found_locs, file_names'
    - '        )'
    - ''
    - '        logging.info(f"==== raw output ====")'
    - '        logging.info(raw_output)'
    - '        logging.info("=" * 80)'
    - '        logging.info(f"==== extracted locs ====")'
    - '        for loc in model_found_locs_separated:'
    - '            logging.info(loc)'
    - '        logging.info("=" * 80)'
    - ''
    - '        return model_found_locs_separated, {"raw_output_loc": raw_output},
      traj'
    - ''
    - '    def localize_line_from_coarse_function_locs('
    - '        self,'
    - '        file_names,'
    - '        coarse_locs,'
    - '        context_window: int,'
    - '        add_space: bool,'
    - '        sticky_scroll: bool,'
    - '        no_line_number: bool,'
    - '        temperature: float = 0.0,'
    - '        num_samples: int = 1,'
    - '        mock=False,'
    - '    ):'
    - '        from agentless.util.api_requests import ('
    - '            create_chatgpt_config,'
    - '            num_tokens_from_messages,'
    - '            request_chatgpt_engine,'
    - '        )'
    - ''
    - '        file_contents = get_repo_files(self.structure, file_names)'
    - '        topn_content, file_loc_intervals = construct_topn_file_context('
    - '            coarse_locs,'
    - '            file_names,'
    - '            file_contents,'
    - '            self.structure,'
    - '            context_window=context_window,'
    - '            loc_interval=True,'
    - '            add_space=add_space,'
    - '            sticky_scroll=sticky_scroll,'
    - '            no_line_number=no_line_number,'
    - '        )'
    - '        if no_line_number:'
    - '            template = self.obtain_relevant_code_combine_top_n_no_line_number_prompt'
    - '        else:'
    - '            template = self.obtain_relevant_code_combine_top_n_prompt'
    - '        message = template.format('
    - '            problem_statement=self.problem_statement, file_contents=topn_content'
    - '        )'
    - '        logging.info(f"prompting with message:\n{message}")'
    - '        logging.info("=" * 80)'
    - '        assert num_tokens_from_messages(message, "gpt-4o-2024-05-13") < 128000'
    - '        if mock:'
    - '            traj = {'
    - '                "prompt": message,'
    - '                "usage": {'
    - '                    "prompt_tokens": num_tokens_from_messages('
    - '                        message, "gpt-4o-2024-05-13"'
    - '                    ),'
    - '                },'
    - '            }'
    - '            return [], {"raw_output_loc": ""}, traj'
    - '        config = create_chatgpt_config('
    - '            message=message,'
    - '            max_tokens=self.max_tokens,'
    - '            temperature=temperature,'
    - '            batch_size=num_samples,'
    - '            model="gpt-4o-2024-05-13",  # use gpt-4o for now.'
    - '        )'
    - '        # config = create_codegeex_config('
    - '        #     message=message,'
    - '        #     max_tokens=self.max_tokens,'
    - '        #     temperature=temperature,'
    - '        #     model="codegeex-4",'
    - '        # )'
    - ''
    - '        ret = request_chatgpt_engine(config)'
    - '        # ret = request_codegeex_engine(config)'
    - ''
    - '        raw_outputs = [choice.message.content for choice in ret.choices]'
    - '        traj = {'
    - '            "prompt": message,'
    - '            "response": raw_outputs,'
    - '            "usage": {'
    - '                "prompt_tokens": ret.usage.prompt_tokens,'
    - '                "completion_tokens": ret.usage.completion_tokens,'
    - '            },'
    - '        }'
    - '        model_found_locs_separated_in_samples = []'
    - '        for raw_output in raw_outputs:'
    - '            model_found_locs = extract_code_blocks(raw_output)'
    - '            model_found_locs_separated = extract_locs_for_files('
    - '                model_found_locs, file_names'
    - '            )'
    - '            model_found_locs_separated_in_samples.append(model_found_locs_separated)'
    - ''
    - '            logging.info(f"==== raw output ====")'
    - '            logging.info(raw_output)'
    - '            logging.info("=" * 80)'
    - '            print(raw_output)'
    - '            print("=" * 80)'
    - '            logging.info(f"==== extracted locs ====")'
    - '            for loc in model_found_locs_separated:'
    - '                logging.info(loc)'
    - '            logging.info("=" * 80)'
    - '        logging.info("==== Input coarse_locs")'
    - '        coarse_info = ""'
    - '        for fn, found_locs in coarse_locs.items():'
    - '            coarse_info += f"### {fn}\n"'
    - '            if isinstance(found_locs, str):'
    - '                coarse_info += found_locs + "\n"'
    - '            else:'
    - '                coarse_info += "\n".join(found_locs) + "\n"'
    - '        logging.info("\n" + coarse_info)'
    - '        if len(model_found_locs_separated_in_samples) == 1:'
    - '            model_found_locs_separated_in_samples = ('
    - '                model_found_locs_separated_in_samples[0]'
    - '            )'
    - ''
    - '        return ('
    - '            model_found_locs_separated_in_samples,'
    - '            {"raw_output_loc": raw_outputs},'
    - '            traj,'
    - '        )'
  __pycache__:
    FL.cpython-311.pyc: {}
    FL_gpt.cpython-311.pyc: {}
  localize.py:
    classes: []
    functions:
    - end_line: 180
      name: localize
      start_line: 33
      text:
      - 'def localize(args):'
      - ''
      - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
      - ''
      - '    if args.start_file:'
      - '        start_file_locs = load_jsonl(args.start_file)'
      - ''
      - '    for bug in swe_bench_data:'
      - ''
      - '        if args.target_id is not None:'
      - '            if args.target_id != bug["instance_id"]:'
      - '                continue'
      - ''
      - '        if PROJECT_FILE_LOC is not None:'
      - '            project_file = os.path.join(PROJECT_FILE_LOC, bug["instance_id"]
        + ".json")'
      - '            d = load_json(project_file)'
      - '        else:'
      - '            # we need to get the project structure directly'
      - '            d = get_project_structure_from_scratch('
      - '                bug["repo"], bug["base_commit"], bug["instance_id"], "playground"'
      - '            )'
      - ''
      - '        instance_id = d["instance_id"]'
      - ''
      - '        logging.info(f"================ localize {instance_id} ================")'
      - ''
      - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
      - '        problem_statement = bench_data["problem_statement"]'
      - '        structure = d["structure"]'
      - '        filter_none_python(structure)'
      - '        # some basic filtering steps'
      - '        # filter out test files (unless its pytest)'
      - '        if not d["instance_id"].startswith("pytest"):'
      - '            filter_out_test_files(structure)'
      - ''
      - '        found_files = []'
      - '        found_related_locs = []'
      - '        found_edit_locs = []'
      - ''
      - '        additional_artifact_loc_file = None'
      - '        additional_artifact_loc_related = None'
      - '        additional_artifact_loc_edit_location = None'
      - '        file_traj, related_loc_traj, edit_loc_traj = {}, {}, {}'
      - ''
      - '        # file level localization'
      - '        if args.file_level:'
      - '            fl = LLMFL('
      - '                d["instance_id"],'
      - '                structure,'
      - '                problem_statement,'
      - '            )'
      - '            found_files, additional_artifact_loc_file, file_traj = fl.localize('
      - '                mock=args.mock'
      - '            )'
      - '        else:'
      - '            # assume start_file is provided'
      - '            for locs in start_file_locs:'
      - '                if locs["instance_id"] == d["instance_id"]:'
      - '                    found_files = locs["found_files"]'
      - '                    additional_artifact_loc_file = locs["additional_artifact_loc_file"]'
      - '                    file_traj = locs["file_traj"]'
      - ''
      - '                    if "found_related_locs" in locs:'
      - '                        found_related_locs = locs["found_related_locs"]'
      - '                        additional_artifact_loc_related = locs['
      - '                            "additional_artifact_loc_related"'
      - '                        ]'
      - '                        related_loc_traj = locs["related_loc_traj"]'
      - '                    break'
      - ''
      - '        # related class, functions, global var localization'
      - '        if args.related_level:'
      - '            if len(found_files) != 0:'
      - '                pred_files = found_files[: args.top_n]'
      - '                fl = LLMFL('
      - '                    d["instance_id"],'
      - '                    structure,'
      - '                    problem_statement,'
      - '                )'
      - ''
      - '                additional_artifact_loc_related = []'
      - '                found_related_locs = []'
      - '                related_loc_traj = {}'
      - ''
      - '                if args.compress:'
      - '                    ('
      - '                        found_related_locs,'
      - '                        additional_artifact_loc_related,'
      - '                        related_loc_traj,'
      - '                    ) = fl.localize_function_from_compressed_files('
      - '                        pred_files,'
      - '                        mock=args.mock,'
      - '                    )'
      - '                    additional_artifact_loc_related = [additional_artifact_loc_related]'
      - '                else:'
      - '                    assert False, "Not implemented yet."'
      - ''
      - '        if args.fine_grain_line_level:'
      - '            # Only supports the following args for now'
      - ''
      - '            pred_files = found_files[: args.top_n]'
      - '            fl = LLMFL('
      - '                instance_id,'
      - '                structure,'
      - '                problem_statement,'
      - '            )'
      - '            coarse_found_locs = {}'
      - '            for i, pred_file in enumerate(pred_files):'
      - '                if len(found_related_locs) > i:'
      - '                    coarse_found_locs[pred_file] = found_related_locs[i]'
      - '            ('
      - '                found_edit_locs,'
      - '                additional_artifact_loc_edit_location,'
      - '                edit_loc_traj,'
      - '            ) = fl.localize_line_from_coarse_function_locs('
      - '                pred_files,'
      - '                coarse_found_locs,'
      - '                context_window=args.context_window,'
      - '                add_space=args.add_space,'
      - '                no_line_number=args.no_line_number,'
      - '                sticky_scroll=args.sticky_scroll,'
      - '                mock=args.mock,'
      - '                temperature=args.temperature,'
      - '                num_samples=args.num_samples,'
      - '            )'
      - ''
      - '            additional_artifact_loc_edit_location = ['
      - '                additional_artifact_loc_edit_location'
      - '            ]'
      - ''
      - '        with open(args.output_file, "a") as f:'
      - '            f.write('
      - '                json.dumps('
      - '                    {'
      - '                        "instance_id": d["instance_id"],'
      - '                        "found_files": found_files,'
      - '                        "additional_artifact_loc_file": additional_artifact_loc_file,'
      - '                        "file_traj": file_traj,'
      - '                        "found_related_locs": found_related_locs,'
      - '                        "additional_artifact_loc_related": additional_artifact_loc_related,'
      - '                        "related_loc_traj": related_loc_traj,'
      - '                        "found_edit_locs": found_edit_locs,'
      - '                        "additional_artifact_loc_edit_location": additional_artifact_loc_edit_location,'
      - '                        "edit_loc_traj": edit_loc_traj,'
      - '                    }'
      - '                )'
      - '                + "\n"'
      - '            )'
    - end_line: 244
      name: merge
      start_line: 183
      text:
      - 'def merge(args):'
      - '    """Merge predicted locations."""'
      - '    start_file_locs = load_jsonl(args.start_file)'
      - ''
      - '    # Dump each location sample.'
      - '    for st_id in range(args.num_samples):'
      - '        en_id = st_id'
      - '        merged_locs = []'
      - '        for locs in start_file_locs:'
      - '            merged_found_locs = []'
      - '            if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
      - '                merged_found_locs = ['
      - '                    "\n".join(x) for x in locs["found_edit_locs"][st_id]'
      - '                ]'
      - '            merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
      - '        with open('
      - '            f"{args.output_folder}/loc_merged_{st_id}-{en_id}_outputs.jsonl",
        "w"'
      - '        ) as f:'
      - '            for data in merged_locs:'
      - '                f.write(json.dumps(data) + "\n")'
      - ''
      - '    # Pair wise merge'
      - '    for st_id in range(0, args.num_samples - 1, 2):'
      - '        en_id = st_id + 1'
      - '        print(f"Merging sample {st_id} and {en_id}...")'
      - '        merged_locs = []'
      - '        for locs in start_file_locs:'
      - '            merged_found_locs = []'
      - '            if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
      - '                merged_found_locs = ['
      - '                    "\n".join(x) for x in locs["found_edit_locs"][st_id]'
      - '                ]'
      - '                for sample_found_locs in locs["found_edit_locs"][st_id +
        1 : en_id + 1]:'
      - '                    for i, file_found_locs in enumerate(sample_found_locs):'
      - '                        if isinstance(file_found_locs, str):'
      - '                            merged_found_locs[i] += "\n" + file_found_locs'
      - '                        else:'
      - '                            merged_found_locs[i] += "\n" + "\n".join(file_found_locs)'
      - '            merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
      - '        with open('
      - '            f"{args.output_folder}/loc_merged_{st_id}-{en_id}_outputs.jsonl",
        "w"'
      - '        ) as f:'
      - '            for data in merged_locs:'
      - '                f.write(json.dumps(data) + "\n")'
      - ''
      - '    ### Merge all'
      - '    all_merged_locs = []'
      - '    print("Merging all samples...")'
      - '    for locs in start_file_locs:'
      - '        merged_found_locs = []'
      - '        if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
      - '            merged_found_locs = ["\n".join(x) for x in locs["found_edit_locs"][0]]'
      - '            for sample_found_locs in locs["found_edit_locs"][1:]:'
      - '                for i, file_found_locs in enumerate(sample_found_locs):'
      - '                    if isinstance(file_found_locs, str):'
      - '                        merged_found_locs[i] += "\n" + file_found_locs'
      - '                    else:'
      - '                        merged_found_locs[i] += "\n" + "\n".join(file_found_locs)'
      - '        all_merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
      - '    with open(f"{args.output_folder}/loc_all_merged_outputs.jsonl", "w")
        as f:'
      - '        for data in all_merged_locs:'
      - '            f.write(json.dumps(data) + "\n")'
    - end_line: 309
      name: main
      start_line: 247
      text:
      - 'def main():'
      - '    parser = argparse.ArgumentParser()'
      - ''
      - '    parser.add_argument("--output_folder", type=str, required=True)'
      - '    parser.add_argument("--output_file", type=str, default="loc_outputs.jsonl")'
      - '    parser.add_argument('
      - '        "--start_file",'
      - '        type=str,'
      - '        help="""previous output file to start with to reduce'
      - '        the work, should use in combination without --file_level""",'
      - '    )'
      - '    parser.add_argument("--file_level", action="store_true")'
      - '    parser.add_argument("--related_level", action="store_true")'
      - '    parser.add_argument("--fine_grain_line_level", action="store_true")'
      - '    parser.add_argument("--top_n", type=int, default=3)'
      - '    parser.add_argument("--temperature", type=float, default=0.0)'
      - '    parser.add_argument("--num_samples", type=int, default=1)'
      - '    parser.add_argument("--compress", action="store_true")'
      - '    parser.add_argument("--merge", action="store_true")'
      - '    parser.add_argument("--add_space", action="store_true")'
      - '    parser.add_argument("--no_line_number", action="store_true")'
      - '    parser.add_argument("--sticky_scroll", action="store_true")'
      - '    parser.add_argument("--context_window", type=int, default=10)'
      - '    parser.add_argument("--target_id", type=str)'
      - '    parser.add_argument('
      - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
      - '    )'
      - ''
      - '    args = parser.parse_args()'
      - ''
      - '    import os'
      - ''
      - '    args.output_file = os.path.join(args.output_folder, args.output_file)'
      - ''
      - '    assert not os.path.exists(args.output_file), "Output file already exists"'
      - ''
      - '    assert not ('
      - '        args.file_level and args.start_file'
      - '    ), "Cannot use both file_level and start_file"'
      - ''
      - '    assert not ('
      - '        args.file_level and args.fine_grain_line_level and not args.related_level'
      - '    ), "Cannot use both file_level and fine_grain_line_level without related_level"'
      - ''
      - '    assert not ('
      - '        (not args.file_level) and (not args.start_file)'
      - '    ), "Must use either file_level or start_file"'
      - ''
      - '    os.makedirs(args.output_folder, exist_ok=True)'
      - ''
      - '    # write the arguments'
      - '    with open(f"{args.output_folder}/args.json", "w") as f:'
      - '        json.dump(vars(args), f, indent=4)'
      - ''
      - '    logging.basicConfig('
      - '        filename=f"{args.output_folder}/localize.log",'
      - '        level=logging.DEBUG,'
      - '        format="%(asctime)s - %(levelname)s - %(message)s",'
      - '    )'
      - '    if args.merge:'
      - '        merge(args)'
      - '    else:'
      - '        localize(args)'
    text:
    - import argparse
    - import json
    - import logging
    - import os
    - ''
    - from datasets import load_dataset
    - from tqdm import tqdm
    - ''
    - ''
    - '# for using CodegeeX 4'
    - from agentless.fl.FL import LLMFL
    - ''
    - '# for using GPT '
    - '# from agentless.fl.FL_gpt import LLMFL'
    - ''
    - ''
    - from agentless.util.preprocess_data import (
    - '    filter_none_python,'
    - '    filter_out_test_files,'
    - '    get_full_file_paths_and_classes_and_functions,'
    - '    show_project_structure,'
    - )
    - from agentless.util.utils import load_json, load_jsonl
    - from get_repo_structure.get_repo_structure import (
    - '    clone_repo,'
    - '    get_project_structure_from_scratch,'
    - )
    - ''
    - '# SET THIS IF YOU WANT TO USE THE PREPROCESSED FILES'
    - PROJECT_FILE_LOC = os.environ.get("PROJECT_FILE_LOC", None)
    - ''
    - ''
    - 'def localize(args):'
    - ''
    - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
    - ''
    - '    if args.start_file:'
    - '        start_file_locs = load_jsonl(args.start_file)'
    - ''
    - '    for bug in swe_bench_data:'
    - ''
    - '        if args.target_id is not None:'
    - '            if args.target_id != bug["instance_id"]:'
    - '                continue'
    - ''
    - '        if PROJECT_FILE_LOC is not None:'
    - '            project_file = os.path.join(PROJECT_FILE_LOC, bug["instance_id"]
      + ".json")'
    - '            d = load_json(project_file)'
    - '        else:'
    - '            # we need to get the project structure directly'
    - '            d = get_project_structure_from_scratch('
    - '                bug["repo"], bug["base_commit"], bug["instance_id"], "playground"'
    - '            )'
    - ''
    - '        instance_id = d["instance_id"]'
    - ''
    - '        logging.info(f"================ localize {instance_id} ================")'
    - ''
    - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
    - '        problem_statement = bench_data["problem_statement"]'
    - '        structure = d["structure"]'
    - '        filter_none_python(structure)'
    - '        # some basic filtering steps'
    - '        # filter out test files (unless its pytest)'
    - '        if not d["instance_id"].startswith("pytest"):'
    - '            filter_out_test_files(structure)'
    - ''
    - '        found_files = []'
    - '        found_related_locs = []'
    - '        found_edit_locs = []'
    - ''
    - '        additional_artifact_loc_file = None'
    - '        additional_artifact_loc_related = None'
    - '        additional_artifact_loc_edit_location = None'
    - '        file_traj, related_loc_traj, edit_loc_traj = {}, {}, {}'
    - ''
    - '        # file level localization'
    - '        if args.file_level:'
    - '            fl = LLMFL('
    - '                d["instance_id"],'
    - '                structure,'
    - '                problem_statement,'
    - '            )'
    - '            found_files, additional_artifact_loc_file, file_traj = fl.localize('
    - '                mock=args.mock'
    - '            )'
    - '        else:'
    - '            # assume start_file is provided'
    - '            for locs in start_file_locs:'
    - '                if locs["instance_id"] == d["instance_id"]:'
    - '                    found_files = locs["found_files"]'
    - '                    additional_artifact_loc_file = locs["additional_artifact_loc_file"]'
    - '                    file_traj = locs["file_traj"]'
    - ''
    - '                    if "found_related_locs" in locs:'
    - '                        found_related_locs = locs["found_related_locs"]'
    - '                        additional_artifact_loc_related = locs['
    - '                            "additional_artifact_loc_related"'
    - '                        ]'
    - '                        related_loc_traj = locs["related_loc_traj"]'
    - '                    break'
    - ''
    - '        # related class, functions, global var localization'
    - '        if args.related_level:'
    - '            if len(found_files) != 0:'
    - '                pred_files = found_files[: args.top_n]'
    - '                fl = LLMFL('
    - '                    d["instance_id"],'
    - '                    structure,'
    - '                    problem_statement,'
    - '                )'
    - ''
    - '                additional_artifact_loc_related = []'
    - '                found_related_locs = []'
    - '                related_loc_traj = {}'
    - ''
    - '                if args.compress:'
    - '                    ('
    - '                        found_related_locs,'
    - '                        additional_artifact_loc_related,'
    - '                        related_loc_traj,'
    - '                    ) = fl.localize_function_from_compressed_files('
    - '                        pred_files,'
    - '                        mock=args.mock,'
    - '                    )'
    - '                    additional_artifact_loc_related = [additional_artifact_loc_related]'
    - '                else:'
    - '                    assert False, "Not implemented yet."'
    - ''
    - '        if args.fine_grain_line_level:'
    - '            # Only supports the following args for now'
    - ''
    - '            pred_files = found_files[: args.top_n]'
    - '            fl = LLMFL('
    - '                instance_id,'
    - '                structure,'
    - '                problem_statement,'
    - '            )'
    - '            coarse_found_locs = {}'
    - '            for i, pred_file in enumerate(pred_files):'
    - '                if len(found_related_locs) > i:'
    - '                    coarse_found_locs[pred_file] = found_related_locs[i]'
    - '            ('
    - '                found_edit_locs,'
    - '                additional_artifact_loc_edit_location,'
    - '                edit_loc_traj,'
    - '            ) = fl.localize_line_from_coarse_function_locs('
    - '                pred_files,'
    - '                coarse_found_locs,'
    - '                context_window=args.context_window,'
    - '                add_space=args.add_space,'
    - '                no_line_number=args.no_line_number,'
    - '                sticky_scroll=args.sticky_scroll,'
    - '                mock=args.mock,'
    - '                temperature=args.temperature,'
    - '                num_samples=args.num_samples,'
    - '            )'
    - ''
    - '            additional_artifact_loc_edit_location = ['
    - '                additional_artifact_loc_edit_location'
    - '            ]'
    - ''
    - '        with open(args.output_file, "a") as f:'
    - '            f.write('
    - '                json.dumps('
    - '                    {'
    - '                        "instance_id": d["instance_id"],'
    - '                        "found_files": found_files,'
    - '                        "additional_artifact_loc_file": additional_artifact_loc_file,'
    - '                        "file_traj": file_traj,'
    - '                        "found_related_locs": found_related_locs,'
    - '                        "additional_artifact_loc_related": additional_artifact_loc_related,'
    - '                        "related_loc_traj": related_loc_traj,'
    - '                        "found_edit_locs": found_edit_locs,'
    - '                        "additional_artifact_loc_edit_location": additional_artifact_loc_edit_location,'
    - '                        "edit_loc_traj": edit_loc_traj,'
    - '                    }'
    - '                )'
    - '                + "\n"'
    - '            )'
    - ''
    - ''
    - 'def merge(args):'
    - '    """Merge predicted locations."""'
    - '    start_file_locs = load_jsonl(args.start_file)'
    - ''
    - '    # Dump each location sample.'
    - '    for st_id in range(args.num_samples):'
    - '        en_id = st_id'
    - '        merged_locs = []'
    - '        for locs in start_file_locs:'
    - '            merged_found_locs = []'
    - '            if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
    - '                merged_found_locs = ['
    - '                    "\n".join(x) for x in locs["found_edit_locs"][st_id]'
    - '                ]'
    - '            merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
    - '        with open('
    - '            f"{args.output_folder}/loc_merged_{st_id}-{en_id}_outputs.jsonl",
      "w"'
    - '        ) as f:'
    - '            for data in merged_locs:'
    - '                f.write(json.dumps(data) + "\n")'
    - ''
    - '    # Pair wise merge'
    - '    for st_id in range(0, args.num_samples - 1, 2):'
    - '        en_id = st_id + 1'
    - '        print(f"Merging sample {st_id} and {en_id}...")'
    - '        merged_locs = []'
    - '        for locs in start_file_locs:'
    - '            merged_found_locs = []'
    - '            if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
    - '                merged_found_locs = ['
    - '                    "\n".join(x) for x in locs["found_edit_locs"][st_id]'
    - '                ]'
    - '                for sample_found_locs in locs["found_edit_locs"][st_id + 1
      : en_id + 1]:'
    - '                    for i, file_found_locs in enumerate(sample_found_locs):'
    - '                        if isinstance(file_found_locs, str):'
    - '                            merged_found_locs[i] += "\n" + file_found_locs'
    - '                        else:'
    - '                            merged_found_locs[i] += "\n" + "\n".join(file_found_locs)'
    - '            merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
    - '        with open('
    - '            f"{args.output_folder}/loc_merged_{st_id}-{en_id}_outputs.jsonl",
      "w"'
    - '        ) as f:'
    - '            for data in merged_locs:'
    - '                f.write(json.dumps(data) + "\n")'
    - ''
    - '    ### Merge all'
    - '    all_merged_locs = []'
    - '    print("Merging all samples...")'
    - '    for locs in start_file_locs:'
    - '        merged_found_locs = []'
    - '        if "found_edit_locs" in locs and len(locs["found_edit_locs"]):'
    - '            merged_found_locs = ["\n".join(x) for x in locs["found_edit_locs"][0]]'
    - '            for sample_found_locs in locs["found_edit_locs"][1:]:'
    - '                for i, file_found_locs in enumerate(sample_found_locs):'
    - '                    if isinstance(file_found_locs, str):'
    - '                        merged_found_locs[i] += "\n" + file_found_locs'
    - '                    else:'
    - '                        merged_found_locs[i] += "\n" + "\n".join(file_found_locs)'
    - '        all_merged_locs.append({**locs, "found_edit_locs": merged_found_locs})'
    - '    with open(f"{args.output_folder}/loc_all_merged_outputs.jsonl", "w") as
      f:'
    - '        for data in all_merged_locs:'
    - '            f.write(json.dumps(data) + "\n")'
    - ''
    - ''
    - 'def main():'
    - '    parser = argparse.ArgumentParser()'
    - ''
    - '    parser.add_argument("--output_folder", type=str, required=True)'
    - '    parser.add_argument("--output_file", type=str, default="loc_outputs.jsonl")'
    - '    parser.add_argument('
    - '        "--start_file",'
    - '        type=str,'
    - '        help="""previous output file to start with to reduce'
    - '        the work, should use in combination without --file_level""",'
    - '    )'
    - '    parser.add_argument("--file_level", action="store_true")'
    - '    parser.add_argument("--related_level", action="store_true")'
    - '    parser.add_argument("--fine_grain_line_level", action="store_true")'
    - '    parser.add_argument("--top_n", type=int, default=3)'
    - '    parser.add_argument("--temperature", type=float, default=0.0)'
    - '    parser.add_argument("--num_samples", type=int, default=1)'
    - '    parser.add_argument("--compress", action="store_true")'
    - '    parser.add_argument("--merge", action="store_true")'
    - '    parser.add_argument("--add_space", action="store_true")'
    - '    parser.add_argument("--no_line_number", action="store_true")'
    - '    parser.add_argument("--sticky_scroll", action="store_true")'
    - '    parser.add_argument("--context_window", type=int, default=10)'
    - '    parser.add_argument("--target_id", type=str)'
    - '    parser.add_argument('
    - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
    - '    )'
    - ''
    - '    args = parser.parse_args()'
    - ''
    - '    import os'
    - ''
    - '    args.output_file = os.path.join(args.output_folder, args.output_file)'
    - ''
    - '    assert not os.path.exists(args.output_file), "Output file already exists"'
    - ''
    - '    assert not ('
    - '        args.file_level and args.start_file'
    - '    ), "Cannot use both file_level and start_file"'
    - ''
    - '    assert not ('
    - '        args.file_level and args.fine_grain_line_level and not args.related_level'
    - '    ), "Cannot use both file_level and fine_grain_line_level without related_level"'
    - ''
    - '    assert not ('
    - '        (not args.file_level) and (not args.start_file)'
    - '    ), "Must use either file_level or start_file"'
    - ''
    - '    os.makedirs(args.output_folder, exist_ok=True)'
    - ''
    - '    # write the arguments'
    - '    with open(f"{args.output_folder}/args.json", "w") as f:'
    - '        json.dump(vars(args), f, indent=4)'
    - ''
    - '    logging.basicConfig('
    - '        filename=f"{args.output_folder}/localize.log",'
    - '        level=logging.DEBUG,'
    - '        format="%(asctime)s - %(levelname)s - %(message)s",'
    - '    )'
    - '    if args.merge:'
    - '        merge(args)'
    - '    else:'
    - '        localize(args)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    main()'
repair:
  .DS_Store: {}
  __pycache__:
    repair.cpython-311.pyc: {}
  repair.py:
    classes: []
    functions:
    - end_line: 198
      name: _post_process_multifile_repair
      start_line: 150
      text:
      - def _post_process_multifile_repair(
      - '    raw_output: str,'
      - '    file_contents: dict[str, str],'
      - '    file_loc_intervals: dict[str, list],'
      - '    diff_format=False,'
      - '):'
      - '    edit_multifile_commands = extract_python_blocks(raw_output)'
      - '    edited_file = ""'
      - '    new_content = ""'
      - '    try:'
      - '        file_to_commands = split_edit_multifile_commands('
      - '            edit_multifile_commands, diff_format=diff_format'
      - '        )'
      - '        logging.info("=== file_to_commands: ===")'
      - '        logging.info(json.dumps(file_to_commands, indent=2))'
      - '        # Let''s only edit the first file in the edit commands.'
      - '        edited_file_key = next(iter(file_to_commands.keys()))'
      - '        logging.info(f"=== edited_file: {edited_file_key} ===")'
      - '        edit_commands = file_to_commands[edited_file_key]'
      - '        logging.info("=== edit_commands: ===")'
      - '        for c in edit_commands:'
      - '            logging.info(c)'
      - '            logging.info("\n" + "-" * 40)'
      - '        edited_file = eval(edited_file_key)  # convert ''"file.py"'' to ''file.py'''
      - '        content = file_contents[edited_file]'
      - '        if diff_format:'
      - '            new_content = parse_diff_edit_commands('
      - '                edit_commands, content, file_loc_intervals[edited_file]'
      - '            )'
      - '        else:'
      - '            new_content = parse_edit_commands(edit_commands, content)'
      - '    except Exception as e:'
      - '        logging.error(e)'
      - '        return edited_file, new_content'
      - ''
      - '    diff = list('
      - '        unified_diff('
      - '            content.split("\n"),'
      - '            new_content.split("\n"),'
      - '            fromfile=edited_file,'
      - '            tofile=edited_file,'
      - '            lineterm="",'
      - '        )'
      - '    )'
      - ''
      - '    logging.info(f"extracted patch:")'
      - '    logging.info("\n".join(diff))'
      - '    print("\n".join(diff))'
      - '    return edited_file, new_content'
    - end_line: 244
      name: construct_topn_file_context
      start_line: 201
      text:
      - def construct_topn_file_context(
      - '    file_to_locs,'
      - '    pred_files,'
      - '    file_contents,'
      - '    structure,'
      - '    context_window: int,'
      - '    loc_interval: bool = True,'
      - '    fine_grain_loc_only: bool = False,'
      - '    add_space: bool = False,'
      - '    sticky_scroll: bool = False,'
      - '    no_line_number: bool = True,'
      - '):'
      - '    """Concatenate provided locations to form a context.'
      - ''
      - '    loc: {"file_name_1": ["loc_str_1"], ...}'
      - '    """'
      - '    file_loc_intervals = dict()'
      - '    topn_content = ""'
      - ''
      - '    for pred_file, locs in file_to_locs.items():'
      - '        content = file_contents[pred_file]'
      - '        line_locs, context_intervals = transfer_arb_locs_to_locs('
      - '            locs,'
      - '            structure,'
      - '            pred_file,'
      - '            context_window,'
      - '            loc_interval,'
      - '            fine_grain_loc_only,'
      - '            file_content=file_contents[pred_file] if pred_file in file_contents
        else "",'
      - '        )'
      - ''
      - '        if len(line_locs) > 0:'
      - '            # Note that if no location is predicted, we exclude this file.'
      - '            file_loc_content = line_wrap_content('
      - '                content,'
      - '                context_intervals,'
      - '                add_space=add_space,'
      - '                no_line_number=no_line_number,'
      - '                sticky_scroll=sticky_scroll,'
      - '            )'
      - '            topn_content += f"### {pred_file}\n{file_loc_content}\n\n\n"'
      - '            file_loc_intervals[pred_file] = context_intervals'
      - ''
      - '    return topn_content, file_loc_intervals'
    - end_line: 543
      name: repair
      start_line: 247
      text:
      - 'def repair(args):'
      - '    logging.basicConfig('
      - '        filename=f"{args.output_folder}/repair.log",'
      - '        level=logging.DEBUG,'
      - '        format="%(asctime)s - %(levelname)s - %(message)s",'
      - '    )'
      - ''
      - '    # write the arguments'
      - '    with open(f"{args.output_folder}/args.json", "w") as f:'
      - '        json.dump(vars(args), f, indent=4)'
      - ''
      - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
      - ''
      - '    locs = load_jsonl(args.loc_file)'
      - ''
      - '    if os.path.exists(args.output_file):'
      - '        prev_o = load_jsonl(args.output_file)'
      - '    else:'
      - '        prev_o = []'
      - ''
      - '    # make copy of loc in output_folder'
      - '    with open(f"{args.output_folder}/used_locs.jsonl", "w") as f:'
      - '        for loc in locs:'
      - '            f.write(json.dumps(loc) + "\n")'
      - ''
      - '    for loc in tqdm(locs):'
      - '        instance_id = loc["instance_id"]'
      - '        found = False'
      - '        for o in prev_o:'
      - '            if o["instance_id"] == instance_id:'
      - '                found = True'
      - '                break'
      - ''
      - '        if found:'
      - '            logging.info(f"skipping {instance_id} since patch already generated")'
      - '            continue'
      - ''
      - '        logging.info(f"================ repairing {instance_id} ================")'
      - ''
      - '        if len(loc["found_files"]) == 0:'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "instance_id": instance_id,'
      - '                            "raw_output": [""],'
      - '                            "try_count": [0],'
      - '                            "all_generations": [[]],'
      - '                            "traj": [],'
      - '                            "prev_content": [[]],'
      - '                            "file_names": [[]],'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - ''
      - '            logging.info(f"skipped since no files were localized")'
      - '            continue'
      - ''
      - '        pred_files = loc["found_files"][: args.top_n]'
      - ''
      - '        # grab buggy problem issue description and structure data'
      - ''
      - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
      - '        problem_statement = bench_data["problem_statement"]'
      - '        structure = get_repo_structure('
      - '            instance_id, bench_data["repo"], bench_data["base_commit"], "playground"'
      - '        )'
      - ''
      - '        files, _, _ = get_full_file_paths_and_classes_and_functions(structure)'
      - ''
      - '        raw_outputs, counts, all_generations, traj, prev_contents, file_names
        = ('
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '        )'
      - ''
      - '        raw_output = ""'
      - '        new_content = ""'
      - '        topn_content = ""'
      - '        # Construct file contents'
      - '        file_contents = dict()'
      - '        for i, pred_file in enumerate(pred_files):'
      - '            content = None'
      - ''
      - '            for file_content in files:'
      - '                if file_content[0] == pred_file:'
      - '                    content = "\n".join(file_content[1])'
      - '                    file_contents[pred_file] = content'
      - '                    break'
      - ''
      - '            assert content is not None, f"{pred_file} file not found"'
      - '        # Construct top-n file context'
      - '        file_to_edit_locs = dict()'
      - '        for i, pred_file in enumerate(pred_files):'
      - '            if "found_edit_locs" in loc and len(loc["found_edit_locs"]) >
        i:'
      - '                file_to_edit_locs[pred_file] = loc["found_edit_locs"][i]'
      - ''
      - '        topn_content, file_loc_intervals = construct_topn_file_context('
      - '            file_to_edit_locs,'
      - '            pred_files,'
      - '            file_contents,'
      - '            structure,'
      - '            context_window=args.context_window,'
      - '            loc_interval=args.loc_interval,'
      - '            fine_grain_loc_only=args.fine_grain_loc_only,'
      - '            add_space=args.add_space,'
      - '            no_line_number=args.diff_format,'
      - '            sticky_scroll=args.sticky_scroll,'
      - '        )'
      - ''
      - '        if topn_content.strip() == "":'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "instance_id": instance_id,'
      - '                            "raw_output": [""],'
      - '                            "try_count": [0],'
      - '                            "all_generations": [[]],'
      - '                            "traj": [],'
      - '                            "prev_content": [[]],'
      - '                            "file_names": [[]],'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - ''
      - '            logging.info(f"skipped since no files were localized")'
      - '            continue'
      - ''
      - '        # Construct prompt.'
      - '        # Note that we assume there''s no feedback, and we always use the
        same prompt in each turn.'
      - '        if args.cot and args.diff_format:'
      - '            prompt_template = repair_prompt_combine_topn_cot_diff'
      - '        elif args.cot:'
      - '            prompt_template = repair_prompt_combine_topn_cot'
      - '        else:'
      - '            prompt_template = repair_prompt_combine_topn'
      - ''
      - '        file_instruction = repair_relevant_file_instruction'
      - ''
      - '        message = prompt_template.format('
      - '            repair_relevant_file_instruction=file_instruction,'
      - '            problem_statement=problem_statement,'
      - '            content=topn_content.rstrip(),  # remove trailing newlines'
      - '        ).strip()'
      - ''
      - '        logging.info(f"prompting with message:\n{message}")'
      - ''
      - '        sample_responses = None'
      - ''
      - '        def get_response(count):'
      - '            nonlocal sample_responses'
      - '            if count == 0:'
      - '                if args.skip_greedy:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            ),'
      - '                        },'
      - '                    }'
      - '                # config = create_chatgpt_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0,  # greedy first'
      - '                #     batch_size=1,'
      - '                #     model=args.model,  # use gpt-4o for now.'
      - '                # )'
      - '                config = create_codegeex_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0,'
      - '                )'
      - '                '
      - '                greedy_response = request_codegeex_engine(config)'
      - '                # greedy_response = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": greedy_response,'
      - '                    "usage": {'
      - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
      - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
      - '                    },'
      - '                }'
      - '            elif args.stop_at_n_unique_valid_samples == -1:'
      - '                # No early-stopping, let''s get all samples at a time'
      - '                assert args.max_samples > 1'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            )'
      - '                            if count == 1'
      - '                            else 0,'
      - '                        },'
      - '                    }'
      - '                if sample_responses is not None:'
      - '                    # Directly return earlier samples'
      - '                    return {'
      - '                        "response": sample_responses#  sample_responses.choices[count
        - 1].message.content,'
      - '                        # "usage": {'
      - '                        #     "completion_tokens": 0,'
      - '                        #     "prompt_tokens": 0,'
      - '                        # },'
      - '                    }'
      - '                assert count == 1'
      - '                # config = create_chatgpt_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0.8,'
      - '                #     batch_size=args.max_samples - 1,  # minus the 1 greedy
        sample'
      - '                #     model=args.model,  # use gpt-4o for now.'
      - '                # )'
      - '                config = create_codegeex_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0.8,'
      - '                )'
      - '                sample_responses = request_codegeex_engine(config)'
      - '                # sample_responses = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": sample_responses # sample_responses.choices[count
        - 1].message.content,'
      - '                    # "usage": {'
      - '                    #     "completion_tokens": sample_responses.usage.completion_tokens,'
      - '                    #     "prompt_tokens": sample_responses.usage.prompt_tokens,'
      - '                    # },'
      - '                }'
      - ''
      - '        count = 0'
      - '        while count < args.max_samples:'
      - '            print(f"trying the {count + 1}-th sample ...")'
      - '            ret = get_response(count)'
      - '            count += 1'
      - '            traj.append('
      - '                {'
      - '                    **ret,'
      - '                    "prompt": message,'
      - '                }'
      - '            )'
      - ''
      - '            if args.mock:'
      - '                continue'
      - '            raw_output = ret["response"]'
      - '            logging.info(f"raw output:\n{raw_output}")'
      - '            all_generations.append(raw_output)'
      - ''
      - '            edited_file, new_content = _post_process_multifile_repair('
      - '                raw_output,'
      - '                file_contents,'
      - '                file_loc_intervals,'
      - '                diff_format=args.diff_format,'
      - '            )'
      - ''
      - '            if new_content == "":'
      - '                prev_contents.append("")'
      - '                file_names.append("")'
      - '            else:'
      - '                prev_content = file_contents[edited_file]'
      - '                prev_contents.append(prev_content)'
      - '                file_names.append(edited_file)'
      - ''
      - '        counts.append(count)'
      - '        raw_outputs.append(raw_output)'
      - '        all_generations = [all_generations]'
      - '        prev_contents = [prev_contents]'
      - '        file_names = [file_names]'
      - ''
      - '        with open(args.output_file, "a") as f:'
      - '            f.write('
      - '                json.dumps('
      - '                    {'
      - '                        "instance_id": instance_id,'
      - '                        "raw_output": raw_outputs,'
      - '                        "all_generations": all_generations,'
      - '                        "try_count": counts,'
      - '                        "traj": traj,'
      - '                        "prev_content": prev_contents,'
      - '                        "file_names": file_names,'
      - '                    }'
      - '                )'
      - '                + "\n"'
      - '            )'
    - end_line: 598
      name: post_process_raw_output
      start_line: 546
      text:
      - 'def post_process_raw_output(raw_output_text, file_contents, file_loc_intervals,
        args):'
      - '    git_diffs = ""'
      - '    raw_git_diffs = ""'
      - '    lint_success = False'
      - '    content = ""'
      - '    try:'
      - '        edited_file, new_content = _post_process_multifile_repair('
      - '            raw_output_text,'
      - '            file_contents,'
      - '            file_loc_intervals,'
      - '            diff_format=args.diff_format,'
      - '        )'
      - '        if edited_file in file_contents:'
      - '            content = file_contents[edited_file]'
      - ''
      - '            git_diff = fake_git_repo("playground", edited_file, content,
        new_content)'
      - ''
      - '            raw_git_diffs += "\n" + git_diff.replace('
      - '                "\ No newline at end of file\n", ""'
      - '            )'
      - ''
      - '            syntax_success = check_syntax(new_content)'
      - '            lint_success, prev_errors, errors = lint_code('
      - '                "playground", "test.py", new_content, file_contents[edited_file]'
      - '            )'
      - ''
      - '            differ_by_empty_lines = check_code_differ_by_just_empty_lines('
      - '                new_content, file_contents[edited_file]'
      - '            )'
      - ''
      - '            print(lint_success, prev_errors, errors, differ_by_empty_lines)'
      - ''
      - '            if syntax_success and not differ_by_empty_lines:'
      - '                git_diffs = raw_git_diffs'
      - '            else:'
      - '                git_diffs = ""  # no need to evaluate'
      - '        else:'
      - '            diff = list('
      - '                unified_diff('
      - '                    content.split("\n"),'
      - '                    new_content.split("\n"),'
      - '                    fromfile=edited_file,'
      - '                    tofile=edited_file,'
      - '                    lineterm="",'
      - '                )'
      - '            )'
      - '            print("Failed parsing diff!")'
      - '            print("\n".join(diff))'
      - '    except Exception as e:'
      - '        print(raw_output_text)'
      - '        print(e)'
      - ''
      - '    return git_diffs, raw_git_diffs, content'
    - end_line: 695
      name: post_process_repair
      start_line: 601
      text:
      - 'def post_process_repair(args):'
      - '    """'
      - '    apply some diff formatting.'
      - '    """'
      - '    raw_outputs = load_jsonl(args.raw_output_file)'
      - '    locs = load_jsonl(args.loc_file)'
      - ''
      - '    for raw_output in raw_outputs:'
      - '        instance_id = raw_output["instance_id"]'
      - ''
      - '        if raw_output["raw_output"] == "":'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "model_name_or_path": "agentless",'
      - '                            "instance_id": instance_id,'
      - '                            "model_patch": "",'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - '            continue'
      - ''
      - '        if args.select_id == -1:'
      - '            # Use the last generation'
      - '            assert False, "not implemented for now"'
      - '        else:'
      - '            # Use the indexed generation'
      - '            generation_idx = args.select_id'
      - '            try:'
      - '                raw_output_text = raw_output["all_generations"][0][generation_idx]'
      - '                original_file_content = raw_output["prev_content"][0][generation_idx]'
      - '                pred_file = raw_output["file_names"][0][generation_idx]'
      - ''
      - '                pred_files = [loc for loc in locs if loc["instance_id"] ==
        instance_id]['
      - '                    0'
      - '                ]["found_files"][: args.top_n]'
      - ''
      - '                git_diffs = ""'
      - '                raw_git_diffs = ""'
      - '                if isinstance(raw_output["raw_output"], str):'
      - '                    # for backward compatibility'
      - '                    raw_output["raw_output"] = [raw_output["raw_output"]]'
      - ''
      - '                file_contents = {pred_file: original_file_content}'
      - ''
      - '                file_loc_intervals = dict()'
      - ''
      - '                loc = [loc for loc in locs if loc["instance_id"] == instance_id][0]'
      - ''
      - '                for i, tmp_pred_file in enumerate(pred_files):'
      - '                    if tmp_pred_file != pred_file:'
      - '                        continue'
      - '                    if "found_edit_locs" in loc and len(loc["found_edit_locs"])
        > i:'
      - '                        line_locs, context_intervals = transfer_arb_locs_to_locs('
      - '                            loc["found_edit_locs"][i],'
      - '                            None,'
      - '                            loc["found_files"][i],'
      - '                            args.context_window,'
      - '                            args.loc_interval,'
      - '                            args.fine_grain_loc_only,'
      - '                            file_content=file_contents[pred_file]'
      - '                            if pred_file in file_contents'
      - '                            else "",'
      - '                        )'
      - '                    else:'
      - '                        line_locs, context_intervals = [], []  # default
        values.'
      - ''
      - '                    file_loc_intervals[pred_file] = context_intervals'
      - '            except:'
      - '                raw_output_text = ""'
      - ''
      - '        if raw_output_text:'
      - '            git_diffs, raw_git_diffs, content = post_process_raw_output('
      - '                raw_output_text, file_contents, file_loc_intervals, args'
      - '            )'
      - '        else:'
      - '            git_diffs = ""'
      - '            raw_git_diffs = ""'
      - '            content = ""'
      - ''
      - '        with open(args.output_file, "a") as f:'
      - '            f.write('
      - '                json.dumps('
      - '                    {'
      - '                        "model_name_or_path": "agentless",'
      - '                        "instance_id": instance_id,'
      - '                        "model_patch": git_diffs.lstrip(),'
      - '                        "raw_model_patch": raw_git_diffs.lstrip(),'
      - '                        "original_file_content": content,'
      - '                    }'
      - '                )'
      - '                + "\n"'
      - '            )'
    - end_line: 764
      name: main
      start_line: 698
      text:
      - 'def main():'
      - '    parser = argparse.ArgumentParser()'
      - '    parser.add_argument("--loc_file", type=str, required=True)'
      - '    parser.add_argument("--top_n", type=int, default=1)'
      - '    parser.add_argument("--loc_interval", action="store_true")'
      - '    parser.add_argument("--context_window", type=int, default=10)'
      - '    parser.add_argument('
      - '        "--stop_at_n_unique_valid_samples",'
      - '        type=int,'
      - '        default=-1,'
      - '        help="Early stop when we get N unique valid samples, set to -1 if
        don''t want to do early stopping.",'
      - '    )'
      - '    parser.add_argument("--gen_and_process", action="store_true")'
      - '    parser.add_argument("--max_samples", type=int, default=20, help="Sampling
        budget.")'
      - '    parser.add_argument('
      - '        "--select_id",'
      - '        type=int,'
      - '        default=-1,'
      - '        help="Index the selected samples during post-processing.",'
      - '    )'
      - '    parser.add_argument('
      - '        "--model", type=str, default="gpt-4o-2024-05-13", choices=["gpt-4o-2024-05-13"]'
      - '    )'
      - '    parser.add_argument("--output_folder", type=str, required=True)'
      - '    parser.add_argument('
      - '        "--only_correct", action="store_true"'
      - '    )  # only work on correct loc files (saves time)'
      - '    parser.add_argument("--post_process", action="store_true")'
      - '    parser.add_argument("--add_space", action="store_true")'
      - '    parser.add_argument("--cot", action="store_true")'
      - '    parser.add_argument("--fine_grain_loc_only", action="store_true")'
      - '    parser.add_argument("--diff_format", action="store_true")'
      - '    parser.add_argument("--skip_greedy", action="store_true")'
      - '    parser.add_argument("--sticky_scroll", action="store_true")'
      - '    parser.add_argument('
      - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
      - '    )'
      - ''
      - '    args = parser.parse_args()'
      - ''
      - '    if not os.path.exists(args.output_folder):'
      - '        os.makedirs(args.output_folder)'
      - ''
      - '    args.output_file = os.path.join(args.output_folder, "output.jsonl")'
      - ''
      - '    if args.post_process:'
      - '        args.raw_output_file = args.output_file'
      - '        if args.select_id == -1:'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", "_processed.jsonl"'
      - '            )'
      - '        else:'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", f"_{args.select_id}_processed.jsonl"'
      - '            )'
      - '        post_process_repair(args)'
      - '    elif args.gen_and_process:'
      - '        repair(args)'
      - '        args.raw_output_file = args.output_file'
      - '        for i in range(args.max_samples):'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", f"_{i}_processed.jsonl"'
      - '            )'
      - '            args.select_id = i'
      - '            post_process_repair(args)'
      - '    else:'
      - '        repair(args)'
    - end_line: 488
      name: get_response
      start_line: 402
      text:
      - '        def get_response(count):'
      - '            nonlocal sample_responses'
      - '            if count == 0:'
      - '                if args.skip_greedy:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            ),'
      - '                        },'
      - '                    }'
      - '                # config = create_chatgpt_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0,  # greedy first'
      - '                #     batch_size=1,'
      - '                #     model=args.model,  # use gpt-4o for now.'
      - '                # )'
      - '                config = create_codegeex_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0,'
      - '                )'
      - '                '
      - '                greedy_response = request_codegeex_engine(config)'
      - '                # greedy_response = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": greedy_response,'
      - '                    "usage": {'
      - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
      - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
      - '                    },'
      - '                }'
      - '            elif args.stop_at_n_unique_valid_samples == -1:'
      - '                # No early-stopping, let''s get all samples at a time'
      - '                assert args.max_samples > 1'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            )'
      - '                            if count == 1'
      - '                            else 0,'
      - '                        },'
      - '                    }'
      - '                if sample_responses is not None:'
      - '                    # Directly return earlier samples'
      - '                    return {'
      - '                        "response": sample_responses#  sample_responses.choices[count
        - 1].message.content,'
      - '                        # "usage": {'
      - '                        #     "completion_tokens": 0,'
      - '                        #     "prompt_tokens": 0,'
      - '                        # },'
      - '                    }'
      - '                assert count == 1'
      - '                # config = create_chatgpt_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0.8,'
      - '                #     batch_size=args.max_samples - 1,  # minus the 1 greedy
        sample'
      - '                #     model=args.model,  # use gpt-4o for now.'
      - '                # )'
      - '                config = create_codegeex_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0.8,'
      - '                )'
      - '                sample_responses = request_codegeex_engine(config)'
      - '                # sample_responses = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": sample_responses # sample_responses.choices[count
        - 1].message.content,'
      - '                    # "usage": {'
      - '                    #     "completion_tokens": sample_responses.usage.completion_tokens,'
      - '                    #     "prompt_tokens": sample_responses.usage.prompt_tokens,'
      - '                    # },'
      - '                }'
    text:
    - import argparse
    - import copy
    - import json
    - import logging
    - import os
    - from difflib import unified_diff
    - ''
    - from datasets import load_dataset
    - from tqdm import tqdm
    - ''
    - from agentless.util.api_requests import (
    - '    create_chatgpt_config,'
    - '    num_tokens_from_messages,'
    - '    request_chatgpt_engine,'
    - )
    - ''
    - from agentless.util.api_requests import (
    - "\tcreate_codegeex_config,"
    - "\tnum_tokens_from_messages,"
    - "\trequest_codegeex_engine"
    - )
    - ''
    - from agentless.util.postprocess_data import (
    - '    check_code_differ_by_just_empty_lines,'
    - '    check_syntax,'
    - '    extract_python_blocks,'
    - '    fake_git_repo,'
    - '    lint_code,'
    - '    parse_diff_edit_commands,'
    - '    parse_edit_commands,'
    - '    remove_empty_lines,'
    - '    split_edit_multifile_commands,'
    - )
    - from agentless.util.preprocess_data import (
    - '    get_full_file_paths_and_classes_and_functions,'
    - '    get_repo_structure,'
    - '    line_wrap_content,'
    - '    transfer_arb_locs_to_locs,'
    - )
    - from agentless.util.utils import load_jsonl
    - ''
    - repair_relevant_file_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs.
    - '"""'
    - repair_relevant_file_with_scope_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs.
    - In the file below, "..." refers to some less relevant content being omited for
      brebity.
    - '"""'
    - with_scope_explanation = """
    - Note that "..." refers to some omited content that is not actually in the files.
      Your *SEARCH/REPLACE* edit must not contain such "...".
    - '"""'
    - repair_relevant_file_with_suspicious_loc_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs. Some suspicious locations are provided for closer inspection.
    - '"""'
    - repair_prompt_combine_topn = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please generate `edit_file` commands to fix the issue.
    - ''
    - 'The `edit_file` command takes four arguments:'
    - ''
    - 'edit_file(filename: str, start: int, end: int, content: str) -> None:'
    - '    Edit a file. It replaces lines `start` through `end` (inclusive) with the
      given text `content` in the open file.'
    - '    Args:'
    - '    filename: str: The full file name to edit.'
    - '    start: int: The start line number. Must satisfy start >= 1.'
    - '    end: int: The end line number. Must satisfy start <= end <= number of lines
      in the file.'
    - '    content: str: The content to replace the lines with.'
    - ''
    - Please note that THE `edit_file` FUNCTION REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the `edit_file` command in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - repair_prompt_combine_topn_cot = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please first localize the bug based on the issue statement, and then generate
      `edit_file` commands to fix the issue.
    - ''
    - 'The `edit_file` command takes four arguments:'
    - ''
    - 'edit_file(filename: str, start: int, end: int, content: str) -> None:'
    - '    Edit a file. It replaces lines `start` through `end` (inclusive) with the
      given text `content` in the open file.'
    - '    Args:'
    - '    filename: str: The full file name to edit.'
    - '    start: int: The start line number. Must satisfy start >= 1.'
    - '    end: int: The end line number. Must satisfy start <= end <= number of lines
      in the file.'
    - '    content: str: The content to replace the lines with.'
    - ''
    - Please note that THE `edit_file` FUNCTION REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the `edit_file` command in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - repair_prompt_combine_topn_cot_diff = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please first localize the bug based on the issue statement, and then generate
      *SEARCH/REPLACE* edits to fix the issue.
    - ''
    - 'Every *SEARCH/REPLACE* edit must use this format:'
    - 1. The file path
    - '2. The start of search block: <<<<<<< SEARCH'
    - 3. A contiguous chunk of lines to search for in the existing source code
    - '4. The dividing line: ======='
    - 5. The lines to replace into the source code
    - '6. The end of the replace block: >>>>>>> REPLACE'
    - ''
    - 'Here is an example:'
    - ''
    - '```python'
    - '### mathweb/flask/app.py'
    - from flask import Flask
    - '```'
    - ''
    - Please note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the *SEARCH/REPLACE* edit in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - def _post_process_multifile_repair(
    - '    raw_output: str,'
    - '    file_contents: dict[str, str],'
    - '    file_loc_intervals: dict[str, list],'
    - '    diff_format=False,'
    - '):'
    - '    edit_multifile_commands = extract_python_blocks(raw_output)'
    - '    edited_file = ""'
    - '    new_content = ""'
    - '    try:'
    - '        file_to_commands = split_edit_multifile_commands('
    - '            edit_multifile_commands, diff_format=diff_format'
    - '        )'
    - '        logging.info("=== file_to_commands: ===")'
    - '        logging.info(json.dumps(file_to_commands, indent=2))'
    - '        # Let''s only edit the first file in the edit commands.'
    - '        edited_file_key = next(iter(file_to_commands.keys()))'
    - '        logging.info(f"=== edited_file: {edited_file_key} ===")'
    - '        edit_commands = file_to_commands[edited_file_key]'
    - '        logging.info("=== edit_commands: ===")'
    - '        for c in edit_commands:'
    - '            logging.info(c)'
    - '            logging.info("\n" + "-" * 40)'
    - '        edited_file = eval(edited_file_key)  # convert ''"file.py"'' to ''file.py'''
    - '        content = file_contents[edited_file]'
    - '        if diff_format:'
    - '            new_content = parse_diff_edit_commands('
    - '                edit_commands, content, file_loc_intervals[edited_file]'
    - '            )'
    - '        else:'
    - '            new_content = parse_edit_commands(edit_commands, content)'
    - '    except Exception as e:'
    - '        logging.error(e)'
    - '        return edited_file, new_content'
    - ''
    - '    diff = list('
    - '        unified_diff('
    - '            content.split("\n"),'
    - '            new_content.split("\n"),'
    - '            fromfile=edited_file,'
    - '            tofile=edited_file,'
    - '            lineterm="",'
    - '        )'
    - '    )'
    - ''
    - '    logging.info(f"extracted patch:")'
    - '    logging.info("\n".join(diff))'
    - '    print("\n".join(diff))'
    - '    return edited_file, new_content'
    - ''
    - ''
    - def construct_topn_file_context(
    - '    file_to_locs,'
    - '    pred_files,'
    - '    file_contents,'
    - '    structure,'
    - '    context_window: int,'
    - '    loc_interval: bool = True,'
    - '    fine_grain_loc_only: bool = False,'
    - '    add_space: bool = False,'
    - '    sticky_scroll: bool = False,'
    - '    no_line_number: bool = True,'
    - '):'
    - '    """Concatenate provided locations to form a context.'
    - ''
    - '    loc: {"file_name_1": ["loc_str_1"], ...}'
    - '    """'
    - '    file_loc_intervals = dict()'
    - '    topn_content = ""'
    - ''
    - '    for pred_file, locs in file_to_locs.items():'
    - '        content = file_contents[pred_file]'
    - '        line_locs, context_intervals = transfer_arb_locs_to_locs('
    - '            locs,'
    - '            structure,'
    - '            pred_file,'
    - '            context_window,'
    - '            loc_interval,'
    - '            fine_grain_loc_only,'
    - '            file_content=file_contents[pred_file] if pred_file in file_contents
      else "",'
    - '        )'
    - ''
    - '        if len(line_locs) > 0:'
    - '            # Note that if no location is predicted, we exclude this file.'
    - '            file_loc_content = line_wrap_content('
    - '                content,'
    - '                context_intervals,'
    - '                add_space=add_space,'
    - '                no_line_number=no_line_number,'
    - '                sticky_scroll=sticky_scroll,'
    - '            )'
    - '            topn_content += f"### {pred_file}\n{file_loc_content}\n\n\n"'
    - '            file_loc_intervals[pred_file] = context_intervals'
    - ''
    - '    return topn_content, file_loc_intervals'
    - ''
    - ''
    - 'def repair(args):'
    - '    logging.basicConfig('
    - '        filename=f"{args.output_folder}/repair.log",'
    - '        level=logging.DEBUG,'
    - '        format="%(asctime)s - %(levelname)s - %(message)s",'
    - '    )'
    - ''
    - '    # write the arguments'
    - '    with open(f"{args.output_folder}/args.json", "w") as f:'
    - '        json.dump(vars(args), f, indent=4)'
    - ''
    - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
    - ''
    - '    locs = load_jsonl(args.loc_file)'
    - ''
    - '    if os.path.exists(args.output_file):'
    - '        prev_o = load_jsonl(args.output_file)'
    - '    else:'
    - '        prev_o = []'
    - ''
    - '    # make copy of loc in output_folder'
    - '    with open(f"{args.output_folder}/used_locs.jsonl", "w") as f:'
    - '        for loc in locs:'
    - '            f.write(json.dumps(loc) + "\n")'
    - ''
    - '    for loc in tqdm(locs):'
    - '        instance_id = loc["instance_id"]'
    - '        found = False'
    - '        for o in prev_o:'
    - '            if o["instance_id"] == instance_id:'
    - '                found = True'
    - '                break'
    - ''
    - '        if found:'
    - '            logging.info(f"skipping {instance_id} since patch already generated")'
    - '            continue'
    - ''
    - '        logging.info(f"================ repairing {instance_id} ================")'
    - ''
    - '        if len(loc["found_files"]) == 0:'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "instance_id": instance_id,'
    - '                            "raw_output": [""],'
    - '                            "try_count": [0],'
    - '                            "all_generations": [[]],'
    - '                            "traj": [],'
    - '                            "prev_content": [[]],'
    - '                            "file_names": [[]],'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - ''
    - '            logging.info(f"skipped since no files were localized")'
    - '            continue'
    - ''
    - '        pred_files = loc["found_files"][: args.top_n]'
    - ''
    - '        # grab buggy problem issue description and structure data'
    - ''
    - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
    - '        problem_statement = bench_data["problem_statement"]'
    - '        structure = get_repo_structure('
    - '            instance_id, bench_data["repo"], bench_data["base_commit"], "playground"'
    - '        )'
    - ''
    - '        files, _, _ = get_full_file_paths_and_classes_and_functions(structure)'
    - ''
    - '        raw_outputs, counts, all_generations, traj, prev_contents, file_names
      = ('
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '        )'
    - ''
    - '        raw_output = ""'
    - '        new_content = ""'
    - '        topn_content = ""'
    - '        # Construct file contents'
    - '        file_contents = dict()'
    - '        for i, pred_file in enumerate(pred_files):'
    - '            content = None'
    - ''
    - '            for file_content in files:'
    - '                if file_content[0] == pred_file:'
    - '                    content = "\n".join(file_content[1])'
    - '                    file_contents[pred_file] = content'
    - '                    break'
    - ''
    - '            assert content is not None, f"{pred_file} file not found"'
    - '        # Construct top-n file context'
    - '        file_to_edit_locs = dict()'
    - '        for i, pred_file in enumerate(pred_files):'
    - '            if "found_edit_locs" in loc and len(loc["found_edit_locs"]) > i:'
    - '                file_to_edit_locs[pred_file] = loc["found_edit_locs"][i]'
    - ''
    - '        topn_content, file_loc_intervals = construct_topn_file_context('
    - '            file_to_edit_locs,'
    - '            pred_files,'
    - '            file_contents,'
    - '            structure,'
    - '            context_window=args.context_window,'
    - '            loc_interval=args.loc_interval,'
    - '            fine_grain_loc_only=args.fine_grain_loc_only,'
    - '            add_space=args.add_space,'
    - '            no_line_number=args.diff_format,'
    - '            sticky_scroll=args.sticky_scroll,'
    - '        )'
    - ''
    - '        if topn_content.strip() == "":'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "instance_id": instance_id,'
    - '                            "raw_output": [""],'
    - '                            "try_count": [0],'
    - '                            "all_generations": [[]],'
    - '                            "traj": [],'
    - '                            "prev_content": [[]],'
    - '                            "file_names": [[]],'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - ''
    - '            logging.info(f"skipped since no files were localized")'
    - '            continue'
    - ''
    - '        # Construct prompt.'
    - '        # Note that we assume there''s no feedback, and we always use the same
      prompt in each turn.'
    - '        if args.cot and args.diff_format:'
    - '            prompt_template = repair_prompt_combine_topn_cot_diff'
    - '        elif args.cot:'
    - '            prompt_template = repair_prompt_combine_topn_cot'
    - '        else:'
    - '            prompt_template = repair_prompt_combine_topn'
    - ''
    - '        file_instruction = repair_relevant_file_instruction'
    - ''
    - '        message = prompt_template.format('
    - '            repair_relevant_file_instruction=file_instruction,'
    - '            problem_statement=problem_statement,'
    - '            content=topn_content.rstrip(),  # remove trailing newlines'
    - '        ).strip()'
    - ''
    - '        logging.info(f"prompting with message:\n{message}")'
    - ''
    - '        sample_responses = None'
    - ''
    - '        def get_response(count):'
    - '            nonlocal sample_responses'
    - '            if count == 0:'
    - '                if args.skip_greedy:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "completion_tokens": 0,'
    - '                            "prompt_tokens": 0,'
    - '                        },'
    - '                    }'
    - '                if args.mock:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "prompt_tokens": num_tokens_from_messages('
    - '                                message, "gpt-4o-2024-05-13"'
    - '                            ),'
    - '                        },'
    - '                    }'
    - '                # config = create_chatgpt_config('
    - '                #     message=message,'
    - '                #     max_tokens=1024,'
    - '                #     temperature=0,  # greedy first'
    - '                #     batch_size=1,'
    - '                #     model=args.model,  # use gpt-4o for now.'
    - '                # )'
    - '                config = create_codegeex_config('
    - '                    message=message,'
    - '                    max_tokens=1024,'
    - '                    temperature=0,'
    - '                )'
    - '                '
    - '                greedy_response = request_codegeex_engine(config)'
    - '                # greedy_response = request_chatgpt_engine(config)'
    - '                return {'
    - '                    "response": greedy_response,'
    - '                    "usage": {'
    - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
    - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
    - '                    },'
    - '                }'
    - '            elif args.stop_at_n_unique_valid_samples == -1:'
    - '                # No early-stopping, let''s get all samples at a time'
    - '                assert args.max_samples > 1'
    - '                if args.mock:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "prompt_tokens": num_tokens_from_messages('
    - '                                message, "gpt-4o-2024-05-13"'
    - '                            )'
    - '                            if count == 1'
    - '                            else 0,'
    - '                        },'
    - '                    }'
    - '                if sample_responses is not None:'
    - '                    # Directly return earlier samples'
    - '                    return {'
    - '                        "response": sample_responses#  sample_responses.choices[count
      - 1].message.content,'
    - '                        # "usage": {'
    - '                        #     "completion_tokens": 0,'
    - '                        #     "prompt_tokens": 0,'
    - '                        # },'
    - '                    }'
    - '                assert count == 1'
    - '                # config = create_chatgpt_config('
    - '                #     message=message,'
    - '                #     max_tokens=1024,'
    - '                #     temperature=0.8,'
    - '                #     batch_size=args.max_samples - 1,  # minus the 1 greedy
      sample'
    - '                #     model=args.model,  # use gpt-4o for now.'
    - '                # )'
    - '                config = create_codegeex_config('
    - '                    message=message,'
    - '                    max_tokens=1024,'
    - '                    temperature=0.8,'
    - '                )'
    - '                sample_responses = request_codegeex_engine(config)'
    - '                # sample_responses = request_chatgpt_engine(config)'
    - '                return {'
    - '                    "response": sample_responses # sample_responses.choices[count
      - 1].message.content,'
    - '                    # "usage": {'
    - '                    #     "completion_tokens": sample_responses.usage.completion_tokens,'
    - '                    #     "prompt_tokens": sample_responses.usage.prompt_tokens,'
    - '                    # },'
    - '                }'
    - ''
    - '        count = 0'
    - '        while count < args.max_samples:'
    - '            print(f"trying the {count + 1}-th sample ...")'
    - '            ret = get_response(count)'
    - '            count += 1'
    - '            traj.append('
    - '                {'
    - '                    **ret,'
    - '                    "prompt": message,'
    - '                }'
    - '            )'
    - ''
    - '            if args.mock:'
    - '                continue'
    - '            raw_output = ret["response"]'
    - '            logging.info(f"raw output:\n{raw_output}")'
    - '            all_generations.append(raw_output)'
    - ''
    - '            edited_file, new_content = _post_process_multifile_repair('
    - '                raw_output,'
    - '                file_contents,'
    - '                file_loc_intervals,'
    - '                diff_format=args.diff_format,'
    - '            )'
    - ''
    - '            if new_content == "":'
    - '                prev_contents.append("")'
    - '                file_names.append("")'
    - '            else:'
    - '                prev_content = file_contents[edited_file]'
    - '                prev_contents.append(prev_content)'
    - '                file_names.append(edited_file)'
    - ''
    - '        counts.append(count)'
    - '        raw_outputs.append(raw_output)'
    - '        all_generations = [all_generations]'
    - '        prev_contents = [prev_contents]'
    - '        file_names = [file_names]'
    - ''
    - '        with open(args.output_file, "a") as f:'
    - '            f.write('
    - '                json.dumps('
    - '                    {'
    - '                        "instance_id": instance_id,'
    - '                        "raw_output": raw_outputs,'
    - '                        "all_generations": all_generations,'
    - '                        "try_count": counts,'
    - '                        "traj": traj,'
    - '                        "prev_content": prev_contents,'
    - '                        "file_names": file_names,'
    - '                    }'
    - '                )'
    - '                + "\n"'
    - '            )'
    - ''
    - ''
    - 'def post_process_raw_output(raw_output_text, file_contents, file_loc_intervals,
      args):'
    - '    git_diffs = ""'
    - '    raw_git_diffs = ""'
    - '    lint_success = False'
    - '    content = ""'
    - '    try:'
    - '        edited_file, new_content = _post_process_multifile_repair('
    - '            raw_output_text,'
    - '            file_contents,'
    - '            file_loc_intervals,'
    - '            diff_format=args.diff_format,'
    - '        )'
    - '        if edited_file in file_contents:'
    - '            content = file_contents[edited_file]'
    - ''
    - '            git_diff = fake_git_repo("playground", edited_file, content, new_content)'
    - ''
    - '            raw_git_diffs += "\n" + git_diff.replace('
    - '                "\ No newline at end of file\n", ""'
    - '            )'
    - ''
    - '            syntax_success = check_syntax(new_content)'
    - '            lint_success, prev_errors, errors = lint_code('
    - '                "playground", "test.py", new_content, file_contents[edited_file]'
    - '            )'
    - ''
    - '            differ_by_empty_lines = check_code_differ_by_just_empty_lines('
    - '                new_content, file_contents[edited_file]'
    - '            )'
    - ''
    - '            print(lint_success, prev_errors, errors, differ_by_empty_lines)'
    - ''
    - '            if syntax_success and not differ_by_empty_lines:'
    - '                git_diffs = raw_git_diffs'
    - '            else:'
    - '                git_diffs = ""  # no need to evaluate'
    - '        else:'
    - '            diff = list('
    - '                unified_diff('
    - '                    content.split("\n"),'
    - '                    new_content.split("\n"),'
    - '                    fromfile=edited_file,'
    - '                    tofile=edited_file,'
    - '                    lineterm="",'
    - '                )'
    - '            )'
    - '            print("Failed parsing diff!")'
    - '            print("\n".join(diff))'
    - '    except Exception as e:'
    - '        print(raw_output_text)'
    - '        print(e)'
    - ''
    - '    return git_diffs, raw_git_diffs, content'
    - ''
    - ''
    - 'def post_process_repair(args):'
    - '    """'
    - '    apply some diff formatting.'
    - '    """'
    - '    raw_outputs = load_jsonl(args.raw_output_file)'
    - '    locs = load_jsonl(args.loc_file)'
    - ''
    - '    for raw_output in raw_outputs:'
    - '        instance_id = raw_output["instance_id"]'
    - ''
    - '        if raw_output["raw_output"] == "":'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "model_name_or_path": "agentless",'
    - '                            "instance_id": instance_id,'
    - '                            "model_patch": "",'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - '            continue'
    - ''
    - '        if args.select_id == -1:'
    - '            # Use the last generation'
    - '            assert False, "not implemented for now"'
    - '        else:'
    - '            # Use the indexed generation'
    - '            generation_idx = args.select_id'
    - '            try:'
    - '                raw_output_text = raw_output["all_generations"][0][generation_idx]'
    - '                original_file_content = raw_output["prev_content"][0][generation_idx]'
    - '                pred_file = raw_output["file_names"][0][generation_idx]'
    - ''
    - '                pred_files = [loc for loc in locs if loc["instance_id"] ==
      instance_id]['
    - '                    0'
    - '                ]["found_files"][: args.top_n]'
    - ''
    - '                git_diffs = ""'
    - '                raw_git_diffs = ""'
    - '                if isinstance(raw_output["raw_output"], str):'
    - '                    # for backward compatibility'
    - '                    raw_output["raw_output"] = [raw_output["raw_output"]]'
    - ''
    - '                file_contents = {pred_file: original_file_content}'
    - ''
    - '                file_loc_intervals = dict()'
    - ''
    - '                loc = [loc for loc in locs if loc["instance_id"] == instance_id][0]'
    - ''
    - '                for i, tmp_pred_file in enumerate(pred_files):'
    - '                    if tmp_pred_file != pred_file:'
    - '                        continue'
    - '                    if "found_edit_locs" in loc and len(loc["found_edit_locs"])
      > i:'
    - '                        line_locs, context_intervals = transfer_arb_locs_to_locs('
    - '                            loc["found_edit_locs"][i],'
    - '                            None,'
    - '                            loc["found_files"][i],'
    - '                            args.context_window,'
    - '                            args.loc_interval,'
    - '                            args.fine_grain_loc_only,'
    - '                            file_content=file_contents[pred_file]'
    - '                            if pred_file in file_contents'
    - '                            else "",'
    - '                        )'
    - '                    else:'
    - '                        line_locs, context_intervals = [], []  # default values.'
    - ''
    - '                    file_loc_intervals[pred_file] = context_intervals'
    - '            except:'
    - '                raw_output_text = ""'
    - ''
    - '        if raw_output_text:'
    - '            git_diffs, raw_git_diffs, content = post_process_raw_output('
    - '                raw_output_text, file_contents, file_loc_intervals, args'
    - '            )'
    - '        else:'
    - '            git_diffs = ""'
    - '            raw_git_diffs = ""'
    - '            content = ""'
    - ''
    - '        with open(args.output_file, "a") as f:'
    - '            f.write('
    - '                json.dumps('
    - '                    {'
    - '                        "model_name_or_path": "agentless",'
    - '                        "instance_id": instance_id,'
    - '                        "model_patch": git_diffs.lstrip(),'
    - '                        "raw_model_patch": raw_git_diffs.lstrip(),'
    - '                        "original_file_content": content,'
    - '                    }'
    - '                )'
    - '                + "\n"'
    - '            )'
    - ''
    - ''
    - 'def main():'
    - '    parser = argparse.ArgumentParser()'
    - '    parser.add_argument("--loc_file", type=str, required=True)'
    - '    parser.add_argument("--top_n", type=int, default=1)'
    - '    parser.add_argument("--loc_interval", action="store_true")'
    - '    parser.add_argument("--context_window", type=int, default=10)'
    - '    parser.add_argument('
    - '        "--stop_at_n_unique_valid_samples",'
    - '        type=int,'
    - '        default=-1,'
    - '        help="Early stop when we get N unique valid samples, set to -1 if don''t
      want to do early stopping.",'
    - '    )'
    - '    parser.add_argument("--gen_and_process", action="store_true")'
    - '    parser.add_argument("--max_samples", type=int, default=20, help="Sampling
      budget.")'
    - '    parser.add_argument('
    - '        "--select_id",'
    - '        type=int,'
    - '        default=-1,'
    - '        help="Index the selected samples during post-processing.",'
    - '    )'
    - '    parser.add_argument('
    - '        "--model", type=str, default="gpt-4o-2024-05-13", choices=["gpt-4o-2024-05-13"]'
    - '    )'
    - '    parser.add_argument("--output_folder", type=str, required=True)'
    - '    parser.add_argument('
    - '        "--only_correct", action="store_true"'
    - '    )  # only work on correct loc files (saves time)'
    - '    parser.add_argument("--post_process", action="store_true")'
    - '    parser.add_argument("--add_space", action="store_true")'
    - '    parser.add_argument("--cot", action="store_true")'
    - '    parser.add_argument("--fine_grain_loc_only", action="store_true")'
    - '    parser.add_argument("--diff_format", action="store_true")'
    - '    parser.add_argument("--skip_greedy", action="store_true")'
    - '    parser.add_argument("--sticky_scroll", action="store_true")'
    - '    parser.add_argument('
    - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
    - '    )'
    - ''
    - '    args = parser.parse_args()'
    - ''
    - '    if not os.path.exists(args.output_folder):'
    - '        os.makedirs(args.output_folder)'
    - ''
    - '    args.output_file = os.path.join(args.output_folder, "output.jsonl")'
    - ''
    - '    if args.post_process:'
    - '        args.raw_output_file = args.output_file'
    - '        if args.select_id == -1:'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", "_processed.jsonl"'
    - '            )'
    - '        else:'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", f"_{args.select_id}_processed.jsonl"'
    - '            )'
    - '        post_process_repair(args)'
    - '    elif args.gen_and_process:'
    - '        repair(args)'
    - '        args.raw_output_file = args.output_file'
    - '        for i in range(args.max_samples):'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", f"_{i}_processed.jsonl"'
    - '            )'
    - '            args.select_id = i'
    - '            post_process_repair(args)'
    - '    else:'
    - '        repair(args)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    main()'
  repair_gpt.py:
    classes: []
    functions:
    - end_line: 198
      name: _post_process_multifile_repair
      start_line: 150
      text:
      - def _post_process_multifile_repair(
      - '    raw_output: str,'
      - '    file_contents: dict[str, str],'
      - '    file_loc_intervals: dict[str, list],'
      - '    diff_format=False,'
      - '):'
      - '    edit_multifile_commands = extract_python_blocks(raw_output)'
      - '    edited_file = ""'
      - '    new_content = ""'
      - '    try:'
      - '        file_to_commands = split_edit_multifile_commands('
      - '            edit_multifile_commands, diff_format=diff_format'
      - '        )'
      - '        logging.info("=== file_to_commands: ===")'
      - '        logging.info(json.dumps(file_to_commands, indent=2))'
      - '        # Let''s only edit the first file in the edit commands.'
      - '        edited_file_key = next(iter(file_to_commands.keys()))'
      - '        logging.info(f"=== edited_file: {edited_file_key} ===")'
      - '        edit_commands = file_to_commands[edited_file_key]'
      - '        logging.info("=== edit_commands: ===")'
      - '        for c in edit_commands:'
      - '            logging.info(c)'
      - '            logging.info("\n" + "-" * 40)'
      - '        edited_file = eval(edited_file_key)  # convert ''"file.py"'' to ''file.py'''
      - '        content = file_contents[edited_file]'
      - '        if diff_format:'
      - '            new_content = parse_diff_edit_commands('
      - '                edit_commands, content, file_loc_intervals[edited_file]'
      - '            )'
      - '        else:'
      - '            new_content = parse_edit_commands(edit_commands, content)'
      - '    except Exception as e:'
      - '        logging.error(e)'
      - '        return edited_file, new_content'
      - ''
      - '    diff = list('
      - '        unified_diff('
      - '            content.split("\n"),'
      - '            new_content.split("\n"),'
      - '            fromfile=edited_file,'
      - '            tofile=edited_file,'
      - '            lineterm="",'
      - '        )'
      - '    )'
      - ''
      - '    logging.info(f"extracted patch:")'
      - '    logging.info("\n".join(diff))'
      - '    print("\n".join(diff))'
      - '    return edited_file, new_content'
    - end_line: 244
      name: construct_topn_file_context
      start_line: 201
      text:
      - def construct_topn_file_context(
      - '    file_to_locs,'
      - '    pred_files,'
      - '    file_contents,'
      - '    structure,'
      - '    context_window: int,'
      - '    loc_interval: bool = True,'
      - '    fine_grain_loc_only: bool = False,'
      - '    add_space: bool = False,'
      - '    sticky_scroll: bool = False,'
      - '    no_line_number: bool = True,'
      - '):'
      - '    """Concatenate provided locations to form a context.'
      - ''
      - '    loc: {"file_name_1": ["loc_str_1"], ...}'
      - '    """'
      - '    file_loc_intervals = dict()'
      - '    topn_content = ""'
      - ''
      - '    for pred_file, locs in file_to_locs.items():'
      - '        content = file_contents[pred_file]'
      - '        line_locs, context_intervals = transfer_arb_locs_to_locs('
      - '            locs,'
      - '            structure,'
      - '            pred_file,'
      - '            context_window,'
      - '            loc_interval,'
      - '            fine_grain_loc_only,'
      - '            file_content=file_contents[pred_file] if pred_file in file_contents
        else "",'
      - '        )'
      - ''
      - '        if len(line_locs) > 0:'
      - '            # Note that if no location is predicted, we exclude this file.'
      - '            file_loc_content = line_wrap_content('
      - '                content,'
      - '                context_intervals,'
      - '                add_space=add_space,'
      - '                no_line_number=no_line_number,'
      - '                sticky_scroll=sticky_scroll,'
      - '            )'
      - '            topn_content += f"### {pred_file}\n{file_loc_content}\n\n\n"'
      - '            file_loc_intervals[pred_file] = context_intervals'
      - ''
      - '    return topn_content, file_loc_intervals'
    - end_line: 537
      name: repair
      start_line: 247
      text:
      - 'def repair(args):'
      - '    logging.basicConfig('
      - '        filename=f"{args.output_folder}/repair.log",'
      - '        level=logging.DEBUG,'
      - '        format="%(asctime)s - %(levelname)s - %(message)s",'
      - '    )'
      - ''
      - '    # write the arguments'
      - '    with open(f"{args.output_folder}/args.json", "w") as f:'
      - '        json.dump(vars(args), f, indent=4)'
      - ''
      - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
      - ''
      - '    locs = load_jsonl(args.loc_file)'
      - ''
      - '    if os.path.exists(args.output_file):'
      - '        prev_o = load_jsonl(args.output_file)'
      - '    else:'
      - '        prev_o = []'
      - ''
      - '    # make copy of loc in output_folder'
      - '    with open(f"{args.output_folder}/used_locs.jsonl", "w") as f:'
      - '        for loc in locs:'
      - '            f.write(json.dumps(loc) + "\n")'
      - ''
      - '    for loc in tqdm(locs):'
      - '        instance_id = loc["instance_id"]'
      - '        found = False'
      - '        for o in prev_o:'
      - '            if o["instance_id"] == instance_id:'
      - '                found = True'
      - '                break'
      - ''
      - '        if found:'
      - '            logging.info(f"skipping {instance_id} since patch already generated")'
      - '            continue'
      - ''
      - '        logging.info(f"================ repairing {instance_id} ================")'
      - ''
      - '        if len(loc["found_files"]) == 0:'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "instance_id": instance_id,'
      - '                            "raw_output": [""],'
      - '                            "try_count": [0],'
      - '                            "all_generations": [[]],'
      - '                            "traj": [],'
      - '                            "prev_content": [[]],'
      - '                            "file_names": [[]],'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - ''
      - '            logging.info(f"skipped since no files were localized")'
      - '            continue'
      - ''
      - '        pred_files = loc["found_files"][: args.top_n]'
      - ''
      - '        # grab buggy problem issue description and structure data'
      - ''
      - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
      - '        problem_statement = bench_data["problem_statement"]'
      - '        structure = get_repo_structure('
      - '            instance_id, bench_data["repo"], bench_data["base_commit"], "playground"'
      - '        )'
      - ''
      - '        files, _, _ = get_full_file_paths_and_classes_and_functions(structure)'
      - ''
      - '        raw_outputs, counts, all_generations, traj, prev_contents, file_names
        = ('
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '            [],'
      - '        )'
      - ''
      - '        raw_output = ""'
      - '        new_content = ""'
      - '        topn_content = ""'
      - '        # Construct file contents'
      - '        file_contents = dict()'
      - '        for i, pred_file in enumerate(pred_files):'
      - '            content = None'
      - ''
      - '            for file_content in files:'
      - '                if file_content[0] == pred_file:'
      - '                    content = "\n".join(file_content[1])'
      - '                    file_contents[pred_file] = content'
      - '                    break'
      - ''
      - '            assert content is not None, f"{pred_file} file not found"'
      - '        # Construct top-n file context'
      - '        file_to_edit_locs = dict()'
      - '        for i, pred_file in enumerate(pred_files):'
      - '            if "found_edit_locs" in loc and len(loc["found_edit_locs"]) >
        i:'
      - '                file_to_edit_locs[pred_file] = loc["found_edit_locs"][i]'
      - ''
      - '        topn_content, file_loc_intervals = construct_topn_file_context('
      - '            file_to_edit_locs,'
      - '            pred_files,'
      - '            file_contents,'
      - '            structure,'
      - '            context_window=args.context_window,'
      - '            loc_interval=args.loc_interval,'
      - '            fine_grain_loc_only=args.fine_grain_loc_only,'
      - '            add_space=args.add_space,'
      - '            no_line_number=args.diff_format,'
      - '            sticky_scroll=args.sticky_scroll,'
      - '        )'
      - ''
      - '        if topn_content.strip() == "":'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "instance_id": instance_id,'
      - '                            "raw_output": [""],'
      - '                            "try_count": [0],'
      - '                            "all_generations": [[]],'
      - '                            "traj": [],'
      - '                            "prev_content": [[]],'
      - '                            "file_names": [[]],'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - ''
      - '            logging.info(f"skipped since no files were localized")'
      - '            continue'
      - ''
      - '        # Construct prompt.'
      - '        # Note that we assume there''s no feedback, and we always use the
        same prompt in each turn.'
      - '        if args.cot and args.diff_format:'
      - '            prompt_template = repair_prompt_combine_topn_cot_diff'
      - '        elif args.cot:'
      - '            prompt_template = repair_prompt_combine_topn_cot'
      - '        else:'
      - '            prompt_template = repair_prompt_combine_topn'
      - ''
      - '        file_instruction = repair_relevant_file_instruction'
      - ''
      - '        message = prompt_template.format('
      - '            repair_relevant_file_instruction=file_instruction,'
      - '            problem_statement=problem_statement,'
      - '            content=topn_content.rstrip(),  # remove trailing newlines'
      - '        ).strip()'
      - ''
      - '        logging.info(f"prompting with message:\n{message}")'
      - ''
      - '        sample_responses = None'
      - ''
      - '        def get_response(count):'
      - '            nonlocal sample_responses'
      - '            if count == 0:'
      - '                if args.skip_greedy:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            ),'
      - '                        },'
      - '                    }'
      - '                config = create_chatgpt_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0,  # greedy first'
      - '                    batch_size=1,'
      - '                    model=args.model,  # use gpt-4o for now.'
      - '                )'
      - '            '
      - '                greedy_response = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": greedy_response.choices[0].message.content,'
      - '                    "usage": {'
      - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
      - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
      - '                    },'
      - '                }'
      - '            elif args.stop_at_n_unique_valid_samples == -1:'
      - '                # No early-stopping, let''s get all samples at a time'
      - '                assert args.max_samples > 1'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            )'
      - '                            if count == 1'
      - '                            else 0,'
      - '                        },'
      - '                    }'
      - '                if sample_responses is not None:'
      - '                    # Directly return earlier samples'
      - '                    return {'
      - '                        "response": sample_responses.choices[count - 1].message.content,'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                assert count == 1'
      - '                config = create_chatgpt_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0.8,'
      - '                    batch_size=args.max_samples - 1,  # minus the 1 greedy
        sample'
      - '                    model=args.model,  # use gpt-4o for now.'
      - '                )'
      - '                # config = create_codegeex_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0.8,'
      - '                # )'
      - '                # sample_responses = request_codegeex_engine(config)'
      - '                sample_responses = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": sample_responses.choices[count - 1].message.content,'
      - '                    "usage": {'
      - '                        "completion_tokens": sample_responses.usage.completion_tokens,'
      - '                        "prompt_tokens": sample_responses.usage.prompt_tokens,'
      - '                    },'
      - '                }'
      - ''
      - '        count = 0'
      - '        while count < args.max_samples:'
      - '            print(f"trying the {count + 1}-th sample ...")'
      - '            ret = get_response(count)'
      - '            count += 1'
      - '            traj.append('
      - '                {'
      - '                    **ret,'
      - '                    "prompt": message,'
      - '                }'
      - '            )'
      - ''
      - '            if args.mock:'
      - '                continue'
      - '            raw_output = ret["response"]'
      - '            logging.info(f"raw output:\n{raw_output}")'
      - '            all_generations.append(raw_output)'
      - ''
      - '            edited_file, new_content = _post_process_multifile_repair('
      - '                raw_output,'
      - '                file_contents,'
      - '                file_loc_intervals,'
      - '                diff_format=args.diff_format,'
      - '            )'
      - ''
      - '            if new_content == "":'
      - '                prev_contents.append("")'
      - '                file_names.append("")'
      - '            else:'
      - '                prev_content = file_contents[edited_file]'
      - '                prev_contents.append(prev_content)'
      - '                file_names.append(edited_file)'
      - ''
      - '        counts.append(count)'
      - '        raw_outputs.append(raw_output)'
      - '        all_generations = [all_generations]'
      - '        prev_contents = [prev_contents]'
      - '        file_names = [file_names]'
      - ''
      - '        with open(args.output_file, "a") as f:'
      - '            f.write('
      - '                json.dumps('
      - '                    {'
      - '                        "instance_id": instance_id,'
      - '                        "raw_output": raw_outputs,'
      - '                        "all_generations": all_generations,'
      - '                        "try_count": counts,'
      - '                        "traj": traj,'
      - '                        "prev_content": prev_contents,'
      - '                        "file_names": file_names,'
      - '                    }'
      - '                )'
      - '                + "\n"'
      - '            )'
    - end_line: 592
      name: post_process_raw_output
      start_line: 540
      text:
      - 'def post_process_raw_output(raw_output_text, file_contents, file_loc_intervals,
        args):'
      - '    git_diffs = ""'
      - '    raw_git_diffs = ""'
      - '    lint_success = False'
      - '    content = ""'
      - '    try:'
      - '        edited_file, new_content = _post_process_multifile_repair('
      - '            raw_output_text,'
      - '            file_contents,'
      - '            file_loc_intervals,'
      - '            diff_format=args.diff_format,'
      - '        )'
      - '        if edited_file in file_contents:'
      - '            content = file_contents[edited_file]'
      - ''
      - '            git_diff = fake_git_repo("playground", edited_file, content,
        new_content)'
      - ''
      - '            raw_git_diffs += "\n" + git_diff.replace('
      - '                "\ No newline at end of file\n", ""'
      - '            )'
      - ''
      - '            syntax_success = check_syntax(new_content)'
      - '            lint_success, prev_errors, errors = lint_code('
      - '                "playground", "test.py", new_content, file_contents[edited_file]'
      - '            )'
      - ''
      - '            differ_by_empty_lines = check_code_differ_by_just_empty_lines('
      - '                new_content, file_contents[edited_file]'
      - '            )'
      - ''
      - '            print(lint_success, prev_errors, errors, differ_by_empty_lines)'
      - ''
      - '            if syntax_success and not differ_by_empty_lines:'
      - '                git_diffs = raw_git_diffs'
      - '            else:'
      - '                git_diffs = ""  # no need to evaluate'
      - '        else:'
      - '            diff = list('
      - '                unified_diff('
      - '                    content.split("\n"),'
      - '                    new_content.split("\n"),'
      - '                    fromfile=edited_file,'
      - '                    tofile=edited_file,'
      - '                    lineterm="",'
      - '                )'
      - '            )'
      - '            print("Failed parsing diff!")'
      - '            print("\n".join(diff))'
      - '    except Exception as e:'
      - '        print(raw_output_text)'
      - '        print(e)'
      - ''
      - '    return git_diffs, raw_git_diffs, content'
    - end_line: 689
      name: post_process_repair
      start_line: 595
      text:
      - 'def post_process_repair(args):'
      - '    """'
      - '    apply some diff formatting.'
      - '    """'
      - '    raw_outputs = load_jsonl(args.raw_output_file)'
      - '    locs = load_jsonl(args.loc_file)'
      - ''
      - '    for raw_output in raw_outputs:'
      - '        instance_id = raw_output["instance_id"]'
      - ''
      - '        if raw_output["raw_output"] == "":'
      - '            with open(args.output_file, "a") as f:'
      - '                f.write('
      - '                    json.dumps('
      - '                        {'
      - '                            "model_name_or_path": "agentless",'
      - '                            "instance_id": instance_id,'
      - '                            "model_patch": "",'
      - '                        }'
      - '                    )'
      - '                    + "\n"'
      - '                )'
      - '            continue'
      - ''
      - '        if args.select_id == -1:'
      - '            # Use the last generation'
      - '            assert False, "not implemented for now"'
      - '        else:'
      - '            # Use the indexed generation'
      - '            generation_idx = args.select_id'
      - '            try:'
      - '                raw_output_text = raw_output["all_generations"][0][generation_idx]'
      - '                original_file_content = raw_output["prev_content"][0][generation_idx]'
      - '                pred_file = raw_output["file_names"][0][generation_idx]'
      - ''
      - '                pred_files = [loc for loc in locs if loc["instance_id"] ==
        instance_id]['
      - '                    0'
      - '                ]["found_files"][: args.top_n]'
      - ''
      - '                git_diffs = ""'
      - '                raw_git_diffs = ""'
      - '                if isinstance(raw_output["raw_output"], str):'
      - '                    # for backward compatibility'
      - '                    raw_output["raw_output"] = [raw_output["raw_output"]]'
      - ''
      - '                file_contents = {pred_file: original_file_content}'
      - ''
      - '                file_loc_intervals = dict()'
      - ''
      - '                loc = [loc for loc in locs if loc["instance_id"] == instance_id][0]'
      - ''
      - '                for i, tmp_pred_file in enumerate(pred_files):'
      - '                    if tmp_pred_file != pred_file:'
      - '                        continue'
      - '                    if "found_edit_locs" in loc and len(loc["found_edit_locs"])
        > i:'
      - '                        line_locs, context_intervals = transfer_arb_locs_to_locs('
      - '                            loc["found_edit_locs"][i],'
      - '                            None,'
      - '                            loc["found_files"][i],'
      - '                            args.context_window,'
      - '                            args.loc_interval,'
      - '                            args.fine_grain_loc_only,'
      - '                            file_content=file_contents[pred_file]'
      - '                            if pred_file in file_contents'
      - '                            else "",'
      - '                        )'
      - '                    else:'
      - '                        line_locs, context_intervals = [], []  # default
        values.'
      - ''
      - '                    file_loc_intervals[pred_file] = context_intervals'
      - '            except:'
      - '                raw_output_text = ""'
      - ''
      - '        if raw_output_text:'
      - '            git_diffs, raw_git_diffs, content = post_process_raw_output('
      - '                raw_output_text, file_contents, file_loc_intervals, args'
      - '            )'
      - '        else:'
      - '            git_diffs = ""'
      - '            raw_git_diffs = ""'
      - '            content = ""'
      - ''
      - '        with open(args.output_file, "a") as f:'
      - '            f.write('
      - '                json.dumps('
      - '                    {'
      - '                        "model_name_or_path": "agentless",'
      - '                        "instance_id": instance_id,'
      - '                        "model_patch": git_diffs.lstrip(),'
      - '                        "raw_model_patch": raw_git_diffs.lstrip(),'
      - '                        "original_file_content": content,'
      - '                    }'
      - '                )'
      - '                + "\n"'
      - '            )'
    - end_line: 758
      name: main
      start_line: 692
      text:
      - 'def main():'
      - '    parser = argparse.ArgumentParser()'
      - '    parser.add_argument("--loc_file", type=str, required=True)'
      - '    parser.add_argument("--top_n", type=int, default=1)'
      - '    parser.add_argument("--loc_interval", action="store_true")'
      - '    parser.add_argument("--context_window", type=int, default=10)'
      - '    parser.add_argument('
      - '        "--stop_at_n_unique_valid_samples",'
      - '        type=int,'
      - '        default=-1,'
      - '        help="Early stop when we get N unique valid samples, set to -1 if
        don''t want to do early stopping.",'
      - '    )'
      - '    parser.add_argument("--gen_and_process", action="store_true")'
      - '    parser.add_argument("--max_samples", type=int, default=20, help="Sampling
        budget.")'
      - '    parser.add_argument('
      - '        "--select_id",'
      - '        type=int,'
      - '        default=-1,'
      - '        help="Index the selected samples during post-processing.",'
      - '    )'
      - '    parser.add_argument('
      - '        "--model", type=str, default="gpt-4o-2024-05-13", choices=["gpt-4o-2024-05-13"]'
      - '    )'
      - '    parser.add_argument("--output_folder", type=str, required=True)'
      - '    parser.add_argument('
      - '        "--only_correct", action="store_true"'
      - '    )  # only work on correct loc files (saves time)'
      - '    parser.add_argument("--post_process", action="store_true")'
      - '    parser.add_argument("--add_space", action="store_true")'
      - '    parser.add_argument("--cot", action="store_true")'
      - '    parser.add_argument("--fine_grain_loc_only", action="store_true")'
      - '    parser.add_argument("--diff_format", action="store_true")'
      - '    parser.add_argument("--skip_greedy", action="store_true")'
      - '    parser.add_argument("--sticky_scroll", action="store_true")'
      - '    parser.add_argument('
      - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
      - '    )'
      - ''
      - '    args = parser.parse_args()'
      - ''
      - '    if not os.path.exists(args.output_folder):'
      - '        os.makedirs(args.output_folder)'
      - ''
      - '    args.output_file = os.path.join(args.output_folder, "output.jsonl")'
      - ''
      - '    if args.post_process:'
      - '        args.raw_output_file = args.output_file'
      - '        if args.select_id == -1:'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", "_processed.jsonl"'
      - '            )'
      - '        else:'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", f"_{args.select_id}_processed.jsonl"'
      - '            )'
      - '        post_process_repair(args)'
      - '    elif args.gen_and_process:'
      - '        repair(args)'
      - '        args.raw_output_file = args.output_file'
      - '        for i in range(args.max_samples):'
      - '            args.output_file = args.raw_output_file.replace('
      - '                ".jsonl", f"_{i}_processed.jsonl"'
      - '            )'
      - '            args.select_id = i'
      - '            post_process_repair(args)'
      - '    else:'
      - '        repair(args)'
    - end_line: 482
      name: get_response
      start_line: 402
      text:
      - '        def get_response(count):'
      - '            nonlocal sample_responses'
      - '            if count == 0:'
      - '                if args.skip_greedy:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            ),'
      - '                        },'
      - '                    }'
      - '                config = create_chatgpt_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0,  # greedy first'
      - '                    batch_size=1,'
      - '                    model=args.model,  # use gpt-4o for now.'
      - '                )'
      - '            '
      - '                greedy_response = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": greedy_response.choices[0].message.content,'
      - '                    "usage": {'
      - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
      - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
      - '                    },'
      - '                }'
      - '            elif args.stop_at_n_unique_valid_samples == -1:'
      - '                # No early-stopping, let''s get all samples at a time'
      - '                assert args.max_samples > 1'
      - '                if args.mock:'
      - '                    return {'
      - '                        "response": "",'
      - '                        "usage": {'
      - '                            "prompt_tokens": num_tokens_from_messages('
      - '                                message, "gpt-4o-2024-05-13"'
      - '                            )'
      - '                            if count == 1'
      - '                            else 0,'
      - '                        },'
      - '                    }'
      - '                if sample_responses is not None:'
      - '                    # Directly return earlier samples'
      - '                    return {'
      - '                        "response": sample_responses.choices[count - 1].message.content,'
      - '                        "usage": {'
      - '                            "completion_tokens": 0,'
      - '                            "prompt_tokens": 0,'
      - '                        },'
      - '                    }'
      - '                assert count == 1'
      - '                config = create_chatgpt_config('
      - '                    message=message,'
      - '                    max_tokens=1024,'
      - '                    temperature=0.8,'
      - '                    batch_size=args.max_samples - 1,  # minus the 1 greedy
        sample'
      - '                    model=args.model,  # use gpt-4o for now.'
      - '                )'
      - '                # config = create_codegeex_config('
      - '                #     message=message,'
      - '                #     max_tokens=1024,'
      - '                #     temperature=0.8,'
      - '                # )'
      - '                # sample_responses = request_codegeex_engine(config)'
      - '                sample_responses = request_chatgpt_engine(config)'
      - '                return {'
      - '                    "response": sample_responses.choices[count - 1].message.content,'
      - '                    "usage": {'
      - '                        "completion_tokens": sample_responses.usage.completion_tokens,'
      - '                        "prompt_tokens": sample_responses.usage.prompt_tokens,'
      - '                    },'
      - '                }'
    text:
    - import argparse
    - import copy
    - import json
    - import logging
    - import os
    - from difflib import unified_diff
    - ''
    - from datasets import load_dataset
    - from tqdm import tqdm
    - ''
    - from agentless.util.api_requests import (
    - '    create_chatgpt_config,'
    - '    num_tokens_from_messages,'
    - '    request_chatgpt_engine,'
    - )
    - ''
    - from agentless.util.api_requests import (
    - "\tcreate_codegeex_config,"
    - "\tnum_tokens_from_messages,"
    - "\trequest_codegeex_engine"
    - )
    - ''
    - from agentless.util.postprocess_data import (
    - '    check_code_differ_by_just_empty_lines,'
    - '    check_syntax,'
    - '    extract_python_blocks,'
    - '    fake_git_repo,'
    - '    lint_code,'
    - '    parse_diff_edit_commands,'
    - '    parse_edit_commands,'
    - '    remove_empty_lines,'
    - '    split_edit_multifile_commands,'
    - )
    - from agentless.util.preprocess_data import (
    - '    get_full_file_paths_and_classes_and_functions,'
    - '    get_repo_structure,'
    - '    line_wrap_content,'
    - '    transfer_arb_locs_to_locs,'
    - )
    - from agentless.util.utils import load_jsonl
    - ''
    - repair_relevant_file_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs.
    - '"""'
    - repair_relevant_file_with_scope_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs.
    - In the file below, "..." refers to some less relevant content being omited for
      brebity.
    - '"""'
    - with_scope_explanation = """
    - Note that "..." refers to some omited content that is not actually in the files.
      Your *SEARCH/REPLACE* edit must not contain such "...".
    - '"""'
    - repair_relevant_file_with_suspicious_loc_instruction = """
    - Below are some code segments, each from a relevant file. One or more of these
      files may contain bugs. Some suspicious locations are provided for closer inspection.
    - '"""'
    - repair_prompt_combine_topn = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please generate `edit_file` commands to fix the issue.
    - ''
    - 'The `edit_file` command takes four arguments:'
    - ''
    - 'edit_file(filename: str, start: int, end: int, content: str) -> None:'
    - '    Edit a file. It replaces lines `start` through `end` (inclusive) with the
      given text `content` in the open file.'
    - '    Args:'
    - '    filename: str: The full file name to edit.'
    - '    start: int: The start line number. Must satisfy start >= 1.'
    - '    end: int: The end line number. Must satisfy start <= end <= number of lines
      in the file.'
    - '    content: str: The content to replace the lines with.'
    - ''
    - Please note that THE `edit_file` FUNCTION REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the `edit_file` command in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - repair_prompt_combine_topn_cot = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please first localize the bug based on the issue statement, and then generate
      `edit_file` commands to fix the issue.
    - ''
    - 'The `edit_file` command takes four arguments:'
    - ''
    - 'edit_file(filename: str, start: int, end: int, content: str) -> None:'
    - '    Edit a file. It replaces lines `start` through `end` (inclusive) with the
      given text `content` in the open file.'
    - '    Args:'
    - '    filename: str: The full file name to edit.'
    - '    start: int: The start line number. Must satisfy start >= 1.'
    - '    end: int: The end line number. Must satisfy start <= end <= number of lines
      in the file.'
    - '    content: str: The content to replace the lines with.'
    - ''
    - Please note that THE `edit_file` FUNCTION REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the `edit_file` command in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - repair_prompt_combine_topn_cot_diff = """
    - 'We are currently solving the following issue within our repository. Here is
      the issue text:'
    - '--- BEGIN ISSUE ---'
    - '{problem_statement}'
    - '--- END ISSUE ---'
    - ''
    - '{repair_relevant_file_instruction}'
    - '--- BEGIN FILE ---'
    - '```'
    - '{content}'
    - '```'
    - '--- END FILE ---'
    - ''
    - Please first localize the bug based on the issue statement, and then generate
      *SEARCH/REPLACE* edits to fix the issue.
    - ''
    - 'Every *SEARCH/REPLACE* edit must use this format:'
    - 1. The file path
    - '2. The start of search block: <<<<<<< SEARCH'
    - 3. A contiguous chunk of lines to search for in the existing source code
    - '4. The dividing line: ======='
    - 5. The lines to replace into the source code
    - '6. The end of the replace block: >>>>>>> REPLACE'
    - ''
    - 'Here is an example:'
    - ''
    - '```python'
    - '### mathweb/flask/app.py'
    - from flask import Flask
    - '```'
    - ''
    - Please note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you
      would like to add the line '        print(x)', you must fully write that out,
      with all those spaces before the code!
    - Wrap the *SEARCH/REPLACE* edit in blocks ```python...```.
    - '"""'
    - ''
    - ''
    - def _post_process_multifile_repair(
    - '    raw_output: str,'
    - '    file_contents: dict[str, str],'
    - '    file_loc_intervals: dict[str, list],'
    - '    diff_format=False,'
    - '):'
    - '    edit_multifile_commands = extract_python_blocks(raw_output)'
    - '    edited_file = ""'
    - '    new_content = ""'
    - '    try:'
    - '        file_to_commands = split_edit_multifile_commands('
    - '            edit_multifile_commands, diff_format=diff_format'
    - '        )'
    - '        logging.info("=== file_to_commands: ===")'
    - '        logging.info(json.dumps(file_to_commands, indent=2))'
    - '        # Let''s only edit the first file in the edit commands.'
    - '        edited_file_key = next(iter(file_to_commands.keys()))'
    - '        logging.info(f"=== edited_file: {edited_file_key} ===")'
    - '        edit_commands = file_to_commands[edited_file_key]'
    - '        logging.info("=== edit_commands: ===")'
    - '        for c in edit_commands:'
    - '            logging.info(c)'
    - '            logging.info("\n" + "-" * 40)'
    - '        edited_file = eval(edited_file_key)  # convert ''"file.py"'' to ''file.py'''
    - '        content = file_contents[edited_file]'
    - '        if diff_format:'
    - '            new_content = parse_diff_edit_commands('
    - '                edit_commands, content, file_loc_intervals[edited_file]'
    - '            )'
    - '        else:'
    - '            new_content = parse_edit_commands(edit_commands, content)'
    - '    except Exception as e:'
    - '        logging.error(e)'
    - '        return edited_file, new_content'
    - ''
    - '    diff = list('
    - '        unified_diff('
    - '            content.split("\n"),'
    - '            new_content.split("\n"),'
    - '            fromfile=edited_file,'
    - '            tofile=edited_file,'
    - '            lineterm="",'
    - '        )'
    - '    )'
    - ''
    - '    logging.info(f"extracted patch:")'
    - '    logging.info("\n".join(diff))'
    - '    print("\n".join(diff))'
    - '    return edited_file, new_content'
    - ''
    - ''
    - def construct_topn_file_context(
    - '    file_to_locs,'
    - '    pred_files,'
    - '    file_contents,'
    - '    structure,'
    - '    context_window: int,'
    - '    loc_interval: bool = True,'
    - '    fine_grain_loc_only: bool = False,'
    - '    add_space: bool = False,'
    - '    sticky_scroll: bool = False,'
    - '    no_line_number: bool = True,'
    - '):'
    - '    """Concatenate provided locations to form a context.'
    - ''
    - '    loc: {"file_name_1": ["loc_str_1"], ...}'
    - '    """'
    - '    file_loc_intervals = dict()'
    - '    topn_content = ""'
    - ''
    - '    for pred_file, locs in file_to_locs.items():'
    - '        content = file_contents[pred_file]'
    - '        line_locs, context_intervals = transfer_arb_locs_to_locs('
    - '            locs,'
    - '            structure,'
    - '            pred_file,'
    - '            context_window,'
    - '            loc_interval,'
    - '            fine_grain_loc_only,'
    - '            file_content=file_contents[pred_file] if pred_file in file_contents
      else "",'
    - '        )'
    - ''
    - '        if len(line_locs) > 0:'
    - '            # Note that if no location is predicted, we exclude this file.'
    - '            file_loc_content = line_wrap_content('
    - '                content,'
    - '                context_intervals,'
    - '                add_space=add_space,'
    - '                no_line_number=no_line_number,'
    - '                sticky_scroll=sticky_scroll,'
    - '            )'
    - '            topn_content += f"### {pred_file}\n{file_loc_content}\n\n\n"'
    - '            file_loc_intervals[pred_file] = context_intervals'
    - ''
    - '    return topn_content, file_loc_intervals'
    - ''
    - ''
    - 'def repair(args):'
    - '    logging.basicConfig('
    - '        filename=f"{args.output_folder}/repair.log",'
    - '        level=logging.DEBUG,'
    - '        format="%(asctime)s - %(levelname)s - %(message)s",'
    - '    )'
    - ''
    - '    # write the arguments'
    - '    with open(f"{args.output_folder}/args.json", "w") as f:'
    - '        json.dump(vars(args), f, indent=4)'
    - ''
    - '    swe_bench_data = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")'
    - ''
    - '    locs = load_jsonl(args.loc_file)'
    - ''
    - '    if os.path.exists(args.output_file):'
    - '        prev_o = load_jsonl(args.output_file)'
    - '    else:'
    - '        prev_o = []'
    - ''
    - '    # make copy of loc in output_folder'
    - '    with open(f"{args.output_folder}/used_locs.jsonl", "w") as f:'
    - '        for loc in locs:'
    - '            f.write(json.dumps(loc) + "\n")'
    - ''
    - '    for loc in tqdm(locs):'
    - '        instance_id = loc["instance_id"]'
    - '        found = False'
    - '        for o in prev_o:'
    - '            if o["instance_id"] == instance_id:'
    - '                found = True'
    - '                break'
    - ''
    - '        if found:'
    - '            logging.info(f"skipping {instance_id} since patch already generated")'
    - '            continue'
    - ''
    - '        logging.info(f"================ repairing {instance_id} ================")'
    - ''
    - '        if len(loc["found_files"]) == 0:'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "instance_id": instance_id,'
    - '                            "raw_output": [""],'
    - '                            "try_count": [0],'
    - '                            "all_generations": [[]],'
    - '                            "traj": [],'
    - '                            "prev_content": [[]],'
    - '                            "file_names": [[]],'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - ''
    - '            logging.info(f"skipped since no files were localized")'
    - '            continue'
    - ''
    - '        pred_files = loc["found_files"][: args.top_n]'
    - ''
    - '        # grab buggy problem issue description and structure data'
    - ''
    - '        bench_data = [x for x in swe_bench_data if x["instance_id"] == instance_id][0]'
    - '        problem_statement = bench_data["problem_statement"]'
    - '        structure = get_repo_structure('
    - '            instance_id, bench_data["repo"], bench_data["base_commit"], "playground"'
    - '        )'
    - ''
    - '        files, _, _ = get_full_file_paths_and_classes_and_functions(structure)'
    - ''
    - '        raw_outputs, counts, all_generations, traj, prev_contents, file_names
      = ('
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '            [],'
    - '        )'
    - ''
    - '        raw_output = ""'
    - '        new_content = ""'
    - '        topn_content = ""'
    - '        # Construct file contents'
    - '        file_contents = dict()'
    - '        for i, pred_file in enumerate(pred_files):'
    - '            content = None'
    - ''
    - '            for file_content in files:'
    - '                if file_content[0] == pred_file:'
    - '                    content = "\n".join(file_content[1])'
    - '                    file_contents[pred_file] = content'
    - '                    break'
    - ''
    - '            assert content is not None, f"{pred_file} file not found"'
    - '        # Construct top-n file context'
    - '        file_to_edit_locs = dict()'
    - '        for i, pred_file in enumerate(pred_files):'
    - '            if "found_edit_locs" in loc and len(loc["found_edit_locs"]) > i:'
    - '                file_to_edit_locs[pred_file] = loc["found_edit_locs"][i]'
    - ''
    - '        topn_content, file_loc_intervals = construct_topn_file_context('
    - '            file_to_edit_locs,'
    - '            pred_files,'
    - '            file_contents,'
    - '            structure,'
    - '            context_window=args.context_window,'
    - '            loc_interval=args.loc_interval,'
    - '            fine_grain_loc_only=args.fine_grain_loc_only,'
    - '            add_space=args.add_space,'
    - '            no_line_number=args.diff_format,'
    - '            sticky_scroll=args.sticky_scroll,'
    - '        )'
    - ''
    - '        if topn_content.strip() == "":'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "instance_id": instance_id,'
    - '                            "raw_output": [""],'
    - '                            "try_count": [0],'
    - '                            "all_generations": [[]],'
    - '                            "traj": [],'
    - '                            "prev_content": [[]],'
    - '                            "file_names": [[]],'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - ''
    - '            logging.info(f"skipped since no files were localized")'
    - '            continue'
    - ''
    - '        # Construct prompt.'
    - '        # Note that we assume there''s no feedback, and we always use the same
      prompt in each turn.'
    - '        if args.cot and args.diff_format:'
    - '            prompt_template = repair_prompt_combine_topn_cot_diff'
    - '        elif args.cot:'
    - '            prompt_template = repair_prompt_combine_topn_cot'
    - '        else:'
    - '            prompt_template = repair_prompt_combine_topn'
    - ''
    - '        file_instruction = repair_relevant_file_instruction'
    - ''
    - '        message = prompt_template.format('
    - '            repair_relevant_file_instruction=file_instruction,'
    - '            problem_statement=problem_statement,'
    - '            content=topn_content.rstrip(),  # remove trailing newlines'
    - '        ).strip()'
    - ''
    - '        logging.info(f"prompting with message:\n{message}")'
    - ''
    - '        sample_responses = None'
    - ''
    - '        def get_response(count):'
    - '            nonlocal sample_responses'
    - '            if count == 0:'
    - '                if args.skip_greedy:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "completion_tokens": 0,'
    - '                            "prompt_tokens": 0,'
    - '                        },'
    - '                    }'
    - '                if args.mock:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "prompt_tokens": num_tokens_from_messages('
    - '                                message, "gpt-4o-2024-05-13"'
    - '                            ),'
    - '                        },'
    - '                    }'
    - '                config = create_chatgpt_config('
    - '                    message=message,'
    - '                    max_tokens=1024,'
    - '                    temperature=0,  # greedy first'
    - '                    batch_size=1,'
    - '                    model=args.model,  # use gpt-4o for now.'
    - '                )'
    - '            '
    - '                greedy_response = request_chatgpt_engine(config)'
    - '                return {'
    - '                    "response": greedy_response.choices[0].message.content,'
    - '                    "usage": {'
    - '                        "completion_tokens": greedy_response.usage.completion_tokens,'
    - '                        "prompt_tokens": greedy_response.usage.prompt_tokens,'
    - '                    },'
    - '                }'
    - '            elif args.stop_at_n_unique_valid_samples == -1:'
    - '                # No early-stopping, let''s get all samples at a time'
    - '                assert args.max_samples > 1'
    - '                if args.mock:'
    - '                    return {'
    - '                        "response": "",'
    - '                        "usage": {'
    - '                            "prompt_tokens": num_tokens_from_messages('
    - '                                message, "gpt-4o-2024-05-13"'
    - '                            )'
    - '                            if count == 1'
    - '                            else 0,'
    - '                        },'
    - '                    }'
    - '                if sample_responses is not None:'
    - '                    # Directly return earlier samples'
    - '                    return {'
    - '                        "response": sample_responses.choices[count - 1].message.content,'
    - '                        "usage": {'
    - '                            "completion_tokens": 0,'
    - '                            "prompt_tokens": 0,'
    - '                        },'
    - '                    }'
    - '                assert count == 1'
    - '                config = create_chatgpt_config('
    - '                    message=message,'
    - '                    max_tokens=1024,'
    - '                    temperature=0.8,'
    - '                    batch_size=args.max_samples - 1,  # minus the 1 greedy
      sample'
    - '                    model=args.model,  # use gpt-4o for now.'
    - '                )'
    - '                # config = create_codegeex_config('
    - '                #     message=message,'
    - '                #     max_tokens=1024,'
    - '                #     temperature=0.8,'
    - '                # )'
    - '                # sample_responses = request_codegeex_engine(config)'
    - '                sample_responses = request_chatgpt_engine(config)'
    - '                return {'
    - '                    "response": sample_responses.choices[count - 1].message.content,'
    - '                    "usage": {'
    - '                        "completion_tokens": sample_responses.usage.completion_tokens,'
    - '                        "prompt_tokens": sample_responses.usage.prompt_tokens,'
    - '                    },'
    - '                }'
    - ''
    - '        count = 0'
    - '        while count < args.max_samples:'
    - '            print(f"trying the {count + 1}-th sample ...")'
    - '            ret = get_response(count)'
    - '            count += 1'
    - '            traj.append('
    - '                {'
    - '                    **ret,'
    - '                    "prompt": message,'
    - '                }'
    - '            )'
    - ''
    - '            if args.mock:'
    - '                continue'
    - '            raw_output = ret["response"]'
    - '            logging.info(f"raw output:\n{raw_output}")'
    - '            all_generations.append(raw_output)'
    - ''
    - '            edited_file, new_content = _post_process_multifile_repair('
    - '                raw_output,'
    - '                file_contents,'
    - '                file_loc_intervals,'
    - '                diff_format=args.diff_format,'
    - '            )'
    - ''
    - '            if new_content == "":'
    - '                prev_contents.append("")'
    - '                file_names.append("")'
    - '            else:'
    - '                prev_content = file_contents[edited_file]'
    - '                prev_contents.append(prev_content)'
    - '                file_names.append(edited_file)'
    - ''
    - '        counts.append(count)'
    - '        raw_outputs.append(raw_output)'
    - '        all_generations = [all_generations]'
    - '        prev_contents = [prev_contents]'
    - '        file_names = [file_names]'
    - ''
    - '        with open(args.output_file, "a") as f:'
    - '            f.write('
    - '                json.dumps('
    - '                    {'
    - '                        "instance_id": instance_id,'
    - '                        "raw_output": raw_outputs,'
    - '                        "all_generations": all_generations,'
    - '                        "try_count": counts,'
    - '                        "traj": traj,'
    - '                        "prev_content": prev_contents,'
    - '                        "file_names": file_names,'
    - '                    }'
    - '                )'
    - '                + "\n"'
    - '            )'
    - ''
    - ''
    - 'def post_process_raw_output(raw_output_text, file_contents, file_loc_intervals,
      args):'
    - '    git_diffs = ""'
    - '    raw_git_diffs = ""'
    - '    lint_success = False'
    - '    content = ""'
    - '    try:'
    - '        edited_file, new_content = _post_process_multifile_repair('
    - '            raw_output_text,'
    - '            file_contents,'
    - '            file_loc_intervals,'
    - '            diff_format=args.diff_format,'
    - '        )'
    - '        if edited_file in file_contents:'
    - '            content = file_contents[edited_file]'
    - ''
    - '            git_diff = fake_git_repo("playground", edited_file, content, new_content)'
    - ''
    - '            raw_git_diffs += "\n" + git_diff.replace('
    - '                "\ No newline at end of file\n", ""'
    - '            )'
    - ''
    - '            syntax_success = check_syntax(new_content)'
    - '            lint_success, prev_errors, errors = lint_code('
    - '                "playground", "test.py", new_content, file_contents[edited_file]'
    - '            )'
    - ''
    - '            differ_by_empty_lines = check_code_differ_by_just_empty_lines('
    - '                new_content, file_contents[edited_file]'
    - '            )'
    - ''
    - '            print(lint_success, prev_errors, errors, differ_by_empty_lines)'
    - ''
    - '            if syntax_success and not differ_by_empty_lines:'
    - '                git_diffs = raw_git_diffs'
    - '            else:'
    - '                git_diffs = ""  # no need to evaluate'
    - '        else:'
    - '            diff = list('
    - '                unified_diff('
    - '                    content.split("\n"),'
    - '                    new_content.split("\n"),'
    - '                    fromfile=edited_file,'
    - '                    tofile=edited_file,'
    - '                    lineterm="",'
    - '                )'
    - '            )'
    - '            print("Failed parsing diff!")'
    - '            print("\n".join(diff))'
    - '    except Exception as e:'
    - '        print(raw_output_text)'
    - '        print(e)'
    - ''
    - '    return git_diffs, raw_git_diffs, content'
    - ''
    - ''
    - 'def post_process_repair(args):'
    - '    """'
    - '    apply some diff formatting.'
    - '    """'
    - '    raw_outputs = load_jsonl(args.raw_output_file)'
    - '    locs = load_jsonl(args.loc_file)'
    - ''
    - '    for raw_output in raw_outputs:'
    - '        instance_id = raw_output["instance_id"]'
    - ''
    - '        if raw_output["raw_output"] == "":'
    - '            with open(args.output_file, "a") as f:'
    - '                f.write('
    - '                    json.dumps('
    - '                        {'
    - '                            "model_name_or_path": "agentless",'
    - '                            "instance_id": instance_id,'
    - '                            "model_patch": "",'
    - '                        }'
    - '                    )'
    - '                    + "\n"'
    - '                )'
    - '            continue'
    - ''
    - '        if args.select_id == -1:'
    - '            # Use the last generation'
    - '            assert False, "not implemented for now"'
    - '        else:'
    - '            # Use the indexed generation'
    - '            generation_idx = args.select_id'
    - '            try:'
    - '                raw_output_text = raw_output["all_generations"][0][generation_idx]'
    - '                original_file_content = raw_output["prev_content"][0][generation_idx]'
    - '                pred_file = raw_output["file_names"][0][generation_idx]'
    - ''
    - '                pred_files = [loc for loc in locs if loc["instance_id"] ==
      instance_id]['
    - '                    0'
    - '                ]["found_files"][: args.top_n]'
    - ''
    - '                git_diffs = ""'
    - '                raw_git_diffs = ""'
    - '                if isinstance(raw_output["raw_output"], str):'
    - '                    # for backward compatibility'
    - '                    raw_output["raw_output"] = [raw_output["raw_output"]]'
    - ''
    - '                file_contents = {pred_file: original_file_content}'
    - ''
    - '                file_loc_intervals = dict()'
    - ''
    - '                loc = [loc for loc in locs if loc["instance_id"] == instance_id][0]'
    - ''
    - '                for i, tmp_pred_file in enumerate(pred_files):'
    - '                    if tmp_pred_file != pred_file:'
    - '                        continue'
    - '                    if "found_edit_locs" in loc and len(loc["found_edit_locs"])
      > i:'
    - '                        line_locs, context_intervals = transfer_arb_locs_to_locs('
    - '                            loc["found_edit_locs"][i],'
    - '                            None,'
    - '                            loc["found_files"][i],'
    - '                            args.context_window,'
    - '                            args.loc_interval,'
    - '                            args.fine_grain_loc_only,'
    - '                            file_content=file_contents[pred_file]'
    - '                            if pred_file in file_contents'
    - '                            else "",'
    - '                        )'
    - '                    else:'
    - '                        line_locs, context_intervals = [], []  # default values.'
    - ''
    - '                    file_loc_intervals[pred_file] = context_intervals'
    - '            except:'
    - '                raw_output_text = ""'
    - ''
    - '        if raw_output_text:'
    - '            git_diffs, raw_git_diffs, content = post_process_raw_output('
    - '                raw_output_text, file_contents, file_loc_intervals, args'
    - '            )'
    - '        else:'
    - '            git_diffs = ""'
    - '            raw_git_diffs = ""'
    - '            content = ""'
    - ''
    - '        with open(args.output_file, "a") as f:'
    - '            f.write('
    - '                json.dumps('
    - '                    {'
    - '                        "model_name_or_path": "agentless",'
    - '                        "instance_id": instance_id,'
    - '                        "model_patch": git_diffs.lstrip(),'
    - '                        "raw_model_patch": raw_git_diffs.lstrip(),'
    - '                        "original_file_content": content,'
    - '                    }'
    - '                )'
    - '                + "\n"'
    - '            )'
    - ''
    - ''
    - 'def main():'
    - '    parser = argparse.ArgumentParser()'
    - '    parser.add_argument("--loc_file", type=str, required=True)'
    - '    parser.add_argument("--top_n", type=int, default=1)'
    - '    parser.add_argument("--loc_interval", action="store_true")'
    - '    parser.add_argument("--context_window", type=int, default=10)'
    - '    parser.add_argument('
    - '        "--stop_at_n_unique_valid_samples",'
    - '        type=int,'
    - '        default=-1,'
    - '        help="Early stop when we get N unique valid samples, set to -1 if don''t
      want to do early stopping.",'
    - '    )'
    - '    parser.add_argument("--gen_and_process", action="store_true")'
    - '    parser.add_argument("--max_samples", type=int, default=20, help="Sampling
      budget.")'
    - '    parser.add_argument('
    - '        "--select_id",'
    - '        type=int,'
    - '        default=-1,'
    - '        help="Index the selected samples during post-processing.",'
    - '    )'
    - '    parser.add_argument('
    - '        "--model", type=str, default="gpt-4o-2024-05-13", choices=["gpt-4o-2024-05-13"]'
    - '    )'
    - '    parser.add_argument("--output_folder", type=str, required=True)'
    - '    parser.add_argument('
    - '        "--only_correct", action="store_true"'
    - '    )  # only work on correct loc files (saves time)'
    - '    parser.add_argument("--post_process", action="store_true")'
    - '    parser.add_argument("--add_space", action="store_true")'
    - '    parser.add_argument("--cot", action="store_true")'
    - '    parser.add_argument("--fine_grain_loc_only", action="store_true")'
    - '    parser.add_argument("--diff_format", action="store_true")'
    - '    parser.add_argument("--skip_greedy", action="store_true")'
    - '    parser.add_argument("--sticky_scroll", action="store_true")'
    - '    parser.add_argument('
    - '        "--mock", action="store_true", help="Mock run to compute prompt tokens."'
    - '    )'
    - ''
    - '    args = parser.parse_args()'
    - ''
    - '    if not os.path.exists(args.output_folder):'
    - '        os.makedirs(args.output_folder)'
    - ''
    - '    args.output_file = os.path.join(args.output_folder, "output.jsonl")'
    - ''
    - '    if args.post_process:'
    - '        args.raw_output_file = args.output_file'
    - '        if args.select_id == -1:'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", "_processed.jsonl"'
    - '            )'
    - '        else:'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", f"_{args.select_id}_processed.jsonl"'
    - '            )'
    - '        post_process_repair(args)'
    - '    elif args.gen_and_process:'
    - '        repair(args)'
    - '        args.raw_output_file = args.output_file'
    - '        for i in range(args.max_samples):'
    - '            args.output_file = args.raw_output_file.replace('
    - '                ".jsonl", f"_{i}_processed.jsonl"'
    - '            )'
    - '            args.select_id = i'
    - '            post_process_repair(args)'
    - '    else:'
    - '        repair(args)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    main()'
  rerank.py:
    classes:
    - end_line: 109
      methods:
      - end_line: 109
        name: default
        start_line: 106
        text:
        - '    def default(self, obj):'
        - '        if isinstance(obj, set):'
        - '            return list(obj)'
        - '        return json.JSONEncoder.default(self, obj)'
      name: SetEncoder
      start_line: 105
      text:
      - 'class SetEncoder(json.JSONEncoder):'
      - '    def default(self, obj):'
      - '        if isinstance(obj, set):'
      - '            return list(obj)'
      - '        return json.JSONEncoder.default(self, obj)'
    functions:
    - end_line: 43
      name: _load_results
      start_line: 16
      text:
      - 'def _load_results(args):'
      - '    global execution_results'
      - ''
      - '    roots = [Path(folder) for folder in args.patch_folder.split(",")]'
      - ''
      - '    # assumes interval'
      - '    intervals = [(0, int(args.num_samples / len(roots)) - 1) for _ in range(len(roots))]'
      - ''
      - '    for index, root in enumerate(roots):'
      - '        interval = intervals[index]'
      - '        for i in range(interval[0], interval[1] + 1):'
      - '            patches = load_jsonl(root / f"output_{i}_normalized.jsonl")'
      - '            print('
      - '                f"Loaded {len(patches)} patches from {root / f''output_{i}_normalized.jsonl''}"'
      - '            )'
      - '            for patch in patches[:300]:'
      - '                try:'
      - '                    execution_results.setdefault(patch["instance_id"], []).append('
      - '                        {'
      - '                            "normalized_patch": patch["normalized_patch"].strip(),'
      - '                            "patch": patch["model_patch"],'
      - '                            "plausible": True,  # default to TRUE for now,
        TODO: add plausible execution.'
      - '                        }'
      - '                    )'
      - '                except:'
      - '                    print(i)'
      - '                    print(patch)'
      - '                    exit(-1)'
    - end_line: 48
      name: get_sample
      start_line: 46
      text:
      - 'def get_sample(instance_id, sample_id) -> tuple[str, bool]:'
      - '    """Returns the diff and pass status."""'
      - '    return execution_results[instance_id][sample_id]'
    - end_line: 70
      name: get_all_patches
      start_line: 51
      text:
      - 'def get_all_patches(instance_id, num_samples, deduplicate) -> list[str]:'
      - '    """Returns all unique patches."""'
      - '    patches = [execution_results[instance_id][i]["patch"] for i in range(num_samples)]'
      - '    if deduplicate:'
      - '        patch_keys = ['
      - '            execution_results[instance_id][i]["normalized_patch"]'
      - '            for i in range(num_samples)'
      - '        ]'
      - '    else:'
      - '        patch_keys = ['
      - '            execution_results[instance_id][i]["patch"] for i in range(num_samples)'
      - '        ]'
      - '    unique_patches = set()'
      - '    patch_ids = []'
      - '    for i in range(num_samples):'
      - '        patch_key = patch_keys[i].strip()'
      - '        if patch_key and patch_key not in unique_patches:'
      - '            unique_patches.add(patch_key)'
      - '            patch_ids.append(i)'
      - '    return [(id, patches[id]) for id in patch_ids]'
    - end_line: 97
      name: get_all_patches_num
      start_line: 73
      text:
      - 'def get_all_patches_num(instance_id, num_samples, deduplicate) -> list[str]:'
      - '    """Returns all unique patches with number."""'
      - '    # print(f"{len(execution_results)}")'
      - '    patches = [execution_results[instance_id][i]["patch"] for i in range(num_samples)]'
      - '    if deduplicate:'
      - '        patch_keys = ['
      - '            execution_results[instance_id][i]["normalized_patch"]'
      - '            for i in range(num_samples)'
      - '        ]'
      - '    else:'
      - '        patch_keys = ['
      - '            execution_results[instance_id][i]["patch"] for i in range(num_samples)'
      - '        ]'
      - '    unique_patches = {}'
      - '    total_patch_num = {}'
      - '    patch_ids = []'
      - '    for i in range(num_samples):'
      - '        if patch_keys[i] and patch_keys[i] not in unique_patches:'
      - '            unique_patches[patch_keys[i]] = i'
      - '            patch_ids.append(i)'
      - '            total_patch_num[i] = 0'
      - '        if patch_keys[i]:'
      - '            total_patch_num[unique_patches[patch_keys[i]]] += 1'
      - ''
      - '    return [(id, patches[id], total_patch_num[id]) for id in patch_ids]'
    - end_line: 220
      name: majority_voting
      start_line: 112
      text:
      - 'def majority_voting(args):'
      - '    all_pred = []'
      - ''
      - '    for instance_id in execution_results:'
      - '        patch_keys = ['
      - '            execution_results[instance_id][i]["normalized_patch"]'
      - '            for i in range(args.num_samples)'
      - '        ]'
      - '        plausible = ['
      - '            execution_results[instance_id][i]["plausible"]'
      - '            for i in range(args.num_samples)'
      - '        ]'
      - ''
      - '        raw_patches = ['
      - '            execution_results[instance_id][i]["patch"]'
      - '            for i in range(args.num_samples)'
      - '            for i in range(args.num_samples)'
      - '        ]'
      - ''
      - '        if args.plausible:'
      - '            patch_ids = list('
      - '                i'
      - '                for i in range(args.num_samples)'
      - '                if patch_keys[i].strip() and plausible[i]'
      - '            )'
      - '        else:'
      - '            patch_ids = list('
      - '                i for i in range(args.num_samples) if patch_keys[i].strip()'
      - '            )'
      - ''
      - '        if not patch_ids:'
      - '            # just vote on all patches'
      - '            if not all([x.strip() == "" for x in raw_patches]):'
      - '                vote = Counter()'
      - '                first_appear_idx = dict()'
      - '                valid_indices = []'
      - '                for i in range(args.num_samples):'
      - '                    sample = get_sample(instance_id, i)'
      - '                    patch_key = sample["normalized_patch"]'
      - '                    if patch_key != "":'
      - '                        valid_indices.append(i)'
      - '                        vote[patch_key] += 1'
      - '                        if patch_key not in first_appear_idx:'
      - '                            first_appear_idx[patch_key] = i'
      - ''
      - '                maj_selected_id = max('
      - '                    valid_indices,'
      - '                    key=lambda i: ('
      - '                        vote[patch_keys[i]],'
      - '                        -first_appear_idx[patch_keys[i]],'
      - '                    ),'
      - '                )'
      - '                patch = get_sample(instance_id, maj_selected_id)["patch"]'
      - '                all_pred.append('
      - '                    {'
      - '                        "model_name_or_path": "agentless",'
      - '                        "instance_id": instance_id,'
      - '                        "model_patch": patch,'
      - '                    }'
      - '                )'
      - '            else:'
      - '                all_pred.append('
      - '                    {'
      - '                        "model_name_or_path": "agentless",'
      - '                        "instance_id": instance_id,'
      - '                        "model_patch": "",'
      - '                    }'
      - '                )'
      - '            continue'
      - ''
      - '        vote = Counter()'
      - '        first_appear_idx = dict()'
      - '        for i in patch_ids:'
      - '            sample = get_sample(instance_id, i)'
      - '            patch_key, patch = ('
      - '                sample["normalized_patch"],'
      - '                sample["patch"],'
      - '            )'
      - '            vote[patch_key] += 1'
      - '            if patch_key not in first_appear_idx:'
      - '                first_appear_idx[patch_key] = i'
      - '        ### pure majority voting'
      - '        maj_selected_id = max('
      - '            patch_ids,'
      - '            key=lambda i: (vote[patch_keys[i]], -first_appear_idx[patch_keys[i]]),'
      - '        )'
      - ''
      - '        if args.target is not None and instance_id == args.target:'
      - '            for patch in vote:'
      - '                print('
      - '                    "=" * 20,'
      - '                    vote[patch],'
      - '                    "=" * 20,'
      - '                )'
      - '                print(patch)'
      - '                print("=" * 50)'
      - ''
      - '        sample = get_sample(instance_id, maj_selected_id)'
      - '        all_pred.append('
      - '            {'
      - '                "model_name_or_path": "agentless",'
      - '                "instance_id": instance_id,'
      - '                "model_patch": sample["patch"],'
      - '            }'
      - '        )'
      - ''
      - '    with open("all_preds.jsonl", "w") as f:'
      - '        for pred in all_pred:'
      - '            f.write(json.dumps(pred) + "\n")'
    - end_line: 248
      name: normalize_patches
      start_line: 223
      text:
      - 'def normalize_patches(args):'
      - '    # separate the patch folders'
      - '    output_folders = [Path(folder) for folder in args.patch_folder.split(",")]'
      - '    num_folders = len(output_folders)'
      - '    # output_folder = Path(args.patch_folder)'
      - '    selected_ids = list(range(int(args.num_samples / num_folders)))'
      - ''
      - '    # print(num_folders, output_folders)'
      - ''
      - '    for output_folder in output_folders:'
      - '        for i in selected_ids:'
      - '            if os.path.exists(output_folder / f"output_{i}_normalized.jsonl"):'
      - '                # skip'
      - '                continue'
      - '            patches = load_jsonl(output_folder / f"output_{i}_processed.jsonl")'
      - '            for d in patches:'
      - '                instance_id = d["instance_id"]'
      - '                patch = d["model_patch"]'
      - '                original_file_content = d["original_file_content"]'
      - '                normalized_patch = normalize_patch('
      - '                    instance_id, patch, original_file_content'
      - '                )'
      - '                d["normalized_patch"] = normalized_patch'
      - '            with open(output_folder / f"output_{i}_normalized.jsonl", "w")
        as f:'
      - '                for d in patches:'
      - '                    f.write(json.dumps(d) + "\n")'
    - end_line: 265
      name: main
      start_line: 251
      text:
      - 'def main():'
      - '    parser = argparse.ArgumentParser()'
      - '    parser.add_argument("--patch_folder", type=str)'
      - '    parser.add_argument("--target", type=str, default=None)'
      - '    parser.add_argument("--num_samples", type=int, default=11)'
      - '    parser.add_argument("--deduplicate", action="store_true")'
      - '    parser.add_argument("--plausible", action="store_true")'
      - '    args = parser.parse_args()'
      - ''
      - '    # first normalize'
      - '    normalize_patches(args)'
      - '    # then load results'
      - '    _load_results(args)'
      - '    # then rerank'
      - '    majority_voting(args)'
    text:
    - import argparse
    - import json
    - import logging
    - import os
    - from collections import Counter, OrderedDict
    - from pathlib import Path
    - ''
    - from tqdm import tqdm
    - ''
    - from agentless.util.postprocess_data import extract_python_blocks, normalize_patch
    - from agentless.util.utils import load_json, load_jsonl
    - ''
    - execution_results = dict()
    - ''
    - ''
    - 'def _load_results(args):'
    - '    global execution_results'
    - ''
    - '    roots = [Path(folder) for folder in args.patch_folder.split(",")]'
    - ''
    - '    # assumes interval'
    - '    intervals = [(0, int(args.num_samples / len(roots)) - 1) for _ in range(len(roots))]'
    - ''
    - '    for index, root in enumerate(roots):'
    - '        interval = intervals[index]'
    - '        for i in range(interval[0], interval[1] + 1):'
    - '            patches = load_jsonl(root / f"output_{i}_normalized.jsonl")'
    - '            print('
    - '                f"Loaded {len(patches)} patches from {root / f''output_{i}_normalized.jsonl''}"'
    - '            )'
    - '            for patch in patches[:300]:'
    - '                try:'
    - '                    execution_results.setdefault(patch["instance_id"], []).append('
    - '                        {'
    - '                            "normalized_patch": patch["normalized_patch"].strip(),'
    - '                            "patch": patch["model_patch"],'
    - '                            "plausible": True,  # default to TRUE for now,
      TODO: add plausible execution.'
    - '                        }'
    - '                    )'
    - '                except:'
    - '                    print(i)'
    - '                    print(patch)'
    - '                    exit(-1)'
    - ''
    - ''
    - 'def get_sample(instance_id, sample_id) -> tuple[str, bool]:'
    - '    """Returns the diff and pass status."""'
    - '    return execution_results[instance_id][sample_id]'
    - ''
    - ''
    - 'def get_all_patches(instance_id, num_samples, deduplicate) -> list[str]:'
    - '    """Returns all unique patches."""'
    - '    patches = [execution_results[instance_id][i]["patch"] for i in range(num_samples)]'
    - '    if deduplicate:'
    - '        patch_keys = ['
    - '            execution_results[instance_id][i]["normalized_patch"]'
    - '            for i in range(num_samples)'
    - '        ]'
    - '    else:'
    - '        patch_keys = ['
    - '            execution_results[instance_id][i]["patch"] for i in range(num_samples)'
    - '        ]'
    - '    unique_patches = set()'
    - '    patch_ids = []'
    - '    for i in range(num_samples):'
    - '        patch_key = patch_keys[i].strip()'
    - '        if patch_key and patch_key not in unique_patches:'
    - '            unique_patches.add(patch_key)'
    - '            patch_ids.append(i)'
    - '    return [(id, patches[id]) for id in patch_ids]'
    - ''
    - ''
    - 'def get_all_patches_num(instance_id, num_samples, deduplicate) -> list[str]:'
    - '    """Returns all unique patches with number."""'
    - '    # print(f"{len(execution_results)}")'
    - '    patches = [execution_results[instance_id][i]["patch"] for i in range(num_samples)]'
    - '    if deduplicate:'
    - '        patch_keys = ['
    - '            execution_results[instance_id][i]["normalized_patch"]'
    - '            for i in range(num_samples)'
    - '        ]'
    - '    else:'
    - '        patch_keys = ['
    - '            execution_results[instance_id][i]["patch"] for i in range(num_samples)'
    - '        ]'
    - '    unique_patches = {}'
    - '    total_patch_num = {}'
    - '    patch_ids = []'
    - '    for i in range(num_samples):'
    - '        if patch_keys[i] and patch_keys[i] not in unique_patches:'
    - '            unique_patches[patch_keys[i]] = i'
    - '            patch_ids.append(i)'
    - '            total_patch_num[i] = 0'
    - '        if patch_keys[i]:'
    - '            total_patch_num[unique_patches[patch_keys[i]]] += 1'
    - ''
    - '    return [(id, patches[id], total_patch_num[id]) for id in patch_ids]'
    - ''
    - ''
    - '######'
    - ''
    - import json
    - ''
    - ''
    - 'class SetEncoder(json.JSONEncoder):'
    - '    def default(self, obj):'
    - '        if isinstance(obj, set):'
    - '            return list(obj)'
    - '        return json.JSONEncoder.default(self, obj)'
    - ''
    - ''
    - 'def majority_voting(args):'
    - '    all_pred = []'
    - ''
    - '    for instance_id in execution_results:'
    - '        patch_keys = ['
    - '            execution_results[instance_id][i]["normalized_patch"]'
    - '            for i in range(args.num_samples)'
    - '        ]'
    - '        plausible = ['
    - '            execution_results[instance_id][i]["plausible"]'
    - '            for i in range(args.num_samples)'
    - '        ]'
    - ''
    - '        raw_patches = ['
    - '            execution_results[instance_id][i]["patch"]'
    - '            for i in range(args.num_samples)'
    - '            for i in range(args.num_samples)'
    - '        ]'
    - ''
    - '        if args.plausible:'
    - '            patch_ids = list('
    - '                i'
    - '                for i in range(args.num_samples)'
    - '                if patch_keys[i].strip() and plausible[i]'
    - '            )'
    - '        else:'
    - '            patch_ids = list('
    - '                i for i in range(args.num_samples) if patch_keys[i].strip()'
    - '            )'
    - ''
    - '        if not patch_ids:'
    - '            # just vote on all patches'
    - '            if not all([x.strip() == "" for x in raw_patches]):'
    - '                vote = Counter()'
    - '                first_appear_idx = dict()'
    - '                valid_indices = []'
    - '                for i in range(args.num_samples):'
    - '                    sample = get_sample(instance_id, i)'
    - '                    patch_key = sample["normalized_patch"]'
    - '                    if patch_key != "":'
    - '                        valid_indices.append(i)'
    - '                        vote[patch_key] += 1'
    - '                        if patch_key not in first_appear_idx:'
    - '                            first_appear_idx[patch_key] = i'
    - ''
    - '                maj_selected_id = max('
    - '                    valid_indices,'
    - '                    key=lambda i: ('
    - '                        vote[patch_keys[i]],'
    - '                        -first_appear_idx[patch_keys[i]],'
    - '                    ),'
    - '                )'
    - '                patch = get_sample(instance_id, maj_selected_id)["patch"]'
    - '                all_pred.append('
    - '                    {'
    - '                        "model_name_or_path": "agentless",'
    - '                        "instance_id": instance_id,'
    - '                        "model_patch": patch,'
    - '                    }'
    - '                )'
    - '            else:'
    - '                all_pred.append('
    - '                    {'
    - '                        "model_name_or_path": "agentless",'
    - '                        "instance_id": instance_id,'
    - '                        "model_patch": "",'
    - '                    }'
    - '                )'
    - '            continue'
    - ''
    - '        vote = Counter()'
    - '        first_appear_idx = dict()'
    - '        for i in patch_ids:'
    - '            sample = get_sample(instance_id, i)'
    - '            patch_key, patch = ('
    - '                sample["normalized_patch"],'
    - '                sample["patch"],'
    - '            )'
    - '            vote[patch_key] += 1'
    - '            if patch_key not in first_appear_idx:'
    - '                first_appear_idx[patch_key] = i'
    - '        ### pure majority voting'
    - '        maj_selected_id = max('
    - '            patch_ids,'
    - '            key=lambda i: (vote[patch_keys[i]], -first_appear_idx[patch_keys[i]]),'
    - '        )'
    - ''
    - '        if args.target is not None and instance_id == args.target:'
    - '            for patch in vote:'
    - '                print('
    - '                    "=" * 20,'
    - '                    vote[patch],'
    - '                    "=" * 20,'
    - '                )'
    - '                print(patch)'
    - '                print("=" * 50)'
    - ''
    - '        sample = get_sample(instance_id, maj_selected_id)'
    - '        all_pred.append('
    - '            {'
    - '                "model_name_or_path": "agentless",'
    - '                "instance_id": instance_id,'
    - '                "model_patch": sample["patch"],'
    - '            }'
    - '        )'
    - ''
    - '    with open("all_preds.jsonl", "w") as f:'
    - '        for pred in all_pred:'
    - '            f.write(json.dumps(pred) + "\n")'
    - ''
    - ''
    - 'def normalize_patches(args):'
    - '    # separate the patch folders'
    - '    output_folders = [Path(folder) for folder in args.patch_folder.split(",")]'
    - '    num_folders = len(output_folders)'
    - '    # output_folder = Path(args.patch_folder)'
    - '    selected_ids = list(range(int(args.num_samples / num_folders)))'
    - ''
    - '    # print(num_folders, output_folders)'
    - ''
    - '    for output_folder in output_folders:'
    - '        for i in selected_ids:'
    - '            if os.path.exists(output_folder / f"output_{i}_normalized.jsonl"):'
    - '                # skip'
    - '                continue'
    - '            patches = load_jsonl(output_folder / f"output_{i}_processed.jsonl")'
    - '            for d in patches:'
    - '                instance_id = d["instance_id"]'
    - '                patch = d["model_patch"]'
    - '                original_file_content = d["original_file_content"]'
    - '                normalized_patch = normalize_patch('
    - '                    instance_id, patch, original_file_content'
    - '                )'
    - '                d["normalized_patch"] = normalized_patch'
    - '            with open(output_folder / f"output_{i}_normalized.jsonl", "w")
      as f:'
    - '                for d in patches:'
    - '                    f.write(json.dumps(d) + "\n")'
    - ''
    - ''
    - 'def main():'
    - '    parser = argparse.ArgumentParser()'
    - '    parser.add_argument("--patch_folder", type=str)'
    - '    parser.add_argument("--target", type=str, default=None)'
    - '    parser.add_argument("--num_samples", type=int, default=11)'
    - '    parser.add_argument("--deduplicate", action="store_true")'
    - '    parser.add_argument("--plausible", action="store_true")'
    - '    args = parser.parse_args()'
    - ''
    - '    # first normalize'
    - '    normalize_patches(args)'
    - '    # then load results'
    - '    _load_results(args)'
    - '    # then rerank'
    - '    majority_voting(args)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    main()'
    - '#'
util:
  __pycache__:
    api_requests.cpython-311.pyc: {}
    codegeex4.cpython-311.pyc: {}
    compress_file.cpython-311.pyc: {}
    parse_global_var.cpython-311.pyc: {}
    postprocess_data.cpython-311.pyc: {}
    preprocess_data.cpython-311.pyc: {}
    utils.cpython-311.pyc: {}
  api_requests.py:
    classes: []
    functions:
    - end_line: 27
      name: num_tokens_from_messages
      start_line: 16
      text:
      - 'def num_tokens_from_messages(message, model="gpt-3.5-turbo-0301"):'
      - '    """Returns the number of tokens used by a list of messages."""'
      - '    try:'
      - '        encoding = tiktoken.encoding_for_model(model)'
      - '    except KeyError:'
      - '        encoding = tiktoken.get_encoding("cl100k_base")'
      - '    if isinstance(message, list):'
      - '        # use last message.'
      - '        num_tokens = len(encoding.encode(message[0]["content"]))'
      - '    else:'
      - '        num_tokens = len(encoding.encode(message))'
      - '    return num_tokens'
    - end_line: 108
      name: create_chatgpt_config
      start_line: 81
      text:
      - def create_chatgpt_config(
      - '    message: Union[str, list],'
      - '    max_tokens: int,'
      - '    temperature: float = 1,'
      - '    batch_size: int = 1,'
      - '    system_message: str = "You are a helpful assistant.",'
      - '    model: str = "gpt-3.5-turbo",'
      - ') -> Dict:'
      - '    if isinstance(message, list):'
      - '        config = {'
      - '            "model": model,'
      - '            "max_tokens": max_tokens,'
      - '            "temperature": temperature,'
      - '            "n": batch_size,'
      - '            "messages": [{"role": "system", "content": system_message}] +
        message,'
      - '        }'
      - '    else:'
      - '        config = {'
      - '            "model": model,'
      - '            "max_tokens": max_tokens,'
      - '            "temperature": temperature,'
      - '            "n": batch_size,'
      - '            "messages": ['
      - '                {"role": "system", "content": system_message},'
      - '                {"role": "user", "content": message},'
      - '            ],'
      - '        }'
      - '    return config'
    - end_line: 113
      name: handler
      start_line: 111
      text:
      - 'def handler(signum, frame):'
      - '    # swallow signum and frame'
      - '    raise Exception("end of time")'
    - end_line: 141
      name: request_chatgpt_engine
      start_line: 116
      text:
      - 'def request_chatgpt_engine(config):'
      - '    ret = None'
      - '    while ret is None:'
      - '        try:'
      - '            signal.signal(signal.SIGALRM, handler)'
      - '            signal.alarm(100)'
      - '            ret = client.chat.completions.create(**config)'
      - '            signal.alarm(0)'
      - '        except openai._exceptions.BadRequestError as e:'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '        except openai._exceptions.RateLimitError as e:'
      - '            print("Rate limit exceeded. Waiting...")'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '            time.sleep(5)'
      - '        except openai._exceptions.APIConnectionError as e:'
      - '            print("API connection error. Waiting...")'
      - '            signal.alarm(0)'
      - '            time.sleep(5)'
      - '        except Exception as e:'
      - '            print("Unknown error. Waiting...")'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '            time.sleep(1)'
      - '    return ret'
    - end_line: 170
      name: create_codegeex_config
      start_line: 146
      text:
      - def create_codegeex_config(
      - '    message: Union[str, list],'
      - '    max_tokens: int,'
      - '    temperature: float = 1,'
      - '    batch_size: int = 1,'
      - '    system_message: str = "You are an intelligent programming assistant named
        CodeGeeX. You will answer any questions users have about programming, coding,
        and computers, and provide code that is formatted correctly.",'
      - '    model: str = "codegeex-4",'
      - ') -> Dict:'
      - '    if isinstance(message, list):'
      - '        config = {'
      - '            "model": model,'
      - '            "max_tokens": max_tokens,'
      - '            "temperature": temperature,'
      - '            "messages": [{"role": "system", "content": system_message}] +
        message,'
      - '        }'
      - '    else:'
      - '        config = {'
      - '            "model": model,'
      - '            "max_tokens": max_tokens,'
      - '            "temperature": temperature,'
      - '            "messages": ['
      - '                {"role": "user", "content": message},'
      - '            ],'
      - '        }'
      - '    return config'
    - end_line: 196
      name: request_codegeex_engine
      start_line: 171
      text:
      - 'def request_codegeex_engine(config):'
      - '    ret = None'
      - '    while ret is None:'
      - '        try:'
      - '            signal.signal(signal.SIGALRM, handler)'
      - '            signal.alarm(100)'
      - '            ret = zhipu_client.chat.completions.create(**config)'
      - '            signal.alarm(0)'
      - '        except openai._exceptions.BadRequestError as e:'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '        except openai._exceptions.RateLimitError as e:'
      - '            print("Rate limit exceeded. Waiting...")'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '            time.sleep(5)'
      - '        except openai._exceptions.APIConnectionError as e:'
      - '            print("API connection error. Waiting...")'
      - '            signal.alarm(0)'
      - '            time.sleep(5)'
      - '        except Exception as e:'
      - '            print("Unknown error. Waiting...")'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '            time.sleep(1)'
      - '    return ret'
    - end_line: 227
      name: create_anthropic_config
      start_line: 199
      text:
      - def create_anthropic_config(
      - '    message: str,'
      - '    prefill_message: str,'
      - '    max_tokens: int,'
      - '    temperature: float = 1,'
      - '    batch_size: int = 1,'
      - '    system_message: str = "You are a helpful assistant.",'
      - '    model: str = "claude-2.1",'
      - ') -> Dict:'
      - '    if isinstance(message, list):'
      - '        config = {'
      - '            "model": model,'
      - '            "temperature": temperature,'
      - '            "max_tokens": max_tokens,'
      - '            "system": system_message,'
      - '            "messages": message,'
      - '        }'
      - '    else:'
      - '        config = {'
      - '            "model": model,'
      - '            "temperature": temperature,'
      - '            "max_tokens": max_tokens,'
      - '            "system": system_message,'
      - '            "messages": ['
      - '                {"role": "user", "content": message},'
      - '                {"role": "assistant", "content": prefill_message},'
      - '            ],'
      - '        }'
      - '    return config'
    - end_line: 243
      name: request_anthropic_engine
      start_line: 230
      text:
      - 'def request_anthropic_engine(client, config):'
      - '    ret = None'
      - '    while ret is None:'
      - '        try:'
      - '            signal.signal(signal.SIGALRM, handler)'
      - '            signal.alarm(100)'
      - '            ret = client.messages.create(**config)'
      - '            signal.alarm(0)'
      - '        except Exception as e:'
      - '            print("Unknown error. Waiting...")'
      - '            print(e)'
      - '            signal.alarm(0)'
      - '            time.sleep(10)'
      - '    return ret'
    text:
    - import signal
    - import time
    - from typing import Dict, Union
    - ''
    - import openai
    - import tiktoken
    - from agentless.util.codegeex4 import generate
    - from zhipuai import ZhipuAI
    - import os
    - import config
    - ''
    - cfg = config.Config(os.path.join(os.getcwd(), "keys.cfg"))
    - client = openai.OpenAI(api_key=cfg['OPENAI_API_KEY'])
    - zhipu_client = ZhipuAI(api_key=cfg["ZHIPU_API_KEY"])
    - ''
    - 'def num_tokens_from_messages(message, model="gpt-3.5-turbo-0301"):'
    - '    """Returns the number of tokens used by a list of messages."""'
    - '    try:'
    - '        encoding = tiktoken.encoding_for_model(model)'
    - '    except KeyError:'
    - '        encoding = tiktoken.get_encoding("cl100k_base")'
    - '    if isinstance(message, list):'
    - '        # use last message.'
    - '        num_tokens = len(encoding.encode(message[0]["content"]))'
    - '    else:'
    - '        num_tokens = len(encoding.encode(message))'
    - '    return num_tokens'
    - ''
    - '       '
    - '# def create_codegeex_config('
    - '#     message: Union[str, list],'
    - '#     max_tokens: int,'
    - '#     temperature: float = 1,'
    - '#     system_message: str = "You are a helpful assistant.",'
    - '# ) -> Dict:'
    - '#     if isinstance(message, list):'
    - '#         config = {'
    - '#             "max_tokens": max_tokens,'
    - '#             "temperature": temperature,'
    - '#             "prompt": f"<|assistant|>\n{system_message}\n<|user|>\n{message}\n<|assistant|>\n",'
    - '#             "url": "http://172.18.64.110:9090/v1/completions"'
    - '#         }'
    - '#     else:'
    - '#         config = {'
    - '#             "max_tokens": max_tokens,'
    - '#             "temperature": temperature,'
    - '#             "prompt": f"<|assistant|>\n{system_message}\n<|user|>\n{message}\n<|assistant|>\n",'
    - '#             "url" : "http://172.18.64.110:9090/v1/completions"'
    - '#         }'
    - '#     return config '
    - ''
    - ''
    - '# def request_codegeex_engine(config):'
    - '#     ret = None'
    - '#     while ret is None:'
    - '#         try:'
    - '#             signal.signal(signal.SIGALRM, handler)'
    - '#             signal.alarm(100)'
    - '#             ret = generate(**config)'
    - '#             signal.alarm(0)'
    - '#         except openai._exceptions.BadRequestError as e:'
    - '#             print(e)'
    - '#             signal.alarm(0)'
    - '#         except openai._exceptions.RateLimitError as e:'
    - '#             print("Rate limit exceeded. Waiting...")'
    - '#             print(e)'
    - '#             signal.alarm(0)'
    - '#             time.sleep(5)'
    - '#         except openai._exceptions.APIConnectionError as e:'
    - '#             print("API connection error. Waiting...")'
    - '#             signal.alarm(0)'
    - '#             time.sleep(5)'
    - '#         except Exception as e:'
    - '#             print("Unknown error. Waiting...")'
    - '#             print(e)'
    - '#             signal.alarm(0)'
    - '#             time.sleep(1)'
    - '#     return ret'
    - '    '
    - ''
    - def create_chatgpt_config(
    - '    message: Union[str, list],'
    - '    max_tokens: int,'
    - '    temperature: float = 1,'
    - '    batch_size: int = 1,'
    - '    system_message: str = "You are a helpful assistant.",'
    - '    model: str = "gpt-3.5-turbo",'
    - ') -> Dict:'
    - '    if isinstance(message, list):'
    - '        config = {'
    - '            "model": model,'
    - '            "max_tokens": max_tokens,'
    - '            "temperature": temperature,'
    - '            "n": batch_size,'
    - '            "messages": [{"role": "system", "content": system_message}] + message,'
    - '        }'
    - '    else:'
    - '        config = {'
    - '            "model": model,'
    - '            "max_tokens": max_tokens,'
    - '            "temperature": temperature,'
    - '            "n": batch_size,'
    - '            "messages": ['
    - '                {"role": "system", "content": system_message},'
    - '                {"role": "user", "content": message},'
    - '            ],'
    - '        }'
    - '    return config'
    - ''
    - ''
    - 'def handler(signum, frame):'
    - '    # swallow signum and frame'
    - '    raise Exception("end of time")'
    - ''
    - ''
    - 'def request_chatgpt_engine(config):'
    - '    ret = None'
    - '    while ret is None:'
    - '        try:'
    - '            signal.signal(signal.SIGALRM, handler)'
    - '            signal.alarm(100)'
    - '            ret = client.chat.completions.create(**config)'
    - '            signal.alarm(0)'
    - '        except openai._exceptions.BadRequestError as e:'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '        except openai._exceptions.RateLimitError as e:'
    - '            print("Rate limit exceeded. Waiting...")'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '            time.sleep(5)'
    - '        except openai._exceptions.APIConnectionError as e:'
    - '            print("API connection error. Waiting...")'
    - '            signal.alarm(0)'
    - '            time.sleep(5)'
    - '        except Exception as e:'
    - '            print("Unknown error. Waiting...")'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '            time.sleep(1)'
    - '    return ret'
    - ''
    - ''
    - ''
    - ''
    - def create_codegeex_config(
    - '    message: Union[str, list],'
    - '    max_tokens: int,'
    - '    temperature: float = 1,'
    - '    batch_size: int = 1,'
    - '    system_message: str = "You are an intelligent programming assistant named
      CodeGeeX. You will answer any questions users have about programming, coding,
      and computers, and provide code that is formatted correctly.",'
    - '    model: str = "codegeex-4",'
    - ') -> Dict:'
    - '    if isinstance(message, list):'
    - '        config = {'
    - '            "model": model,'
    - '            "max_tokens": max_tokens,'
    - '            "temperature": temperature,'
    - '            "messages": [{"role": "system", "content": system_message}] + message,'
    - '        }'
    - '    else:'
    - '        config = {'
    - '            "model": model,'
    - '            "max_tokens": max_tokens,'
    - '            "temperature": temperature,'
    - '            "messages": ['
    - '                {"role": "user", "content": message},'
    - '            ],'
    - '        }'
    - '    return config'
    - 'def request_codegeex_engine(config):'
    - '    ret = None'
    - '    while ret is None:'
    - '        try:'
    - '            signal.signal(signal.SIGALRM, handler)'
    - '            signal.alarm(100)'
    - '            ret = zhipu_client.chat.completions.create(**config)'
    - '            signal.alarm(0)'
    - '        except openai._exceptions.BadRequestError as e:'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '        except openai._exceptions.RateLimitError as e:'
    - '            print("Rate limit exceeded. Waiting...")'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '            time.sleep(5)'
    - '        except openai._exceptions.APIConnectionError as e:'
    - '            print("API connection error. Waiting...")'
    - '            signal.alarm(0)'
    - '            time.sleep(5)'
    - '        except Exception as e:'
    - '            print("Unknown error. Waiting...")'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '            time.sleep(1)'
    - '    return ret'
    - ''
    - ''
    - def create_anthropic_config(
    - '    message: str,'
    - '    prefill_message: str,'
    - '    max_tokens: int,'
    - '    temperature: float = 1,'
    - '    batch_size: int = 1,'
    - '    system_message: str = "You are a helpful assistant.",'
    - '    model: str = "claude-2.1",'
    - ') -> Dict:'
    - '    if isinstance(message, list):'
    - '        config = {'
    - '            "model": model,'
    - '            "temperature": temperature,'
    - '            "max_tokens": max_tokens,'
    - '            "system": system_message,'
    - '            "messages": message,'
    - '        }'
    - '    else:'
    - '        config = {'
    - '            "model": model,'
    - '            "temperature": temperature,'
    - '            "max_tokens": max_tokens,'
    - '            "system": system_message,'
    - '            "messages": ['
    - '                {"role": "user", "content": message},'
    - '                {"role": "assistant", "content": prefill_message},'
    - '            ],'
    - '        }'
    - '    return config'
    - ''
    - ''
    - 'def request_anthropic_engine(client, config):'
    - '    ret = None'
    - '    while ret is None:'
    - '        try:'
    - '            signal.signal(signal.SIGALRM, handler)'
    - '            signal.alarm(100)'
    - '            ret = client.messages.create(**config)'
    - '            signal.alarm(0)'
    - '        except Exception as e:'
    - '            print("Unknown error. Waiting...")'
    - '            print(e)'
    - '            signal.alarm(0)'
    - '            time.sleep(10)'
    - '    return ret'
  codegeex4.py:
    classes: []
    functions:
    - end_line: 52
      name: generate
      start_line: 17
      text:
      - def generate(
      - '        prompt, '
      - '        url,'
      - '        do_sample=False,'
      - '        temperature=0.3,'
      - '        top_p=0.95,'
      - '        max_tokens=256,'
      - '        truncate=12800,'
      - '        stream=False,'
      - '        stop=["<|endoftext|>", "<|user|>", "<|observation|>", "<|assistant|>"],'
      - '        retries=3,'
      - '        delay=2,'
      - '        **kwargs'
      - '):'
      - '    data = {'
      - '        "prompt": prompt,'
      - '        "temperature": temperature,'
      - '        "top_p": top_p,'
      - '        "do_sample": do_sample,'
      - '        "max_tokens": max_tokens,'
      - '        "truncate": truncate,'
      - '        "stream": stream,'
      - '        "stop": stop,'
      - '    }'
      - '    attempts = 0'
      - '    while attempts < retries:'
      - '        try:'
      - '            response = requests.post(url, headers=headers, json=data, verify=False)'
      - '            response = response.json()'
      - '            return response[''choices''][0][''text'']'
      - '        except Exception as e:'
      - '            attempts += 1'
      - '            print(e)'
      - '            logger.error(f"Attempt {attempts}/{retries} failed with error:
        {e}. Retrying in {delay} seconds...")'
      - '            time.sleep(delay)'
      - '    raise Exception(f"All {retries} attempts failed for prompt: {prompt}")'
    - end_line: 80
      name: generate_with_openai
      start_line: 56
      text:
      - 'def generate_with_openai(system_message, user_message):'
      - '    import openai'
      - '    from zhipuai import ZhipuAI'
      - ''
      - '    # client = openai.OpenAI(api_key=os.environ.get("ZHIPU_API_KEY"))'
      - '    client = ZhipuAI(api_key=os.environ.get("ZHIPU_API_KEY"))'
      - '    batch_size = 3'
      - '    max_tokens = 256'
      - '    temperature = 0.3'
      - ''
      - '    config = {'
      - '            "model": "codegeex-4",'
      - '            "n": batch_size,'
      - '            "max_tokens": max_tokens,'
      - '            "temperature": temperature,'
      - '             "messages":['
      - '        {"role": "system", "content": system_message},'
      - '        {"role": "user", "content": user_message},'
      - '    ]'
      - '    }'
      - ''
      - '    response = client.chat.completions.create(**config)'
      - '    print(response.choices[0].message.content)'
      - '    print(response.usage.prompt_tokens)'
      - '    print(response.usage.completion_tokens)'
    - end_line: 110
      name: main
      start_line: 86
      text:
      - 'def main():'
      - '    system_prompt = "You are an intelligent programming assistant named CodeGeeX.
        You will answer any questions users have about programming, coding, and computers,
        and provide code that is formatted correctly. "'
      - '    query = """'
      - '    # \nPlease review the following GitHub problem description and relevant
        files, and provide a set of locations that need to be edited to fix the issue.\nThe
        locations can be specified as class names, function or method names, or exact
        line numbers that require modification.\n\n### GitHub Problem Description
        ###\nPlease support header rows in RestructuredText output\n### Description\r\n\r\nIt
        would be great if the following would work:\r\n\r\n```Python\r\n>>> from astropy.table
        import QTable\r\n>>> import astropy.units as u\r\n>>> import sys\r\n>>> tbl
        = QTable({\''wave\'': [350,950]*u.nm, \''response\'': [0.7, 1.2]*u.count})\r\n>>>
        tbl.write(sys.stdout,  format="ascii.rst")\r\n===== ========\r\n wave response\r\n=====
        ========\r\n350.0      0.7\r\n950.0      1.2\r\n===== ========\r\n>>> tbl.write(sys.stdout,  format="ascii.fixed_width",
        header_rows=["name", "unit"])\r\n|  wave | response |\r\n|    nm |       ct
        |\r\n| 350.0 |      0.7 |\r\n| 950.0 |      1.2 |\r\n>>> tbl.write(sys.stdout,  format="ascii.rst",
        header_rows=["name", "unit"])\r\nTraceback (most recent call last):\r\n  File
        "<stdin>", line 1, in <module>\r\n  File "/usr/lib/python3/dist-packages/astropy/table/connect.py",
        line 129, in __call__\r\n    self.registry.write(instance, *args, **kwargs)\r\n  File
        "/usr/lib/python3/dist-packages/astropy/io/registry/core.py", line 369, in
        write\r\n    return writer(data, *args, **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py",
        line 26, in io_write\r\n    return write(table, filename, **kwargs)\r\n  File
        "/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py", line 856, in write\r\n    writer
        = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py",
        line 800, in get_writer\r\n    writer = core._get_writer(Writer, fast_writer,
        **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/core.py",
        line 1719, in _get_writer\r\n    writer = Writer(**writer_kwargs)\r\nTypeError:
        RST.__init__() got an unexpected keyword argument \''header_rows\''\r\n```\r\n\r\n\r\n###
        Additional context\r\n\r\nRestructuredText output is a great way to fill autogenerated
        documentation with content, so having this flexible makes the life easier
        `:-)`\r\n\r\n\n\n\n###\n\n\n###\n\nPlease provide the class name, function
        or method name, or the exact line numbers that need to be edited.\n### Examples:\n```\nfull_path1/file1.py\nline:
        10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline:
        12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn
        just the location(s)\n'
      - '    # """'
      - '    # query = "Provide edit locations"'
      - ''
      - '    # Sample prompt '
      - '    # prompt = f"<|assistant|>\n{system_prompt}\n<|user|>\n{query}\n"'
      - '    '
      - '    # taking prompt from file'
      - '    # with open("codegeex_inspect/prompt.txt", "r") as f:'
      - '    #     # load from f'
      - '    #     prompt = f.read()'
      - ''
      - ''
      - '    # url = "http://172.18.64.110:9090/v1/completions"'
      - '    # try:'
      - '    #     code = generate(prompt, url)'
      - '    #     logger.info(code)'
      - ''
      - '    # except Exception as e:'
      - '    #     logger.error(e)'
      - ''
      - '    generate_with_openai(system_message=system_prompt, user_message=query)'
    text:
    - ''
    - import json
    - import gzip
    - import fire
    - import time
    - import requests
    - import loguru
    - from tqdm.auto import tqdm
    - import os
    - ''
    - logger = loguru.logger
    - ''
    - headers = {
    - '    ''Content-Type'': ''application/json'''
    - '}'
    - ''
    - def generate(
    - '        prompt, '
    - '        url,'
    - '        do_sample=False,'
    - '        temperature=0.3,'
    - '        top_p=0.95,'
    - '        max_tokens=256,'
    - '        truncate=12800,'
    - '        stream=False,'
    - '        stop=["<|endoftext|>", "<|user|>", "<|observation|>", "<|assistant|>"],'
    - '        retries=3,'
    - '        delay=2,'
    - '        **kwargs'
    - '):'
    - '    data = {'
    - '        "prompt": prompt,'
    - '        "temperature": temperature,'
    - '        "top_p": top_p,'
    - '        "do_sample": do_sample,'
    - '        "max_tokens": max_tokens,'
    - '        "truncate": truncate,'
    - '        "stream": stream,'
    - '        "stop": stop,'
    - '    }'
    - '    attempts = 0'
    - '    while attempts < retries:'
    - '        try:'
    - '            response = requests.post(url, headers=headers, json=data, verify=False)'
    - '            response = response.json()'
    - '            return response[''choices''][0][''text'']'
    - '        except Exception as e:'
    - '            attempts += 1'
    - '            print(e)'
    - '            logger.error(f"Attempt {attempts}/{retries} failed with error:
      {e}. Retrying in {delay} seconds...")'
    - '            time.sleep(delay)'
    - '    raise Exception(f"All {retries} attempts failed for prompt: {prompt}")'
    - ''
    - ''
    - ''
    - 'def generate_with_openai(system_message, user_message):'
    - '    import openai'
    - '    from zhipuai import ZhipuAI'
    - ''
    - '    # client = openai.OpenAI(api_key=os.environ.get("ZHIPU_API_KEY"))'
    - '    client = ZhipuAI(api_key=os.environ.get("ZHIPU_API_KEY"))'
    - '    batch_size = 3'
    - '    max_tokens = 256'
    - '    temperature = 0.3'
    - ''
    - '    config = {'
    - '            "model": "codegeex-4",'
    - '            "n": batch_size,'
    - '            "max_tokens": max_tokens,'
    - '            "temperature": temperature,'
    - '             "messages":['
    - '        {"role": "system", "content": system_message},'
    - '        {"role": "user", "content": user_message},'
    - '    ]'
    - '    }'
    - ''
    - '    response = client.chat.completions.create(**config)'
    - '    print(response.choices[0].message.content)'
    - '    print(response.usage.prompt_tokens)'
    - '    print(response.usage.completion_tokens)'
    - ''
    - ''
    - ''
    - ''
    - ''
    - 'def main():'
    - '    system_prompt = "You are an intelligent programming assistant named CodeGeeX.
      You will answer any questions users have about programming, coding, and computers,
      and provide code that is formatted correctly. "'
    - '    query = """'
    - '    # \nPlease review the following GitHub problem description and relevant
      files, and provide a set of locations that need to be edited to fix the issue.\nThe
      locations can be specified as class names, function or method names, or exact
      line numbers that require modification.\n\n### GitHub Problem Description ###\nPlease
      support header rows in RestructuredText output\n### Description\r\n\r\nIt would
      be great if the following would work:\r\n\r\n```Python\r\n>>> from astropy.table
      import QTable\r\n>>> import astropy.units as u\r\n>>> import sys\r\n>>> tbl
      = QTable({\''wave\'': [350,950]*u.nm, \''response\'': [0.7, 1.2]*u.count})\r\n>>>
      tbl.write(sys.stdout,  format="ascii.rst")\r\n===== ========\r\n wave response\r\n=====
      ========\r\n350.0      0.7\r\n950.0      1.2\r\n===== ========\r\n>>> tbl.write(sys.stdout,  format="ascii.fixed_width",
      header_rows=["name", "unit"])\r\n|  wave | response |\r\n|    nm |       ct
      |\r\n| 350.0 |      0.7 |\r\n| 950.0 |      1.2 |\r\n>>> tbl.write(sys.stdout,  format="ascii.rst",
      header_rows=["name", "unit"])\r\nTraceback (most recent call last):\r\n  File
      "<stdin>", line 1, in <module>\r\n  File "/usr/lib/python3/dist-packages/astropy/table/connect.py",
      line 129, in __call__\r\n    self.registry.write(instance, *args, **kwargs)\r\n  File
      "/usr/lib/python3/dist-packages/astropy/io/registry/core.py", line 369, in write\r\n    return
      writer(data, *args, **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py",
      line 26, in io_write\r\n    return write(table, filename, **kwargs)\r\n  File
      "/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py", line 856, in write\r\n    writer
      = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py",
      line 800, in get_writer\r\n    writer = core._get_writer(Writer, fast_writer,
      **kwargs)\r\n  File "/usr/lib/python3/dist-packages/astropy/io/ascii/core.py",
      line 1719, in _get_writer\r\n    writer = Writer(**writer_kwargs)\r\nTypeError:
      RST.__init__() got an unexpected keyword argument \''header_rows\''\r\n```\r\n\r\n\r\n###
      Additional context\r\n\r\nRestructuredText output is a great way to fill autogenerated
      documentation with content, so having this flexible makes the life easier `:-)`\r\n\r\n\n\n\n###\n\n\n###\n\nPlease
      provide the class name, function or method name, or the exact line numbers that
      need to be edited.\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass:
      MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline:
      12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn
      just the location(s)\n'
    - '    # """'
    - '    # query = "Provide edit locations"'
    - ''
    - '    # Sample prompt '
    - '    # prompt = f"<|assistant|>\n{system_prompt}\n<|user|>\n{query}\n"'
    - '    '
    - '    # taking prompt from file'
    - '    # with open("codegeex_inspect/prompt.txt", "r") as f:'
    - '    #     # load from f'
    - '    #     prompt = f.read()'
    - ''
    - ''
    - '    # url = "http://172.18.64.110:9090/v1/completions"'
    - '    # try:'
    - '    #     code = generate(prompt, url)'
    - '    #     logger.info(code)'
    - ''
    - '    # except Exception as e:'
    - '    #     logger.error(e)'
    - ''
    - '    generate_with_openai(system_message=system_prompt, user_message=query)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    main()'
  compress_file.py:
    classes:
    - end_line: 49
      methods:
      - end_line: 10
        name: __init__
        start_line: 9
        text:
        - '    def __init__(self, keep_constant=True):'
        - '        self.keep_constant = keep_constant'
      - end_line: 26
        name: leave_Module
        start_line: 12
        text:
        - '    def leave_Module('
        - '        self, original_node: cst.Module, updated_node: cst.Module'
        - '    ) -> cst.Module:'
        - '        new_body = ['
        - '            stmt'
        - '            for stmt in updated_node.body'
        - '            if m.matches(stmt, m.ClassDef())'
        - '            or m.matches(stmt, m.FunctionDef())'
        - '            or ('
        - '                self.keep_constant'
        - '                and m.matches(stmt, m.SimpleStatementLine())'
        - '                and m.matches(stmt.body[0], m.Assign())'
        - '            )'
        - '        ]'
        - '        return updated_node.with_changes(body=new_body)'
      - end_line: 41
        name: leave_ClassDef
        start_line: 28
        text:
        - '    def leave_ClassDef('
        - '        self, original_node: cst.ClassDef, updated_node: cst.ClassDef'
        - '    ) -> cst.ClassDef:'
        - '        # Remove docstring in the class body'
        - '        new_body = ['
        - '            stmt'
        - '            for stmt in updated_node.body.body'
        - '            if not ('
        - '                m.matches(stmt, m.SimpleStatementLine())'
        - '                and m.matches(stmt.body[0], m.Expr())'
        - '                and m.matches(stmt.body[0].value, m.SimpleString())'
        - '            )'
        - '        ]'
        - '        return updated_node.with_changes(body=cst.IndentedBlock(body=new_body))'
      - end_line: 49
        name: leave_FunctionDef
        start_line: 43
        text:
        - '    def leave_FunctionDef('
        - '        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef'
        - '    ) -> cst.CSTNode:'
        - '        new_expr = cst.Expr(value=cst.SimpleString(value=self.replacement_string))'
        - '        new_body = cst.IndentedBlock((new_expr,))'
        - '        # another way: replace with pass?'
        - '        return updated_node.with_changes(body=new_body)'
      name: CompressTransformer
      start_line: 5
      text:
      - 'class CompressTransformer(cst.CSTTransformer):'
      - '    DESCRIPTION = str = "Replaces function body with ..."'
      - '    replacement_string = ''"$$FUNC_BODY_REPLACEMENT_STRING$$"'''
      - ''
      - '    def __init__(self, keep_constant=True):'
      - '        self.keep_constant = keep_constant'
      - ''
      - '    def leave_Module('
      - '        self, original_node: cst.Module, updated_node: cst.Module'
      - '    ) -> cst.Module:'
      - '        new_body = ['
      - '            stmt'
      - '            for stmt in updated_node.body'
      - '            if m.matches(stmt, m.ClassDef())'
      - '            or m.matches(stmt, m.FunctionDef())'
      - '            or ('
      - '                self.keep_constant'
      - '                and m.matches(stmt, m.SimpleStatementLine())'
      - '                and m.matches(stmt.body[0], m.Assign())'
      - '            )'
      - '        ]'
      - '        return updated_node.with_changes(body=new_body)'
      - ''
      - '    def leave_ClassDef('
      - '        self, original_node: cst.ClassDef, updated_node: cst.ClassDef'
      - '    ) -> cst.ClassDef:'
      - '        # Remove docstring in the class body'
      - '        new_body = ['
      - '            stmt'
      - '            for stmt in updated_node.body.body'
      - '            if not ('
      - '                m.matches(stmt, m.SimpleStatementLine())'
      - '                and m.matches(stmt.body[0], m.Expr())'
      - '                and m.matches(stmt.body[0].value, m.SimpleString())'
      - '            )'
      - '        ]'
      - '        return updated_node.with_changes(body=cst.IndentedBlock(body=new_body))'
      - ''
      - '    def leave_FunctionDef('
      - '        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef'
      - '    ) -> cst.CSTNode:'
      - '        new_expr = cst.Expr(value=cst.SimpleString(value=self.replacement_string))'
      - '        new_body = cst.IndentedBlock((new_expr,))'
      - '        # another way: replace with pass?'
      - '        return updated_node.with_changes(body=new_body)'
    functions:
    - end_line: 88
      name: get_skeleton
      start_line: 77
      text:
      - 'def get_skeleton(raw_code, keep_constant: bool = True):'
      - '    try:'
      - '        tree = cst.parse_module(raw_code)'
      - '    except:'
      - '        return raw_code'
      - ''
      - '    transformer = CompressTransformer(keep_constant=keep_constant)'
      - '    modified_tree = tree.visit(transformer)'
      - '    code = modified_tree.code'
      - '    code = code.replace(CompressTransformer.replacement_string + "\n", "...\n")'
      - '    code = code.replace(CompressTransformer.replacement_string, "...\n")'
      - '    return code'
    - end_line: 93
      name: test_compress
      start_line: 91
      text:
      - 'def test_compress():'
      - '    skeleton = get_skeleton(code, True)'
      - '    print(skeleton)'
    text:
    - import libcst as cst
    - import libcst.matchers as m
    - ''
    - ''
    - 'class CompressTransformer(cst.CSTTransformer):'
    - '    DESCRIPTION = str = "Replaces function body with ..."'
    - '    replacement_string = ''"$$FUNC_BODY_REPLACEMENT_STRING$$"'''
    - ''
    - '    def __init__(self, keep_constant=True):'
    - '        self.keep_constant = keep_constant'
    - ''
    - '    def leave_Module('
    - '        self, original_node: cst.Module, updated_node: cst.Module'
    - '    ) -> cst.Module:'
    - '        new_body = ['
    - '            stmt'
    - '            for stmt in updated_node.body'
    - '            if m.matches(stmt, m.ClassDef())'
    - '            or m.matches(stmt, m.FunctionDef())'
    - '            or ('
    - '                self.keep_constant'
    - '                and m.matches(stmt, m.SimpleStatementLine())'
    - '                and m.matches(stmt.body[0], m.Assign())'
    - '            )'
    - '        ]'
    - '        return updated_node.with_changes(body=new_body)'
    - ''
    - '    def leave_ClassDef('
    - '        self, original_node: cst.ClassDef, updated_node: cst.ClassDef'
    - '    ) -> cst.ClassDef:'
    - '        # Remove docstring in the class body'
    - '        new_body = ['
    - '            stmt'
    - '            for stmt in updated_node.body.body'
    - '            if not ('
    - '                m.matches(stmt, m.SimpleStatementLine())'
    - '                and m.matches(stmt.body[0], m.Expr())'
    - '                and m.matches(stmt.body[0].value, m.SimpleString())'
    - '            )'
    - '        ]'
    - '        return updated_node.with_changes(body=cst.IndentedBlock(body=new_body))'
    - ''
    - '    def leave_FunctionDef('
    - '        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef'
    - '    ) -> cst.CSTNode:'
    - '        new_expr = cst.Expr(value=cst.SimpleString(value=self.replacement_string))'
    - '        new_body = cst.IndentedBlock((new_expr,))'
    - '        # another way: replace with pass?'
    - '        return updated_node.with_changes(body=new_body)'
    - ''
    - ''
    - code = """
    - \"\"\"
    - this is a module
    - '...'
    - \"\"\"
    - const = {1,2,3}
    - import os
    - ''
    - 'class fooClass:'
    - '    ''''''this is a class'''''''
    - ''
    - '    def __init__(self, x):'
    - '        ''''''initialization.'''''''
    - '        self.x = x'
    - ''
    - '    def print(self):'
    - '        print(self.x)'
    - ''
    - 'def test():'
    - '    a = fooClass(3)'
    - '    a.print()'
    - ''
    - '"""'
    - ''
    - ''
    - 'def get_skeleton(raw_code, keep_constant: bool = True):'
    - '    try:'
    - '        tree = cst.parse_module(raw_code)'
    - '    except:'
    - '        return raw_code'
    - ''
    - '    transformer = CompressTransformer(keep_constant=keep_constant)'
    - '    modified_tree = tree.visit(transformer)'
    - '    code = modified_tree.code'
    - '    code = code.replace(CompressTransformer.replacement_string + "\n", "...\n")'
    - '    code = code.replace(CompressTransformer.replacement_string, "...\n")'
    - '    return code'
    - ''
    - ''
    - 'def test_compress():'
    - '    skeleton = get_skeleton(code, True)'
    - '    print(skeleton)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    test_compress()'
  parse_global_var.py:
    classes:
    - end_line: 22
      methods:
      - end_line: 11
        name: __init__
        start_line: 10
        text:
        - '    def __init__(self):'
        - '        self.global_assigns = []'
      - end_line: 22
        name: leave_Module
        start_line: 13
        text:
        - '    def leave_Module(self, original_node: cst.Module) -> list:'
        - '        assigns = []'
        - '        for stmt in original_node.body:'
        - '            if m.matches(stmt, m.SimpleStatementLine()) and m.matches('
        - '                stmt.body[0], m.Assign()'
        - '            ):'
        - '                start_pos = self.get_metadata(cst.metadata.PositionProvider,
          stmt).start'
        - '                end_pos = self.get_metadata(cst.metadata.PositionProvider,
          stmt).end'
        - '                assigns.append([stmt, start_pos, end_pos])'
        - '        self.global_assigns.extend(assigns)'
      name: GlobalVariableVisitor
      start_line: 7
      text:
      - 'class GlobalVariableVisitor(cst.CSTVisitor):'
      - '    METADATA_DEPENDENCIES = (cst.metadata.PositionProvider,)'
      - ''
      - '    def __init__(self):'
      - '        self.global_assigns = []'
      - ''
      - '    def leave_Module(self, original_node: cst.Module) -> list:'
      - '        assigns = []'
      - '        for stmt in original_node.body:'
      - '            if m.matches(stmt, m.SimpleStatementLine()) and m.matches('
      - '                stmt.body[0], m.Assign()'
      - '            ):'
      - '                start_pos = self.get_metadata(cst.metadata.PositionProvider,
        stmt).start'
      - '                end_pos = self.get_metadata(cst.metadata.PositionProvider,
        stmt).end'
      - '                assigns.append([stmt, start_pos, end_pos])'
      - '        self.global_assigns.extend(assigns)'
    functions:
    - end_line: 52
      name: parse_global_var_from_code
      start_line: 25
      text:
      - 'def parse_global_var_from_code(file_content: str) -> dict[str, dict]:'
      - '    """Parse global variables."""'
      - '    try:'
      - '        tree = cst.parse_module(file_content)'
      - '    except:'
      - '        return file_content'
      - ''
      - '    wrapper = cst.metadata.MetadataWrapper(tree)'
      - '    visitor = GlobalVariableVisitor()'
      - '    wrapper.visit(visitor)'
      - ''
      - '    global_assigns = {}'
      - '    for assign_stmt, start_pos, end_pos in visitor.global_assigns:'
      - '        for t in assign_stmt.body:'
      - '            try:'
      - '                targets = [t.targets[0].target.value]'
      - '            except:'
      - '                try:'
      - '                    targets = t.targets[0].target.elements'
      - '                    targets = [x.value.value for x in targets]'
      - '                except:'
      - '                    targets = []'
      - '            for target_var in targets:'
      - '                global_assigns[target_var] = {'
      - '                    "start_line": start_pos.line,'
      - '                    "end_line": end_pos.line,'
      - '                }'
      - '    return global_assigns'
    - end_line: 90
      name: test_parse_global_var_from_file
      start_line: 55
      text:
      - 'def test_parse_global_var_from_file():'
      - '    code = """'
      - \"\"\"
      - this is a module
      - '...'
      - \"\"\"
      - const_var = {1,2,3}
      - const_dict = {
      - '    ''day'': ''Monday'','
      - '    ''month'': ''January'','
      - '}'
      - a, b = 1, 2
      - import os
      - ''
      - 'class fooClass:'
      - '    ''''''this is a class'''''''
      - ''
      - '    def __init__(self, x):'
      - '        ''''''initialization.'''''''
      - '        self.x = x'
      - ''
      - '    def print(self):'
      - '        print(self.x)'
      - ''
      - 'def test():'
      - '    a = fooClass(3)'
      - '    a.print()'
      - ''
      - '"""'
      - '    res = parse_global_var_from_code(code)'
      - '    assert res == {'
      - '        "const_var": {"start_line": 6, "end_line": 6},'
      - '        "const_dict": {"start_line": 7, "end_line": 10},'
      - '        "a": {"start_line": 11, "end_line": 11},'
      - '        "b": {"start_line": 11, "end_line": 11},'
      - '    }'
    text:
    - '# TODO: maybe merge this into the structure preprocessing.'
    - import libcst as cst
    - import libcst.matchers as m
    - from libcst.display import dump
    - ''
    - ''
    - 'class GlobalVariableVisitor(cst.CSTVisitor):'
    - '    METADATA_DEPENDENCIES = (cst.metadata.PositionProvider,)'
    - ''
    - '    def __init__(self):'
    - '        self.global_assigns = []'
    - ''
    - '    def leave_Module(self, original_node: cst.Module) -> list:'
    - '        assigns = []'
    - '        for stmt in original_node.body:'
    - '            if m.matches(stmt, m.SimpleStatementLine()) and m.matches('
    - '                stmt.body[0], m.Assign()'
    - '            ):'
    - '                start_pos = self.get_metadata(cst.metadata.PositionProvider,
      stmt).start'
    - '                end_pos = self.get_metadata(cst.metadata.PositionProvider,
      stmt).end'
    - '                assigns.append([stmt, start_pos, end_pos])'
    - '        self.global_assigns.extend(assigns)'
    - ''
    - ''
    - 'def parse_global_var_from_code(file_content: str) -> dict[str, dict]:'
    - '    """Parse global variables."""'
    - '    try:'
    - '        tree = cst.parse_module(file_content)'
    - '    except:'
    - '        return file_content'
    - ''
    - '    wrapper = cst.metadata.MetadataWrapper(tree)'
    - '    visitor = GlobalVariableVisitor()'
    - '    wrapper.visit(visitor)'
    - ''
    - '    global_assigns = {}'
    - '    for assign_stmt, start_pos, end_pos in visitor.global_assigns:'
    - '        for t in assign_stmt.body:'
    - '            try:'
    - '                targets = [t.targets[0].target.value]'
    - '            except:'
    - '                try:'
    - '                    targets = t.targets[0].target.elements'
    - '                    targets = [x.value.value for x in targets]'
    - '                except:'
    - '                    targets = []'
    - '            for target_var in targets:'
    - '                global_assigns[target_var] = {'
    - '                    "start_line": start_pos.line,'
    - '                    "end_line": end_pos.line,'
    - '                }'
    - '    return global_assigns'
    - ''
    - ''
    - 'def test_parse_global_var_from_file():'
    - '    code = """'
    - \"\"\"
    - this is a module
    - '...'
    - \"\"\"
    - const_var = {1,2,3}
    - const_dict = {
    - '    ''day'': ''Monday'','
    - '    ''month'': ''January'','
    - '}'
    - a, b = 1, 2
    - import os
    - ''
    - 'class fooClass:'
    - '    ''''''this is a class'''''''
    - ''
    - '    def __init__(self, x):'
    - '        ''''''initialization.'''''''
    - '        self.x = x'
    - ''
    - '    def print(self):'
    - '        print(self.x)'
    - ''
    - 'def test():'
    - '    a = fooClass(3)'
    - '    a.print()'
    - ''
    - '"""'
    - '    res = parse_global_var_from_code(code)'
    - '    assert res == {'
    - '        "const_var": {"start_line": 6, "end_line": 6},'
    - '        "const_dict": {"start_line": 7, "end_line": 10},'
    - '        "a": {"start_line": 11, "end_line": 11},'
    - '        "b": {"start_line": 11, "end_line": 11},'
    - '    }'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    test_parse_global_var_from_file()'
  postprocess_data.py:
    classes:
    - end_line: 230
      methods:
      - end_line: 215
        name: __init__
        start_line: 214
        text:
        - '        def __init__(self):'
        - '            self.parents = []'
      - end_line: 220
        name: visit
        start_line: 217
        text:
        - '        def visit(self, node):'
        - '            self.parents.append(node)'
        - '            super().visit(node)'
        - '            self.parents.pop()'
      - end_line: 225
        name: visit_FunctionDef
        start_line: 222
        text:
        - '        def visit_FunctionDef(self, node):'
        - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
        - '                functions[node.name] = ast.unparse(node)'
        - '            self.generic_visit(node)'
      - end_line: 230
        name: visit_AsyncFunctionDef
        start_line: 227
        text:
        - '        def visit_AsyncFunctionDef(self, node):'
        - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
        - '                functions[node.name] = ast.unparse(node)'
        - '            self.generic_visit(node)'
      name: FunctionVisitor
      start_line: 213
      text:
      - '    class FunctionVisitor(ast.NodeVisitor):'
      - '        def __init__(self):'
      - '            self.parents = []'
      - ''
      - '        def visit(self, node):'
      - '            self.parents.append(node)'
      - '            super().visit(node)'
      - '            self.parents.pop()'
      - ''
      - '        def visit_FunctionDef(self, node):'
      - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
      - '                functions[node.name] = ast.unparse(node)'
      - '            self.generic_visit(node)'
      - ''
      - '        def visit_AsyncFunctionDef(self, node):'
      - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
      - '                functions[node.name] = ast.unparse(node)'
      - '            self.generic_visit(node)'
    - end_line: 240
      methods:
      - end_line: 240
        name: visit_ClassDef
        start_line: 233
        text:
        - '        def visit_ClassDef(self, node):'
        - '            class_name = node.name'
        - '            for body_item in node.body:'
        - '                if isinstance(body_item, ast.FunctionDef) or isinstance('
        - '                    body_item, ast.AsyncFunctionDef'
        - '                ):'
        - '                    functions[f"{class_name}.{body_item.name}"] = ast.unparse(body_item)'
        - '            self.generic_visit(node)'
      name: ClassVisitor
      start_line: 232
      text:
      - '    class ClassVisitor(ast.NodeVisitor):'
      - '        def visit_ClassDef(self, node):'
      - '            class_name = node.name'
      - '            for body_item in node.body:'
      - '                if isinstance(body_item, ast.FunctionDef) or isinstance('
      - '                    body_item, ast.AsyncFunctionDef'
      - '                ):'
      - '                    functions[f"{class_name}.{body_item.name}"] = ast.unparse(body_item)'
      - '            self.generic_visit(node)'
    functions:
    - end_line: 21
      name: check_syntax
      start_line: 14
      text:
      - 'def check_syntax(code):'
      - '    if not code.strip():  # Check for cases where the model didn''t return
        a python block'
      - '        return False'
      - '    try:'
      - '        ast.parse(code)'
      - '    except SyntaxError as e:'
      - '        return False'
      - '    return True'
    - end_line: 29
      name: remove_empty_lines
      start_line: 24
      text:
      - 'def remove_empty_lines(code: str) -> str:'
      - '    # Split the code into lines'
      - '    lines = code.splitlines()'
      - '    # Remove empty lines'
      - '    filtered_lines = [line for line in lines if line.strip() != ""]'
      - '    return "\n".join(filtered_lines)'
    - end_line: 37
      name: check_code_differ_by_just_empty_lines
      start_line: 32
      text:
      - 'def check_code_differ_by_just_empty_lines(code, prev_code) -> bool:'
      - '    # Normalize both code snippets'
      - '    normalized_code1 = remove_empty_lines(code)'
      - '    normalized_code2 = remove_empty_lines(prev_code)'
      - ''
      - '    return normalized_code1 == normalized_code2'
    - end_line: 92
      name: lint_code
      start_line: 40
      text:
      - 'def lint_code(repo_playground, temp_name, code, prev_code="") -> tuple[bool,
        set, set]:'
      - ''
      - '    # Generate a temperary folder and add uuid to avoid collision'
      - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
      - ''
      - '    # assert playground doesn''t exist'
      - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
        exists"'
      - ''
      - '    # create playground'
      - '    os.makedirs(repo_playground)'
      - ''
      - '    with open(f"{repo_playground}/{temp_name}", "w") as f:'
      - '        f.write(prev_code)'
      - ''
      - '    # lint the code'
      - '    # check for fatal errors'
      - '    fatal = "E9,F821,F823,F831,F406,F407,F701,F702,F704,F706"'
      - '    o = subprocess.run('
      - '        f"flake8 --select={fatal} --isolated {repo_playground}/{temp_name}",'
      - '        shell=True,'
      - '        capture_output=True,'
      - '    )'
      - '    s = o.stdout.decode("utf-8")'
      - ''
      - '    prev_errors = set()'
      - '    if s != "":'
      - '        for error in s.split(f"{repo_playground}/{temp_name}:")[1:]:'
      - '            num_free_error = ":".join(error.split(":")[2:]).strip()'
      - '            prev_errors.add(num_free_error)'
      - ''
      - '    with open(f"{repo_playground}/{temp_name}", "w") as f:'
      - '        f.write(code)'
      - ''
      - '    o = subprocess.run('
      - '        f"flake8 --select={fatal} --isolated {repo_playground}/{temp_name}",'
      - '        shell=True,'
      - '        capture_output=True,'
      - '    )'
      - '    s = o.stdout.decode("utf-8")'
      - ''
      - '    # remove playground'
      - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
      - ''
      - '    errors = set()'
      - '    if s != "":'
      - '        for error in s.split(f"{repo_playground}/{temp_name}:")[1:]:'
      - '            num_free_error = ":".join(error.split(":")[2:]).strip()'
      - '            errors.add(num_free_error)'
      - ''
      - '    if len(errors - prev_errors) > 0:'
      - '        return False, prev_errors, errors'
      - ''
      - '    return True, set(), set()'
    - end_line: 138
      name: fake_git_repo
      start_line: 95
      text:
      - 'def fake_git_repo(repo_playground, file_path, old_content, new_content) ->
        str:'
      - '    """create a fake git repo to obtain git diff format"""'
      - ''
      - '    # Generate a temperary folder and add uuid to avoid collision'
      - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
      - ''
      - '    # assert playground doesn''t exist'
      - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
        exists"'
      - ''
      - '    # create playground'
      - '    os.makedirs(repo_playground)'
      - ''
      - '    # create a fake git repo'
      - '    subprocess.run(f"cd {repo_playground} && git init", shell=True)'
      - ''
      - '    # create a file'
      - '    subprocess.run('
      - '        f"mkdir -p {repo_playground}/{os.path.dirname(file_path)}", shell=True'
      - '    )'
      - ''
      - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
      - '        f.write(old_content)'
      - ''
      - '    # add file to git'
      - '    subprocess.run('
      - '        f"cd {repo_playground} && git add {file_path} && git commit -m ''initial
        commit''",'
      - '        shell=True,'
      - '    )'
      - ''
      - '    # edit file'
      - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
      - '        f.write(new_content)'
      - ''
      - '    # get git diff'
      - '    o = subprocess.run('
      - '        f"cd {repo_playground} && git diff {file_path}", shell=True, capture_output=True'
      - '    )'
      - ''
      - '    s = o.stdout.decode("utf-8")'
      - ''
      - '    # remove playground'
      - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
      - ''
      - '    return s'
    - end_line: 206
      name: fake_git_apply
      start_line: 141
      text:
      - 'def fake_git_apply(repo_playground, file_path, old_content, patch) -> str:'
      - '    """create a fake git repo to obtain new file content"""'
      - ''
      - '    # Generate a temperary folder and add uuid to avoid collision'
      - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
      - ''
      - '    # assert playground doesn''t exist'
      - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
        exists"'
      - ''
      - '    # create playground'
      - '    os.makedirs(repo_playground)'
      - ''
      - '    # create a fake git repo'
      - '    subprocess.run(f"cd {repo_playground} && git init", shell=True)'
      - ''
      - '    # create a file'
      - '    subprocess.run('
      - '        f"mkdir -p {repo_playground}/{os.path.dirname(file_path)}", shell=True'
      - '    )'
      - ''
      - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
      - '        f.write(old_content)'
      - ''
      - '    # add file to git'
      - '    subprocess.run('
      - '        f"cd {repo_playground} && git add {file_path} && git commit -m ''initial
        commit''",'
      - '        shell=True,'
      - '    )'
      - ''
      - '    # apply patch file'
      - '    patch_file = f"{str(uuid.uuid4())}.patch"'
      - '    with open(f"{repo_playground}/{patch_file}", "w") as f:'
      - '        f.write(patch)'
      - '    o = subprocess.run('
      - '        f"cd {repo_playground} && git apply --whitespace=nowarn {patch_file}",'
      - '        shell=True,'
      - '        capture_output=True,'
      - '    )'
      - '    if o.stderr.decode("utf-8"):'
      - '        print("stderr> ", o.stderr.decode("utf-8"))'
      - '        # TODO: This rarely happen but the patch should be valid, needs to
        look into it'
      - ''
      - '        with open(f"{repo_playground}/{file_path}", "w") as f:'
      - '            f.write(old_content + "\n")'
      - ''
      - '        o = subprocess.run('
      - '            f"cd {repo_playground} && git apply --whitespace=nowarn {patch_file}",'
      - '            shell=True,'
      - '            capture_output=True,'
      - '        )'
      - ''
      - '        if o.stderr.decode("utf-8"):'
      - '            print("stderr> ", o.stderr.decode("utf-8"))'
      - '            assert False, "shouldn''t happen"'
      - ''
      - '    # get git diff'
      - '    o = subprocess.run('
      - '        f"cd {repo_playground} && cat {file_path}", shell=True, capture_output=True'
      - '    )'
      - ''
      - '    s = o.stdout.decode("utf-8")'
      - ''
      - '    # remove playground'
      - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
      - ''
      - '    return s'
    - end_line: 244
      name: get_functions
      start_line: 209
      text:
      - 'def get_functions(tree):'
      - '    """Get a set of function and method names from the AST tree."""'
      - '    functions = {}'
      - ''
      - '    class FunctionVisitor(ast.NodeVisitor):'
      - '        def __init__(self):'
      - '            self.parents = []'
      - ''
      - '        def visit(self, node):'
      - '            self.parents.append(node)'
      - '            super().visit(node)'
      - '            self.parents.pop()'
      - ''
      - '        def visit_FunctionDef(self, node):'
      - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
      - '                functions[node.name] = ast.unparse(node)'
      - '            self.generic_visit(node)'
      - ''
      - '        def visit_AsyncFunctionDef(self, node):'
      - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
      - '                functions[node.name] = ast.unparse(node)'
      - '            self.generic_visit(node)'
      - ''
      - '    class ClassVisitor(ast.NodeVisitor):'
      - '        def visit_ClassDef(self, node):'
      - '            class_name = node.name'
      - '            for body_item in node.body:'
      - '                if isinstance(body_item, ast.FunctionDef) or isinstance('
      - '                    body_item, ast.AsyncFunctionDef'
      - '                ):'
      - '                    functions[f"{class_name}.{body_item.name}"] = ast.unparse(body_item)'
      - '            self.generic_visit(node)'
      - ''
      - '    FunctionVisitor().visit(tree)'
      - '    ClassVisitor().visit(tree)'
      - '    return functions'
    - end_line: 269
      name: is_just_new_function
      start_line: 247
      text:
      - 'def is_just_new_function(code1, code2):'
      - '    tree1 = ast.parse(code1)'
      - '    tree2 = ast.parse(code2)'
      - ''
      - '    functions1 = get_functions(tree1)'
      - '    functions2 = get_functions(tree2)'
      - ''
      - '    # The new functions in the second code'
      - '    if len(set(list(functions1.keys())) - set(list(functions2.keys()))) >
        0:'
      - '        # removes functions'
      - '        return False'
      - ''
      - '    for func in functions1:'
      - '        if functions1[func] != functions2[func]:'
      - '            # modifies existing functions'
      - '            return False'
      - ''
      - '    if len(set(list(functions2.keys())) - set(list(functions1.keys()))) >
        0:'
      - '        return True'
      - ''
      - '    # modifying global stuff is okay, because its actually same as functions
        almost.'
      - ''
      - '    return False'
    - end_line: 306
      name: remove_comments_and_docstrings
      start_line: 277
      text:
      - 'def remove_comments_and_docstrings(source):'
      - '    io_obj = io.StringIO(source)'
      - '    out = ""'
      - '    prev_toktype = tokenize.INDENT'
      - '    last_lineno = -1'
      - '    last_col = 0'
      - '    for tok in tokenize.generate_tokens(io_obj.readline):'
      - '        token_type = tok[0]'
      - '        token_string = tok[1]'
      - '        start_line, start_col = tok[2]'
      - '        end_line, end_col = tok[3]'
      - '        ltext = tok[4]'
      - '        if start_line > last_lineno:'
      - '            last_col = 0'
      - '        if start_col > last_col:'
      - '            out += " " * (start_col - last_col)'
      - '        if token_type == tokenize.COMMENT:'
      - '            pass'
      - '        elif token_type == tokenize.STRING:'
      - '            if prev_toktype != tokenize.INDENT:'
      - '                if prev_toktype != tokenize.NEWLINE:'
      - '                    if start_col > 0:'
      - '                        out += token_string'
      - '        else:'
      - '            out += token_string'
      - '        prev_toktype = token_type'
      - '        last_col = end_col'
      - '        last_lineno = end_line'
      - '    out = "\n".join(l for l in out.splitlines() if l.strip())'
      - '    return out'
    - end_line: 371
      name: normalize_patch
      start_line: 309
      text:
      - 'def normalize_patch(instance_id, patch: str, original_file_content: str)
        -> str:'
      - '    "Remove edits to trailing spaces and comments in the patch."'
      - '    if not patch.strip():'
      - '        return ""'
      - '    # Extract info.'
      - '    file_changes = parse_patch(patch)'
      - '    if not file_changes:'
      - '        print(patch)'
      - '        print("=")'
      - '        import json'
      - ''
      - '        print(json.dumps(file_changes, indent=2))'
      - '        exit(0)'
      - ''
      - '    edited_file = file_changes[0]["file"]'
      - '    old_content = original_file_content'
      - '    # Get new file'
      - '    new_content = fake_git_apply("playground", edited_file, old_content,
        patch)'
      - '    if new_content is None:'
      - '        # Error during applying diff'
      - '        # print("ERROR in applying patch")'
      - '        return patch'
      - ''
      - '    # Normalize file contents'
      - '    def normalize_code(code):'
      - '        try:'
      - '            node = ast.parse(code)'
      - '            return ast.unparse(node)'
      - '        except:'
      - '            return code'
      - ''
      - '    old_content = normalize_code(old_content)'
      - '    new_content = normalize_code(new_content)'
      - ''
      - '    try:'
      - '        remove_docstring_old_content = remove_comments_and_docstrings(old_content)'
      - '        ast.parse(remove_docstring_old_content)  # check'
      - '        remove_docstring_new_content = remove_comments_and_docstrings(new_content)'
      - '        ast.parse(remove_docstring_new_content)  # check'
      - '    except:'
      - '        # when does this exception happen?'
      - '        # when the code has some class or function with empty docstring (thats
        valid python code)'
      - '        # but removing it is not, to be save we just use the original.'
      - '        remove_docstring_old_content = old_content'
      - '        remove_docstring_new_content = new_content'
      - ''
      - '    diff = fake_git_repo('
      - '        "playground",'
      - '        edited_file,'
      - '        remove_docstring_old_content,'
      - '        remove_docstring_new_content,'
      - '    )'
      - ''
      - '    if is_just_new_function(remove_docstring_old_content, remove_docstring_new_content):'
      - '        # modify the diff to ignore context.'
      - '        new_diff = []'
      - '        for line in diff.splitlines():'
      - '            if line.startswith("-") or line.startswith("+"):'
      - '                new_diff.append(line)'
      - '        diff = "\n".join(new_diff)'
      - ''
      - '    # Note that the normalized diff may not be applied to the original file.'
      - '    return diff'
    - end_line: 384
      name: extract_python_blocks
      start_line: 374
      text:
      - 'def extract_python_blocks(text):'
      - '    # Regular expression pattern to match ```python\n{text}\n```'
      - '    pattern = r"```python\n(.*?)\n```"'
      - ''
      - '    with open("text.txt", "w") as f:'
      - '        f.write(text)'
      - ''
      - '    # Use re.findall to find all matches'
      - '    matches = re.findall(pattern, text, re.DOTALL)'
      - ''
      - '    return matches'
    - end_line: 394
      name: extract_code_blocks
      start_line: 387
      text:
      - 'def extract_code_blocks(text):'
      - '    pattern = r"```\n(.*?)\n```"'
      - '    matches = re.findall(pattern, text, re.DOTALL)'
      - '    if len(matches) == 0:'
      - '        if "```" in text:'
      - '            # handle the case where the code block is not complete'
      - '            return [text.split("```", 1)[-1].strip()]'
      - '    return matches'
    - end_line: 436
      name: extract_locs_for_files
      start_line: 397
      text:
      - 'def extract_locs_for_files(locs, file_names):'
      - '    # TODO: keep the order from this fine-grained FL results.'
      - ''
      - '    '
      - ''
      - '    results = {fn: [] for fn in file_names} # dictionary with file names
        as the keys'
      - '    with open("localize_related.txt", "a") as f:'
      - '        f.write("Results Dict (contains filenames)\n" + str(results) + "\n"*3)'
      - '        f.write("Locs:\n" + str(locs) + "\n"*3)'
      - ''
      - '    current_file_name = None'
      - '    for loc in locs:'
      - '        for line in loc.splitlines():'
      - ''
      - '            with open("localize_related.txt", "a") as f:'
      - '                f.write("Current Line:\n" + str(line) + "\n")'
      - '                f.write("processed: " + str(line.strip().split(" ")[1]) +
        "\n") '
      - ''
      - '            if line.strip().endswith(".py"):'
      - '                current_file_name = line.strip().split(" ")[1]'
      - '              '
      - '            elif line.strip() and any('
      - '                line.startswith(w)'
      - '                for w in ["line:", "function:", "class:", "variable:", "path:"]
        # add a path '
      - '            ):'
      - '                if current_file_name in results: # only take file name in
        results '
      - '                    with open("localize_related.txt", "a") as f:'
      - '                        f.write("Curr filename found in results\n")'
      - '                    results[current_file_name].append(line)'
      - '                else:'
      - '                    pass'
      - '    output = [["\n".join(results[fn])] for fn in file_names]'
      - ''
      - ''
      - '    with open("localize_related.txt", "a") as f:'
      - '        f.write("Model Found Locs Separated:\n")'
      - '        for i in output:'
      - '            f.write(str(i) + "\n"*3)'
      - ''
      - '    return output'
    - end_line: 440
      name: extract_starting_number
      start_line: 439
      text:
      - 'def extract_starting_number(subcommand):'
      - '    return int(subcommand.split(",")[0].split("start=")[-1])'
    - end_line: 444
      name: extract_ending_number
      start_line: 443
      text:
      - 'def extract_ending_number(subcommand):'
      - '    return int(subcommand.split(",")[1].split("end=")[-1])'
    - end_line: 454
      name: overlap
      start_line: 447
      text:
      - 'def overlap(subcommand1, subcommand2):'
      - '    start1, end1 = extract_starting_number(subcommand1), extract_ending_number('
      - '        subcommand1'
      - '    )'
      - '    start2, end2 = extract_starting_number(subcommand2), extract_ending_number('
      - '        subcommand2'
      - '    )'
      - '    return not (end1 < start2 or end2 < start1)'
    - end_line: 495
      name: split_edit_multifile_commands
      start_line: 457
      text:
      - 'def split_edit_multifile_commands(commands, diff_format=False) -> dict[str,
        str]:'
      - '    """Split commands based on edited files."""'
      - '    file_to_commands = OrderedDict()'
      - '    if diff_format:'
      - '        for command in commands:'
      - '            file_name = None'
      - '            for subcommand in command.split(">>>>>>> REPLACE")[:-1]:'
      - '                subcommand = subcommand.strip()'
      - '                if "<<<<<<< SEARCH" in subcommand:'
      - '                    fn = subcommand.split("<<<<<<< SEARCH")[0].lstrip("#").strip()'
      - '                    if fn:'
      - '                        file_name = "''" + fn + "''"'
      - ''
      - '                if len(subcommand.split("<<<<<<< SEARCH")) != 2:'
      - '                    continue'
      - '                converted_command = ('
      - '                    "<<<<<<< SEARCH"'
      - '                    + subcommand.split("<<<<<<< SEARCH")[1]'
      - '                    + "\n"'
      - '                    + ">>>>>>> REPLACE"'
      - '                )'
      - '                # deduplicate'
      - '                if ('
      - '                    file_name not in file_to_commands'
      - '                    or converted_command not in file_to_commands[file_name]'
      - '                ):'
      - '                    file_to_commands.setdefault(file_name, []).append(converted_command)'
      - '    else:'
      - '        for command in commands:'
      - '            for subcommand in command.split("edit_file(")[1:]:'
      - '                file_name, start, end, content = subcommand.split(",", 3)'
      - '                converted_command = "edit_file(" + ",".join([start, end,
        content])'
      - '                # deduplicate'
      - '                if ('
      - '                    file_name not in file_to_commands'
      - '                    or converted_command not in file_to_commands[file_name]'
      - '                ):'
      - '                    file_to_commands.setdefault(file_name, []).append(converted_command)'
      - '    return file_to_commands'
    - end_line: 605
      name: parse_diff_edit_commands
      start_line: 498
      text:
      - def parse_diff_edit_commands(
      - '    commands, content, file_loc_intervals: list[tuple[int, int]]'
      - '):'
      - '    def parse_for_threedots(original, replace, file_loc_intervals, content):'
      - '        # if dot dot dot in replace, its always safe to remove it'
      - '        if replace.startswith("...\n") and len(replace) > 4:'
      - '            # im parsing this first because its used later on'
      - '            replace = replace[4:]'
      - ''
      - '        # just dot dot dot, then need to do something special'
      - '        if original == "...":'
      - '            if not replace[0].isspace():'
      - '                # this is okay'
      - '                # find a suitable original string to replace'
      - '                for interval in file_loc_intervals:'
      - '                    start, end = interval'
      - '                    start = max(start - 1, 0)'
      - '                    context_segment = "\n".join(content.splitlines()[start:end])'
      - ''
      - '                    for line in context_segment.splitlines():'
      - '                        if len(line) > 0 and not line[0].isspace():'
      - '                            if content.count(line) == 1:'
      - '                                original = line'
      - '                                # keep the line'
      - '                                replace = replace + "\n\n" + line'
      - '                                break'
      - ''
      - '                    if original != "...":'
      - '                        break'
      - ''
      - '                if original == "...":'
      - '                    print("cannot find suitable location")'
      - ''
      - '        # dot dot dot with something else in original, then its safe to replace'
      - '        if original.startswith("...\n") and len(original) > 4:'
      - '            # remove dot dot dot from original'
      - '            original = original[4:]'
      - ''
      - '        return original, replace'
      - ''
      - '    if len(file_loc_intervals) == 0:'
      - '        if original in content:'
      - '            content = content.replace(original, replace)'
      - '            replaced = True'
      - '        else:'
      - '            print("not replaced")'
      - '        return content'
      - '    # let''s first make sure the intervals are sorted'
      - '    file_loc_intervals.sort()'
      - '    replaced = False'
      - '    # apply the edits from the end of file to the beginning of file'
      - '    # this is to make sure context is correct'
      - '    for interval in file_loc_intervals[::-1]:'
      - '        start, end = interval'
      - '        start = max(start - 1, 0)'
      - '        context_segment = "\n".join(content.splitlines()[start:end])'
      - '        context_segment = "\n" + context_segment + "\n"'
      - ''
      - '        # since we want to replace the original context, let''s first check
        for all edits.'
      - '        can_apply = []'
      - '        for subcommand in commands:'
      - '            if not subcommand.startswith("<<<<<<< SEARCH") and subcommand.endswith('
      - '                ">>>>>>> REPLACE"'
      - '            ):'
      - '                continue'
      - ''
      - '            subcommand = "\n".join(subcommand.splitlines()[1:-1])'
      - '            if len(subcommand.split("\n=======\n")) != 2:'
      - '                continue'
      - ''
      - '            original, replace = subcommand.split("\n=======\n")'
      - ''
      - '            original, replace = parse_for_threedots('
      - '                original, replace, file_loc_intervals, content'
      - '            )'
      - ''
      - '            original = "\n" + original + "\n"'
      - '            replace = "\n" + replace + "\n"'
      - ''
      - '            if original in context_segment:'
      - '                can_apply.append(subcommand)'
      - ''
      - '        # apply edits backwards'
      - '        for subcommand in can_apply[::-1]:'
      - '            original, replace = subcommand.split("\n=======\n")'
      - ''
      - '            original, replace = parse_for_threedots('
      - '                original, replace, file_loc_intervals, content'
      - '            )'
      - ''
      - '            original = "\n" + original + "\n"'
      - '            replace = "\n" + replace + "\n"'
      - '            if ('
      - '                original in context_segment'
      - '            ):  # This may not be true after some previously applied edits'
      - '                context_segment = context_segment.replace(original, replace)'
      - '                replaced = True'
      - '        # reassembly'
      - '        content = ('
      - '            "\n".join(content.splitlines()[:start])'
      - '            + context_segment'
      - '            + "\n".join(content.splitlines()[end:])'
      - '        )'
      - ''
      - '    if not replaced:'
      - '        print("not replaced")'
      - ''
      - '    return content'
    - end_line: 676
      name: parse_edit_commands
      start_line: 608
      text:
      - 'def parse_edit_commands(commands, content):'
      - '    content_lines = content.splitlines()'
      - '    map_content_lines_to_line_num = [i for i in range(0, len(content_lines)
        + 1)]'
      - ''
      - '    # Make a list of the subcommands'
      - '    subcommands = []'
      - ''
      - '    for command in commands:'
      - '        for subcommand in command.split("edit_file(")[1:]:'
      - '            subcommands.append(subcommand)'
      - ''
      - '    # Remove duplicates while preserving order'
      - '    seen = set()'
      - '    unique_subcommands = []'
      - '    for subcommand in subcommands:'
      - '        if subcommand not in seen:'
      - '            unique_subcommands.append(subcommand)'
      - '            seen.add(subcommand)'
      - ''
      - '    # Sort the unique subcommands by the starting number in reverse order'
      - '    unique_sorted_subcommands = sorted('
      - '        unique_subcommands, key=extract_starting_number, reverse=True'
      - '    )'
      - ''
      - '    for subcommand in unique_sorted_subcommands:'
      - '        command_start = int(subcommand.split(",")[0].split("start=")[-1])'
      - '        command_end = int(subcommand.split(",")[1].split("end=")[-1])'
      - ''
      - '        start = map_content_lines_to_line_num.index(command_start)'
      - '        end = map_content_lines_to_line_num.index(command_end)'
      - ''
      - '        try:'
      - '            changed_content = eval('
      - '                ")".join(",".join(subcommand.split(",")[2:]).split(")")[:-1])'
      - '            )'
      - '            # small thing to ensure right white space indent'
      - '            if ('
      - '                start == end'
      - '                and len(changed_content.splitlines()) == 1'
      - '                and (len(changed_content) - len(changed_content.lstrip()))
        == 0'
      - '            ):'
      - '                indent_length = len(content_lines[start - 1]) - len('
      - '                    content_lines[start - 1].lstrip()'
      - '                )'
      - '                changed_content = " " * indent_length + changed_content.lstrip()'
      - ''
      - '        # catch syntax error'
      - '        except:'
      - '            # try to fix, specially for case where there are """ or ''''''
        in the string, that cannot be'
      - '            # easily evaluated.'
      - '            eval_str = ")".join('
      - '                ",".join(subcommand.split(",")[2:]).split(")")[:-1]'
      - '            ).strip()'
      - ''
      - '            if eval_str.startswith("content="):'
      - '                eval_str = eval_str[8:]'
      - ''
      - '            if eval_str.startswith(''"""'') or eval_str.startswith("''''''"):'
      - '                eval_str = eval_str[3:-3]'
      - '            if eval_str.startswith(''"'') or eval_str.startswith("''"):'
      - '                eval_str = eval_str[1:-1]'
      - ''
      - '            changed_content = eval_str'
      - ''
      - '        content_lines[start - 1 : end] = changed_content.splitlines()'
      - ''
      - '    content = "\n".join(content_lines)'
      - ''
      - '    return content'
    - end_line: 886
      name: test_parse
      start_line: 679
      text:
      - 'def test_parse():'
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 1, "import os")
      - '```'
      - '"""'
      - ''
      - '    content = """'
      - import sys
      - '""".strip()'
      - ''
      - '    commands = extract_python_blocks(raw_output)'
      - ''
      - '    content = parse_edit_commands(commands, content)'
      - ''
      - '    assert content == "import os", content'
      - ''
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 1, '''import os\nimport sys''')
      - '```'
      - '"""'
      - ''
      - '    content = """'
      - import sys
      - '""".strip()'
      - ''
      - '    commands = extract_python_blocks(raw_output)'
      - ''
      - '    content = parse_edit_commands(commands, content)'
      - ''
      - '    assert content == "import os\nimport sys", content'
      - ''
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 1, '''import os''')
      - edit_file(1, 1, '''import sys''')
      - '```'
      - '"""'
      - ''
      - '    content = """'
      - import sys
      - '""".strip()'
      - ''
      - '    commands = extract_python_blocks(raw_output)'
      - ''
      - '    content = parse_edit_commands(commands, content)'
      - ''
      - '    assert content == "import sys", content'
      - ''
      - '    content = """'
      - test
      - testing
      - '""".strip()'
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 1, "testing\ntesting2")
      - edit_file(2, 2, "testing3")
      - '```'
      - '"""'
      - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
      - ''
      - '    assert content == "testing\ntesting2\ntesting3", content'
      - ''
      - '    content = """'
      - test
      - testing
      - testinging
      - '""".strip()'
      - ''
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 2, "testing")
      - edit_file(3, 3, "testing3")
      - '```'
      - '"""'
      - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
      - ''
      - '    assert content == "testing\ntesting3", content'
      - ''
      - '    content = """'
      - test
      - testing
      - testinging
      - test
      - testing
      - '""".strip()'
      - ''
      - '    raw_output = """'
      - '```python'
      - edit_file(1, 2, "testing")
      - edit_file(3, 3, "testing3")
      - edit_file(4, 4, "testing\ntesting2")
      - edit_file(5, 5, "testing3")
      - '```'
      - '"""'
      - ''
      - '    edited_content = """'
      - testing
      - testing3
      - testing
      - testing2
      - testing3
      - '""".strip()'
      - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
      - ''
      - '    assert content == edited_content, content'
      - ''
      - '    # Test for if command is present twice in the output (adapted from real
        output)'
      - '    content = """'
      - test
      - testing
      - testinging
      - test
      - testing
      - '""".strip()'
      - ''
      - '    raw_output = ('
      - '        """'
      - '"To fix the issue where `viewcode`'
      - 1. Add a condition
      - ''
      - '```python'
      - edit_file(4, 4, """
      - '        """'
      - test4
      - '"""'
      - '        """)'
      - '```'
      - ''
      - 2. Ensure that the `doctree_read`
      - ''
      - '```python'
      - edit_file(2, 3, """
      - '        """'
      - test-2-3
      - '"""'
      - '        """)'
      - '```'
      - ''
      - 'Here is the complete set of commands to address the issue:'
      - ''
      - '```python'
      - edit_file(4, 4, """
      - '        """'
      - test4
      - '"""'
      - '        """)'
      - '```'
      - ''
      - '```python'
      - edit_file(2, 3, """
      - '        """'
      - test-2-3
      - '"""'
      - '        """)'
      - '```'
      - '"""'
      - '    )'
      - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
      - ''
      - '    revised_content = """'
      - test
      - test-2-3
      - test4
      - testing
      - '""".strip()'
      - ''
      - '    assert content == revised_content, content'
      - ''
      - '    raw_output = """'
      - '```'
      - django/db/migrations/optimizer.py
      - 'function: MigrationOptimizer.optimize_inner'
      - ''
      - django/db/migrations/operations/fields.py
      - 'function: AlterField.reduce'
      - '```'
      - '"""'
      - '    files = ['
      - '        "django/db/migrations/optimizer.py",'
      - '        "django/db/migrations/operations/fields.py",'
      - '        "django/db/migrations/operations/models.py",'
      - '    ]'
      - '    extracted_locs = extract_locs_for_files([raw_output], files)'
      - '    print(extracted_locs)'
      - '    assert extracted_locs == ['
      - '        ["function: MigrationOptimizer.optimize_inner"],'
      - '        ["function: AlterField.reduce"],'
      - '        [""],'
      - '    ]'
      - ''
      - '    raw_output = """'
      - '```python'
      - edit_file(start=1, end=1, content="testing not")
      - '```'
      - '"""'
      - ''
      - '    content = """'
      - testing
      - '""".strip()'
      - ''
      - '    edited_content = """'
      - testing not
      - '""".strip()'
      - ''
      - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
      - '    assert content == edited_content, edited_content'
    - end_line: 338
      name: normalize_code
      start_line: 333
      text:
      - '    def normalize_code(code):'
      - '        try:'
      - '            node = ast.parse(code)'
      - '            return ast.unparse(node)'
      - '        except:'
      - '            return code'
    - end_line: 536
      name: parse_for_threedots
      start_line: 501
      text:
      - '    def parse_for_threedots(original, replace, file_loc_intervals, content):'
      - '        # if dot dot dot in replace, its always safe to remove it'
      - '        if replace.startswith("...\n") and len(replace) > 4:'
      - '            # im parsing this first because its used later on'
      - '            replace = replace[4:]'
      - ''
      - '        # just dot dot dot, then need to do something special'
      - '        if original == "...":'
      - '            if not replace[0].isspace():'
      - '                # this is okay'
      - '                # find a suitable original string to replace'
      - '                for interval in file_loc_intervals:'
      - '                    start, end = interval'
      - '                    start = max(start - 1, 0)'
      - '                    context_segment = "\n".join(content.splitlines()[start:end])'
      - ''
      - '                    for line in context_segment.splitlines():'
      - '                        if len(line) > 0 and not line[0].isspace():'
      - '                            if content.count(line) == 1:'
      - '                                original = line'
      - '                                # keep the line'
      - '                                replace = replace + "\n\n" + line'
      - '                                break'
      - ''
      - '                    if original != "...":'
      - '                        break'
      - ''
      - '                if original == "...":'
      - '                    print("cannot find suitable location")'
      - ''
      - '        # dot dot dot with something else in original, then its safe to replace'
      - '        if original.startswith("...\n") and len(original) > 4:'
      - '            # remove dot dot dot from original'
      - '            original = original[4:]'
      - ''
      - '        return original, replace'
    text:
    - import ast
    - import copy
    - import logging
    - import os
    - import re
    - import subprocess
    - import uuid
    - from collections import OrderedDict
    - ''
    - from agentless.util.preprocess_data import get_repo_files
    - from get_repo_structure.get_patch_info import parse_patch
    - ''
    - ''
    - 'def check_syntax(code):'
    - '    if not code.strip():  # Check for cases where the model didn''t return
      a python block'
    - '        return False'
    - '    try:'
    - '        ast.parse(code)'
    - '    except SyntaxError as e:'
    - '        return False'
    - '    return True'
    - ''
    - ''
    - 'def remove_empty_lines(code: str) -> str:'
    - '    # Split the code into lines'
    - '    lines = code.splitlines()'
    - '    # Remove empty lines'
    - '    filtered_lines = [line for line in lines if line.strip() != ""]'
    - '    return "\n".join(filtered_lines)'
    - ''
    - ''
    - 'def check_code_differ_by_just_empty_lines(code, prev_code) -> bool:'
    - '    # Normalize both code snippets'
    - '    normalized_code1 = remove_empty_lines(code)'
    - '    normalized_code2 = remove_empty_lines(prev_code)'
    - ''
    - '    return normalized_code1 == normalized_code2'
    - ''
    - ''
    - 'def lint_code(repo_playground, temp_name, code, prev_code="") -> tuple[bool,
      set, set]:'
    - ''
    - '    # Generate a temperary folder and add uuid to avoid collision'
    - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
    - ''
    - '    # assert playground doesn''t exist'
    - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
      exists"'
    - ''
    - '    # create playground'
    - '    os.makedirs(repo_playground)'
    - ''
    - '    with open(f"{repo_playground}/{temp_name}", "w") as f:'
    - '        f.write(prev_code)'
    - ''
    - '    # lint the code'
    - '    # check for fatal errors'
    - '    fatal = "E9,F821,F823,F831,F406,F407,F701,F702,F704,F706"'
    - '    o = subprocess.run('
    - '        f"flake8 --select={fatal} --isolated {repo_playground}/{temp_name}",'
    - '        shell=True,'
    - '        capture_output=True,'
    - '    )'
    - '    s = o.stdout.decode("utf-8")'
    - ''
    - '    prev_errors = set()'
    - '    if s != "":'
    - '        for error in s.split(f"{repo_playground}/{temp_name}:")[1:]:'
    - '            num_free_error = ":".join(error.split(":")[2:]).strip()'
    - '            prev_errors.add(num_free_error)'
    - ''
    - '    with open(f"{repo_playground}/{temp_name}", "w") as f:'
    - '        f.write(code)'
    - ''
    - '    o = subprocess.run('
    - '        f"flake8 --select={fatal} --isolated {repo_playground}/{temp_name}",'
    - '        shell=True,'
    - '        capture_output=True,'
    - '    )'
    - '    s = o.stdout.decode("utf-8")'
    - ''
    - '    # remove playground'
    - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
    - ''
    - '    errors = set()'
    - '    if s != "":'
    - '        for error in s.split(f"{repo_playground}/{temp_name}:")[1:]:'
    - '            num_free_error = ":".join(error.split(":")[2:]).strip()'
    - '            errors.add(num_free_error)'
    - ''
    - '    if len(errors - prev_errors) > 0:'
    - '        return False, prev_errors, errors'
    - ''
    - '    return True, set(), set()'
    - ''
    - ''
    - 'def fake_git_repo(repo_playground, file_path, old_content, new_content) ->
      str:'
    - '    """create a fake git repo to obtain git diff format"""'
    - ''
    - '    # Generate a temperary folder and add uuid to avoid collision'
    - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
    - ''
    - '    # assert playground doesn''t exist'
    - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
      exists"'
    - ''
    - '    # create playground'
    - '    os.makedirs(repo_playground)'
    - ''
    - '    # create a fake git repo'
    - '    subprocess.run(f"cd {repo_playground} && git init", shell=True)'
    - ''
    - '    # create a file'
    - '    subprocess.run('
    - '        f"mkdir -p {repo_playground}/{os.path.dirname(file_path)}", shell=True'
    - '    )'
    - ''
    - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
    - '        f.write(old_content)'
    - ''
    - '    # add file to git'
    - '    subprocess.run('
    - '        f"cd {repo_playground} && git add {file_path} && git commit -m ''initial
      commit''",'
    - '        shell=True,'
    - '    )'
    - ''
    - '    # edit file'
    - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
    - '        f.write(new_content)'
    - ''
    - '    # get git diff'
    - '    o = subprocess.run('
    - '        f"cd {repo_playground} && git diff {file_path}", shell=True, capture_output=True'
    - '    )'
    - ''
    - '    s = o.stdout.decode("utf-8")'
    - ''
    - '    # remove playground'
    - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
    - ''
    - '    return s'
    - ''
    - ''
    - 'def fake_git_apply(repo_playground, file_path, old_content, patch) -> str:'
    - '    """create a fake git repo to obtain new file content"""'
    - ''
    - '    # Generate a temperary folder and add uuid to avoid collision'
    - '    repo_playground = os.path.join(repo_playground, str(uuid.uuid4()))'
    - ''
    - '    # assert playground doesn''t exist'
    - '    assert not os.path.exists(repo_playground), f"{repo_playground} already
      exists"'
    - ''
    - '    # create playground'
    - '    os.makedirs(repo_playground)'
    - ''
    - '    # create a fake git repo'
    - '    subprocess.run(f"cd {repo_playground} && git init", shell=True)'
    - ''
    - '    # create a file'
    - '    subprocess.run('
    - '        f"mkdir -p {repo_playground}/{os.path.dirname(file_path)}", shell=True'
    - '    )'
    - ''
    - '    with open(f"{repo_playground}/{file_path}", "w") as f:'
    - '        f.write(old_content)'
    - ''
    - '    # add file to git'
    - '    subprocess.run('
    - '        f"cd {repo_playground} && git add {file_path} && git commit -m ''initial
      commit''",'
    - '        shell=True,'
    - '    )'
    - ''
    - '    # apply patch file'
    - '    patch_file = f"{str(uuid.uuid4())}.patch"'
    - '    with open(f"{repo_playground}/{patch_file}", "w") as f:'
    - '        f.write(patch)'
    - '    o = subprocess.run('
    - '        f"cd {repo_playground} && git apply --whitespace=nowarn {patch_file}",'
    - '        shell=True,'
    - '        capture_output=True,'
    - '    )'
    - '    if o.stderr.decode("utf-8"):'
    - '        print("stderr> ", o.stderr.decode("utf-8"))'
    - '        # TODO: This rarely happen but the patch should be valid, needs to
      look into it'
    - ''
    - '        with open(f"{repo_playground}/{file_path}", "w") as f:'
    - '            f.write(old_content + "\n")'
    - ''
    - '        o = subprocess.run('
    - '            f"cd {repo_playground} && git apply --whitespace=nowarn {patch_file}",'
    - '            shell=True,'
    - '            capture_output=True,'
    - '        )'
    - ''
    - '        if o.stderr.decode("utf-8"):'
    - '            print("stderr> ", o.stderr.decode("utf-8"))'
    - '            assert False, "shouldn''t happen"'
    - ''
    - '    # get git diff'
    - '    o = subprocess.run('
    - '        f"cd {repo_playground} && cat {file_path}", shell=True, capture_output=True'
    - '    )'
    - ''
    - '    s = o.stdout.decode("utf-8")'
    - ''
    - '    # remove playground'
    - '    subprocess.run(f"rm -rf {repo_playground}", shell=True)'
    - ''
    - '    return s'
    - ''
    - ''
    - 'def get_functions(tree):'
    - '    """Get a set of function and method names from the AST tree."""'
    - '    functions = {}'
    - ''
    - '    class FunctionVisitor(ast.NodeVisitor):'
    - '        def __init__(self):'
    - '            self.parents = []'
    - ''
    - '        def visit(self, node):'
    - '            self.parents.append(node)'
    - '            super().visit(node)'
    - '            self.parents.pop()'
    - ''
    - '        def visit_FunctionDef(self, node):'
    - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
    - '                functions[node.name] = ast.unparse(node)'
    - '            self.generic_visit(node)'
    - ''
    - '        def visit_AsyncFunctionDef(self, node):'
    - '            if not any(isinstance(parent, ast.ClassDef) for parent in self.parents):'
    - '                functions[node.name] = ast.unparse(node)'
    - '            self.generic_visit(node)'
    - ''
    - '    class ClassVisitor(ast.NodeVisitor):'
    - '        def visit_ClassDef(self, node):'
    - '            class_name = node.name'
    - '            for body_item in node.body:'
    - '                if isinstance(body_item, ast.FunctionDef) or isinstance('
    - '                    body_item, ast.AsyncFunctionDef'
    - '                ):'
    - '                    functions[f"{class_name}.{body_item.name}"] = ast.unparse(body_item)'
    - '            self.generic_visit(node)'
    - ''
    - '    FunctionVisitor().visit(tree)'
    - '    ClassVisitor().visit(tree)'
    - '    return functions'
    - ''
    - ''
    - 'def is_just_new_function(code1, code2):'
    - '    tree1 = ast.parse(code1)'
    - '    tree2 = ast.parse(code2)'
    - ''
    - '    functions1 = get_functions(tree1)'
    - '    functions2 = get_functions(tree2)'
    - ''
    - '    # The new functions in the second code'
    - '    if len(set(list(functions1.keys())) - set(list(functions2.keys()))) > 0:'
    - '        # removes functions'
    - '        return False'
    - ''
    - '    for func in functions1:'
    - '        if functions1[func] != functions2[func]:'
    - '            # modifies existing functions'
    - '            return False'
    - ''
    - '    if len(set(list(functions2.keys())) - set(list(functions1.keys()))) > 0:'
    - '        return True'
    - ''
    - '    # modifying global stuff is okay, because its actually same as functions
      almost.'
    - ''
    - '    return False'
    - ''
    - ''
    - import io
    - import re
    - import tokenize
    - ''
    - ''
    - 'def remove_comments_and_docstrings(source):'
    - '    io_obj = io.StringIO(source)'
    - '    out = ""'
    - '    prev_toktype = tokenize.INDENT'
    - '    last_lineno = -1'
    - '    last_col = 0'
    - '    for tok in tokenize.generate_tokens(io_obj.readline):'
    - '        token_type = tok[0]'
    - '        token_string = tok[1]'
    - '        start_line, start_col = tok[2]'
    - '        end_line, end_col = tok[3]'
    - '        ltext = tok[4]'
    - '        if start_line > last_lineno:'
    - '            last_col = 0'
    - '        if start_col > last_col:'
    - '            out += " " * (start_col - last_col)'
    - '        if token_type == tokenize.COMMENT:'
    - '            pass'
    - '        elif token_type == tokenize.STRING:'
    - '            if prev_toktype != tokenize.INDENT:'
    - '                if prev_toktype != tokenize.NEWLINE:'
    - '                    if start_col > 0:'
    - '                        out += token_string'
    - '        else:'
    - '            out += token_string'
    - '        prev_toktype = token_type'
    - '        last_col = end_col'
    - '        last_lineno = end_line'
    - '    out = "\n".join(l for l in out.splitlines() if l.strip())'
    - '    return out'
    - ''
    - ''
    - 'def normalize_patch(instance_id, patch: str, original_file_content: str) ->
      str:'
    - '    "Remove edits to trailing spaces and comments in the patch."'
    - '    if not patch.strip():'
    - '        return ""'
    - '    # Extract info.'
    - '    file_changes = parse_patch(patch)'
    - '    if not file_changes:'
    - '        print(patch)'
    - '        print("=")'
    - '        import json'
    - ''
    - '        print(json.dumps(file_changes, indent=2))'
    - '        exit(0)'
    - ''
    - '    edited_file = file_changes[0]["file"]'
    - '    old_content = original_file_content'
    - '    # Get new file'
    - '    new_content = fake_git_apply("playground", edited_file, old_content, patch)'
    - '    if new_content is None:'
    - '        # Error during applying diff'
    - '        # print("ERROR in applying patch")'
    - '        return patch'
    - ''
    - '    # Normalize file contents'
    - '    def normalize_code(code):'
    - '        try:'
    - '            node = ast.parse(code)'
    - '            return ast.unparse(node)'
    - '        except:'
    - '            return code'
    - ''
    - '    old_content = normalize_code(old_content)'
    - '    new_content = normalize_code(new_content)'
    - ''
    - '    try:'
    - '        remove_docstring_old_content = remove_comments_and_docstrings(old_content)'
    - '        ast.parse(remove_docstring_old_content)  # check'
    - '        remove_docstring_new_content = remove_comments_and_docstrings(new_content)'
    - '        ast.parse(remove_docstring_new_content)  # check'
    - '    except:'
    - '        # when does this exception happen?'
    - '        # when the code has some class or function with empty docstring (thats
      valid python code)'
    - '        # but removing it is not, to be save we just use the original.'
    - '        remove_docstring_old_content = old_content'
    - '        remove_docstring_new_content = new_content'
    - ''
    - '    diff = fake_git_repo('
    - '        "playground",'
    - '        edited_file,'
    - '        remove_docstring_old_content,'
    - '        remove_docstring_new_content,'
    - '    )'
    - ''
    - '    if is_just_new_function(remove_docstring_old_content, remove_docstring_new_content):'
    - '        # modify the diff to ignore context.'
    - '        new_diff = []'
    - '        for line in diff.splitlines():'
    - '            if line.startswith("-") or line.startswith("+"):'
    - '                new_diff.append(line)'
    - '        diff = "\n".join(new_diff)'
    - ''
    - '    # Note that the normalized diff may not be applied to the original file.'
    - '    return diff'
    - ''
    - ''
    - 'def extract_python_blocks(text):'
    - '    # Regular expression pattern to match ```python\n{text}\n```'
    - '    pattern = r"```python\n(.*?)\n```"'
    - ''
    - '    with open("text.txt", "w") as f:'
    - '        f.write(text)'
    - ''
    - '    # Use re.findall to find all matches'
    - '    matches = re.findall(pattern, text, re.DOTALL)'
    - ''
    - '    return matches'
    - ''
    - ''
    - 'def extract_code_blocks(text):'
    - '    pattern = r"```\n(.*?)\n```"'
    - '    matches = re.findall(pattern, text, re.DOTALL)'
    - '    if len(matches) == 0:'
    - '        if "```" in text:'
    - '            # handle the case where the code block is not complete'
    - '            return [text.split("```", 1)[-1].strip()]'
    - '    return matches'
    - ''
    - ''
    - 'def extract_locs_for_files(locs, file_names):'
    - '    # TODO: keep the order from this fine-grained FL results.'
    - ''
    - '    '
    - ''
    - '    results = {fn: [] for fn in file_names} # dictionary with file names as
      the keys'
    - '    with open("localize_related.txt", "a") as f:'
    - '        f.write("Results Dict (contains filenames)\n" + str(results) + "\n"*3)'
    - '        f.write("Locs:\n" + str(locs) + "\n"*3)'
    - ''
    - '    current_file_name = None'
    - '    for loc in locs:'
    - '        for line in loc.splitlines():'
    - ''
    - '            with open("localize_related.txt", "a") as f:'
    - '                f.write("Current Line:\n" + str(line) + "\n")'
    - '                f.write("processed: " + str(line.strip().split(" ")[1]) + "\n") '
    - ''
    - '            if line.strip().endswith(".py"):'
    - '                current_file_name = line.strip().split(" ")[1]'
    - '              '
    - '            elif line.strip() and any('
    - '                line.startswith(w)'
    - '                for w in ["line:", "function:", "class:", "variable:", "path:"]
      # add a path '
    - '            ):'
    - '                if current_file_name in results: # only take file name in results '
    - '                    with open("localize_related.txt", "a") as f:'
    - '                        f.write("Curr filename found in results\n")'
    - '                    results[current_file_name].append(line)'
    - '                else:'
    - '                    pass'
    - '    output = [["\n".join(results[fn])] for fn in file_names]'
    - ''
    - ''
    - '    with open("localize_related.txt", "a") as f:'
    - '        f.write("Model Found Locs Separated:\n")'
    - '        for i in output:'
    - '            f.write(str(i) + "\n"*3)'
    - ''
    - '    return output'
    - ''
    - ''
    - 'def extract_starting_number(subcommand):'
    - '    return int(subcommand.split(",")[0].split("start=")[-1])'
    - ''
    - ''
    - 'def extract_ending_number(subcommand):'
    - '    return int(subcommand.split(",")[1].split("end=")[-1])'
    - ''
    - ''
    - 'def overlap(subcommand1, subcommand2):'
    - '    start1, end1 = extract_starting_number(subcommand1), extract_ending_number('
    - '        subcommand1'
    - '    )'
    - '    start2, end2 = extract_starting_number(subcommand2), extract_ending_number('
    - '        subcommand2'
    - '    )'
    - '    return not (end1 < start2 or end2 < start1)'
    - ''
    - ''
    - 'def split_edit_multifile_commands(commands, diff_format=False) -> dict[str,
      str]:'
    - '    """Split commands based on edited files."""'
    - '    file_to_commands = OrderedDict()'
    - '    if diff_format:'
    - '        for command in commands:'
    - '            file_name = None'
    - '            for subcommand in command.split(">>>>>>> REPLACE")[:-1]:'
    - '                subcommand = subcommand.strip()'
    - '                if "<<<<<<< SEARCH" in subcommand:'
    - '                    fn = subcommand.split("<<<<<<< SEARCH")[0].lstrip("#").strip()'
    - '                    if fn:'
    - '                        file_name = "''" + fn + "''"'
    - ''
    - '                if len(subcommand.split("<<<<<<< SEARCH")) != 2:'
    - '                    continue'
    - '                converted_command = ('
    - '                    "<<<<<<< SEARCH"'
    - '                    + subcommand.split("<<<<<<< SEARCH")[1]'
    - '                    + "\n"'
    - '                    + ">>>>>>> REPLACE"'
    - '                )'
    - '                # deduplicate'
    - '                if ('
    - '                    file_name not in file_to_commands'
    - '                    or converted_command not in file_to_commands[file_name]'
    - '                ):'
    - '                    file_to_commands.setdefault(file_name, []).append(converted_command)'
    - '    else:'
    - '        for command in commands:'
    - '            for subcommand in command.split("edit_file(")[1:]:'
    - '                file_name, start, end, content = subcommand.split(",", 3)'
    - '                converted_command = "edit_file(" + ",".join([start, end, content])'
    - '                # deduplicate'
    - '                if ('
    - '                    file_name not in file_to_commands'
    - '                    or converted_command not in file_to_commands[file_name]'
    - '                ):'
    - '                    file_to_commands.setdefault(file_name, []).append(converted_command)'
    - '    return file_to_commands'
    - ''
    - ''
    - def parse_diff_edit_commands(
    - '    commands, content, file_loc_intervals: list[tuple[int, int]]'
    - '):'
    - '    def parse_for_threedots(original, replace, file_loc_intervals, content):'
    - '        # if dot dot dot in replace, its always safe to remove it'
    - '        if replace.startswith("...\n") and len(replace) > 4:'
    - '            # im parsing this first because its used later on'
    - '            replace = replace[4:]'
    - ''
    - '        # just dot dot dot, then need to do something special'
    - '        if original == "...":'
    - '            if not replace[0].isspace():'
    - '                # this is okay'
    - '                # find a suitable original string to replace'
    - '                for interval in file_loc_intervals:'
    - '                    start, end = interval'
    - '                    start = max(start - 1, 0)'
    - '                    context_segment = "\n".join(content.splitlines()[start:end])'
    - ''
    - '                    for line in context_segment.splitlines():'
    - '                        if len(line) > 0 and not line[0].isspace():'
    - '                            if content.count(line) == 1:'
    - '                                original = line'
    - '                                # keep the line'
    - '                                replace = replace + "\n\n" + line'
    - '                                break'
    - ''
    - '                    if original != "...":'
    - '                        break'
    - ''
    - '                if original == "...":'
    - '                    print("cannot find suitable location")'
    - ''
    - '        # dot dot dot with something else in original, then its safe to replace'
    - '        if original.startswith("...\n") and len(original) > 4:'
    - '            # remove dot dot dot from original'
    - '            original = original[4:]'
    - ''
    - '        return original, replace'
    - ''
    - '    if len(file_loc_intervals) == 0:'
    - '        if original in content:'
    - '            content = content.replace(original, replace)'
    - '            replaced = True'
    - '        else:'
    - '            print("not replaced")'
    - '        return content'
    - '    # let''s first make sure the intervals are sorted'
    - '    file_loc_intervals.sort()'
    - '    replaced = False'
    - '    # apply the edits from the end of file to the beginning of file'
    - '    # this is to make sure context is correct'
    - '    for interval in file_loc_intervals[::-1]:'
    - '        start, end = interval'
    - '        start = max(start - 1, 0)'
    - '        context_segment = "\n".join(content.splitlines()[start:end])'
    - '        context_segment = "\n" + context_segment + "\n"'
    - ''
    - '        # since we want to replace the original context, let''s first check
      for all edits.'
    - '        can_apply = []'
    - '        for subcommand in commands:'
    - '            if not subcommand.startswith("<<<<<<< SEARCH") and subcommand.endswith('
    - '                ">>>>>>> REPLACE"'
    - '            ):'
    - '                continue'
    - ''
    - '            subcommand = "\n".join(subcommand.splitlines()[1:-1])'
    - '            if len(subcommand.split("\n=======\n")) != 2:'
    - '                continue'
    - ''
    - '            original, replace = subcommand.split("\n=======\n")'
    - ''
    - '            original, replace = parse_for_threedots('
    - '                original, replace, file_loc_intervals, content'
    - '            )'
    - ''
    - '            original = "\n" + original + "\n"'
    - '            replace = "\n" + replace + "\n"'
    - ''
    - '            if original in context_segment:'
    - '                can_apply.append(subcommand)'
    - ''
    - '        # apply edits backwards'
    - '        for subcommand in can_apply[::-1]:'
    - '            original, replace = subcommand.split("\n=======\n")'
    - ''
    - '            original, replace = parse_for_threedots('
    - '                original, replace, file_loc_intervals, content'
    - '            )'
    - ''
    - '            original = "\n" + original + "\n"'
    - '            replace = "\n" + replace + "\n"'
    - '            if ('
    - '                original in context_segment'
    - '            ):  # This may not be true after some previously applied edits'
    - '                context_segment = context_segment.replace(original, replace)'
    - '                replaced = True'
    - '        # reassembly'
    - '        content = ('
    - '            "\n".join(content.splitlines()[:start])'
    - '            + context_segment'
    - '            + "\n".join(content.splitlines()[end:])'
    - '        )'
    - ''
    - '    if not replaced:'
    - '        print("not replaced")'
    - ''
    - '    return content'
    - ''
    - ''
    - 'def parse_edit_commands(commands, content):'
    - '    content_lines = content.splitlines()'
    - '    map_content_lines_to_line_num = [i for i in range(0, len(content_lines)
      + 1)]'
    - ''
    - '    # Make a list of the subcommands'
    - '    subcommands = []'
    - ''
    - '    for command in commands:'
    - '        for subcommand in command.split("edit_file(")[1:]:'
    - '            subcommands.append(subcommand)'
    - ''
    - '    # Remove duplicates while preserving order'
    - '    seen = set()'
    - '    unique_subcommands = []'
    - '    for subcommand in subcommands:'
    - '        if subcommand not in seen:'
    - '            unique_subcommands.append(subcommand)'
    - '            seen.add(subcommand)'
    - ''
    - '    # Sort the unique subcommands by the starting number in reverse order'
    - '    unique_sorted_subcommands = sorted('
    - '        unique_subcommands, key=extract_starting_number, reverse=True'
    - '    )'
    - ''
    - '    for subcommand in unique_sorted_subcommands:'
    - '        command_start = int(subcommand.split(",")[0].split("start=")[-1])'
    - '        command_end = int(subcommand.split(",")[1].split("end=")[-1])'
    - ''
    - '        start = map_content_lines_to_line_num.index(command_start)'
    - '        end = map_content_lines_to_line_num.index(command_end)'
    - ''
    - '        try:'
    - '            changed_content = eval('
    - '                ")".join(",".join(subcommand.split(",")[2:]).split(")")[:-1])'
    - '            )'
    - '            # small thing to ensure right white space indent'
    - '            if ('
    - '                start == end'
    - '                and len(changed_content.splitlines()) == 1'
    - '                and (len(changed_content) - len(changed_content.lstrip()))
      == 0'
    - '            ):'
    - '                indent_length = len(content_lines[start - 1]) - len('
    - '                    content_lines[start - 1].lstrip()'
    - '                )'
    - '                changed_content = " " * indent_length + changed_content.lstrip()'
    - ''
    - '        # catch syntax error'
    - '        except:'
    - '            # try to fix, specially for case where there are """ or ''''''
      in the string, that cannot be'
    - '            # easily evaluated.'
    - '            eval_str = ")".join('
    - '                ",".join(subcommand.split(",")[2:]).split(")")[:-1]'
    - '            ).strip()'
    - ''
    - '            if eval_str.startswith("content="):'
    - '                eval_str = eval_str[8:]'
    - ''
    - '            if eval_str.startswith(''"""'') or eval_str.startswith("''''''"):'
    - '                eval_str = eval_str[3:-3]'
    - '            if eval_str.startswith(''"'') or eval_str.startswith("''"):'
    - '                eval_str = eval_str[1:-1]'
    - ''
    - '            changed_content = eval_str'
    - ''
    - '        content_lines[start - 1 : end] = changed_content.splitlines()'
    - ''
    - '    content = "\n".join(content_lines)'
    - ''
    - '    return content'
    - ''
    - ''
    - 'def test_parse():'
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 1, "import os")
    - '```'
    - '"""'
    - ''
    - '    content = """'
    - import sys
    - '""".strip()'
    - ''
    - '    commands = extract_python_blocks(raw_output)'
    - ''
    - '    content = parse_edit_commands(commands, content)'
    - ''
    - '    assert content == "import os", content'
    - ''
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 1, '''import os\nimport sys''')
    - '```'
    - '"""'
    - ''
    - '    content = """'
    - import sys
    - '""".strip()'
    - ''
    - '    commands = extract_python_blocks(raw_output)'
    - ''
    - '    content = parse_edit_commands(commands, content)'
    - ''
    - '    assert content == "import os\nimport sys", content'
    - ''
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 1, '''import os''')
    - edit_file(1, 1, '''import sys''')
    - '```'
    - '"""'
    - ''
    - '    content = """'
    - import sys
    - '""".strip()'
    - ''
    - '    commands = extract_python_blocks(raw_output)'
    - ''
    - '    content = parse_edit_commands(commands, content)'
    - ''
    - '    assert content == "import sys", content'
    - ''
    - '    content = """'
    - test
    - testing
    - '""".strip()'
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 1, "testing\ntesting2")
    - edit_file(2, 2, "testing3")
    - '```'
    - '"""'
    - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
    - ''
    - '    assert content == "testing\ntesting2\ntesting3", content'
    - ''
    - '    content = """'
    - test
    - testing
    - testinging
    - '""".strip()'
    - ''
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 2, "testing")
    - edit_file(3, 3, "testing3")
    - '```'
    - '"""'
    - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
    - ''
    - '    assert content == "testing\ntesting3", content'
    - ''
    - '    content = """'
    - test
    - testing
    - testinging
    - test
    - testing
    - '""".strip()'
    - ''
    - '    raw_output = """'
    - '```python'
    - edit_file(1, 2, "testing")
    - edit_file(3, 3, "testing3")
    - edit_file(4, 4, "testing\ntesting2")
    - edit_file(5, 5, "testing3")
    - '```'
    - '"""'
    - ''
    - '    edited_content = """'
    - testing
    - testing3
    - testing
    - testing2
    - testing3
    - '""".strip()'
    - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
    - ''
    - '    assert content == edited_content, content'
    - ''
    - '    # Test for if command is present twice in the output (adapted from real
      output)'
    - '    content = """'
    - test
    - testing
    - testinging
    - test
    - testing
    - '""".strip()'
    - ''
    - '    raw_output = ('
    - '        """'
    - '"To fix the issue where `viewcode`'
    - 1. Add a condition
    - ''
    - '```python'
    - edit_file(4, 4, """
    - '        """'
    - test4
    - '"""'
    - '        """)'
    - '```'
    - ''
    - 2. Ensure that the `doctree_read`
    - ''
    - '```python'
    - edit_file(2, 3, """
    - '        """'
    - test-2-3
    - '"""'
    - '        """)'
    - '```'
    - ''
    - 'Here is the complete set of commands to address the issue:'
    - ''
    - '```python'
    - edit_file(4, 4, """
    - '        """'
    - test4
    - '"""'
    - '        """)'
    - '```'
    - ''
    - '```python'
    - edit_file(2, 3, """
    - '        """'
    - test-2-3
    - '"""'
    - '        """)'
    - '```'
    - '"""'
    - '    )'
    - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
    - ''
    - '    revised_content = """'
    - test
    - test-2-3
    - test4
    - testing
    - '""".strip()'
    - ''
    - '    assert content == revised_content, content'
    - ''
    - '    raw_output = """'
    - '```'
    - django/db/migrations/optimizer.py
    - 'function: MigrationOptimizer.optimize_inner'
    - ''
    - django/db/migrations/operations/fields.py
    - 'function: AlterField.reduce'
    - '```'
    - '"""'
    - '    files = ['
    - '        "django/db/migrations/optimizer.py",'
    - '        "django/db/migrations/operations/fields.py",'
    - '        "django/db/migrations/operations/models.py",'
    - '    ]'
    - '    extracted_locs = extract_locs_for_files([raw_output], files)'
    - '    print(extracted_locs)'
    - '    assert extracted_locs == ['
    - '        ["function: MigrationOptimizer.optimize_inner"],'
    - '        ["function: AlterField.reduce"],'
    - '        [""],'
    - '    ]'
    - ''
    - '    raw_output = """'
    - '```python'
    - edit_file(start=1, end=1, content="testing not")
    - '```'
    - '"""'
    - ''
    - '    content = """'
    - testing
    - '""".strip()'
    - ''
    - '    edited_content = """'
    - testing not
    - '""".strip()'
    - ''
    - '    content = parse_edit_commands(extract_python_blocks(raw_output), content)'
    - '    assert content == edited_content, edited_content'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    # test_parse()'
    - ''
    - '    # Example Usage'
    - '    code1 = """'
    - 'class MyClass:'
    - '    def existing_method(self):'
    - '        pass'
    - '"""'
    - ''
    - '    code2 = """'
    - 'class MyClass:'
    - '    def existing_method(self):'
    - '        pass'
    - ''
    - '    async def new_method(self):'
    - '        pass'
    - '"""'
    - ''
    - '    print(is_just_new_function(code1, code2))  # Output: True'
  preprocess_data.py:
    classes: []
    functions:
    - end_line: 86
      name: line_wrap_content
      start_line: 11
      text:
      - def line_wrap_content(
      - '    content: str,'
      - '    context_intervals=None,'
      - '    add_space=False,'
      - '    no_line_number=False,'
      - '    sticky_scroll=False,'
      - '):'
      - '    """add n| to each line, where n increases"""'
      - ''
      - '    def is_scope(line):'
      - '        # TODO: this might not be precise, can improve with syntax parsing'
      - '        return line.startswith("class ") or line.strip().startswith("def
        ")'
      - ''
      - '    lines = content.split("\n")'
      - '    new_lines = []'
      - '    if context_intervals is None or context_intervals == []:'
      - '        context_intervals = [(0, len(lines))]'
      - ''
      - '    prev_scopes = []'
      - '    line_format = "{line}"'
      - '    if not no_line_number:'
      - '        line_format = ('
      - '            "{line_number}|{line}" if not add_space else "{line_number}|
        {line} "'
      - '        )'
      - '    for interval in context_intervals:'
      - '        min_line, max_line = interval'
      - ''
      - '        if min_line != 0:'
      - '            new_lines.append("...")'
      - ''
      - '        scopes = []'
      - '        for i, line in enumerate(lines):'
      - '            if sticky_scroll:'
      - '                # add current line to scope if necessary'
      - '                if is_scope(line):'
      - '                    indent_level = len(line) - len(line.lstrip())'
      - '                    while scopes and scopes[-1]["indent_level"] >= indent_level:'
      - '                        scopes.pop()'
      - '                    scopes.append('
      - '                        {"line": line, "line_number": i, "indent_level":
        indent_level}'
      - '                    )'
      - ''
      - '            if min_line != -1 and i < min_line - 1:'
      - '                continue'
      - '            if sticky_scroll and i == min_line - 1:'
      - '                # add scope lines'
      - '                last_scope_line = None'
      - '                for j, scope_line in enumerate(scopes):'
      - '                    # don''t repeat previous scopes'
      - '                    if ('
      - '                        len(prev_scopes) > j'
      - '                        and prev_scopes[j]["line_number"] == scope_line["line_number"]'
      - '                    ):'
      - '                        continue'
      - '                    # don''t repeat current line'
      - '                    if i == scope_line["line_number"]:'
      - '                        continue'
      - '                    new_lines.append('
      - '                        line_format.format('
      - '                            line_number=scope_line["line_number"] + 1,'
      - '                            line=scope_line["line"],'
      - '                        )'
      - '                    )'
      - '                    last_scope_line = scope_line["line_number"]'
      - '                if last_scope_line is not None and last_scope_line < i -
        1:'
      - '                    new_lines.append("...")'
      - ''
      - '            new_lines.append(line_format.format(line_number=i + 1, line=line))'
      - '            if max_line != -1 and i >= max_line - 1:'
      - '                break'
      - '        prev_scopes = scopes'
      - ''
      - '    if max_line != len(lines):'
      - '        new_lines.append("...")'
      - ''
      - '    return "\n".join(new_lines)'
    - end_line: 110
      name: merge_intervals
      start_line: 89
      text:
      - 'def merge_intervals(intervals):'
      - '    # intervals inclusive'
      - '    if not intervals:'
      - '        return []'
      - ''
      - '    # Sort the intervals based on the starting value of each tuple'
      - '    intervals.sort(key=lambda interval: interval[0])'
      - ''
      - '    merged_intervals = [intervals[0]]'
      - ''
      - '    for current in intervals[1:]:'
      - '        last = merged_intervals[-1]'
      - ''
      - '        # Check if there is overlap'
      - '        if current[0] <= last[1]:'
      - '            # If there is overlap, merge the intervals'
      - '            merged_intervals[-1] = (last[0], max(last[1], current[1]))'
      - '        else:'
      - '            # If there is no overlap, just add the current interval to the
        result list'
      - '            merged_intervals.append(current)'
      - ''
      - '    return merged_intervals'
    - end_line: 321
      name: transfer_arb_locs_to_locs
      start_line: 113
      text:
      - def transfer_arb_locs_to_locs(
      - '    locs,'
      - '    structure,'
      - '    pred_file,'
      - '    context_window=10,'
      - '    loc_interval=False,'
      - '    fine_grain_only=False,'
      - '    remove_line=False,'
      - '    file_content="",'
      - ') -> tuple[list, list]:'
      - '    if structure is None:'
      - '        class_info, function_names, file_lines = parse_python_file("", file_content)'
      - '        structure = {}'
      - '        structure[pred_file] = {'
      - '            "classes": class_info,'
      - '            "functions": function_names,'
      - '            "text": file_lines,'
      - '        }'
      - ''
      - '    files, classes, functions = get_full_file_paths_and_classes_and_functions(structure)'
      - ''
      - '    line_loc = []'
      - '    if isinstance(locs, str):'
      - '        # if its a single loc'
      - '        locs = [locs]'
      - '    # TODO: parse it in advance'
      - '    global_vars = parse_global_var_from_code(file_content)'
      - ''
      - '    for model_pred_locs in locs:'
      - '        current_class_name = ""'
      - '        for loc in model_pred_locs.splitlines():'
      - '            # handle cases like "class: MyClass.my_method"'
      - '            if loc.startswith("class: ") and "." not in loc:'
      - '                loc = loc[len("class: ") :].strip()'
      - '                relevant_class = ['
      - '                    clazz'
      - '                    for clazz in classes'
      - '                    if clazz["file"] == pred_file and clazz["name"] == loc'
      - '                ]'
      - ''
      - '                if len(relevant_class) == 0:'
      - '                    print(f"{loc} class could not be found")'
      - '                else:'
      - '                    line_loc.append('
      - '                        (relevant_class[0]["start_line"], relevant_class[0]["end_line"])'
      - '                    )'
      - '                    current_class_name = loc'
      - ''
      - '            elif loc.startswith("function: ") or "." in loc:'
      - '                full_loc = loc'
      - '                loc = loc.split(":", 1)[-1].strip()'
      - ''
      - '                if "." in loc:'
      - '                    # assume its a method within a class'
      - '                    method_name = loc.split(".")[1]'
      - '                    class_name = loc.split(".")[0]'
      - ''
      - '                    relevant_class = ['
      - '                        clazz'
      - '                        for clazz in classes'
      - '                        if clazz["file"] == pred_file and clazz["name"] ==
        class_name'
      - '                    ]'
      - '                    if len(relevant_class) == 0:'
      - '                        print(f"{class_name} class could not be found")'
      - '                    else:'
      - '                        relevant_method = ['
      - '                            method'
      - '                            for method in relevant_class[0]["methods"]'
      - '                            if method["name"] == method_name'
      - '                        ]'
      - '                        if len(relevant_method) == 0:'
      - '                            print(f"{full_loc} method could not be found")'
      - '                        else:'
      - '                            line_loc.append('
      - '                                ('
      - '                                    relevant_method[0]["start_line"],'
      - '                                    relevant_method[0]["end_line"],'
      - '                                )'
      - '                            )'
      - ''
      - '                else:'
      - '                    relevant_function = ['
      - '                        function'
      - '                        for function in functions'
      - '                        if function["file"] == pred_file and function["name"]
        == loc'
      - '                    ]'
      - '                    if len(relevant_function) == 0:'
      - '                        print(f"{loc} function could not be found")'
      - '                        if current_class_name != "":'
      - '                            # check if its a method'
      - '                            relevant_class = ['
      - '                                clazz'
      - '                                for clazz in classes'
      - '                                if clazz["file"] == pred_file'
      - '                                and clazz["name"] == current_class_name'
      - '                            ]'
      - '                            relevant_method = ['
      - '                                method'
      - '                                for method in relevant_class[0]["methods"]'
      - '                                if method["name"] == loc'
      - '                            ]'
      - '                            if len(relevant_method) == 0:'
      - '                                print(f"{loc} method could not be found")'
      - '                                # print([method for method in relevant_class[0][''methods'']])'
      - '                                #'
      - '                                # for file_content in files:'
      - '                                #     if file_content[0] == pred_file:'
      - '                                #         print("\n".join(file_content[1]))'
      - '                                #         exit()'
      - '                            else:'
      - '                                line_loc.append('
      - '                                    ('
      - '                                        relevant_method[0]["start_line"],'
      - '                                        relevant_method[0]["end_line"],'
      - '                                    )'
      - '                                )'
      - '                        else:'
      - '                            # look for it in any class'
      - '                            relevant_method = []'
      - '                            for clazz in classes:'
      - '                                if clazz["file"] == pred_file:'
      - '                                    relevant_method.extend('
      - '                                        ['
      - '                                            method'
      - '                                            for method in clazz["methods"]'
      - '                                            if method["name"] == loc'
      - '                                        ]'
      - '                                    )'
      - ''
      - '                            if len(relevant_method) == 1:'
      - '                                line_loc.append('
      - '                                    ('
      - '                                        relevant_method[0]["start_line"],'
      - '                                        relevant_method[0]["end_line"],'
      - '                                    )'
      - '                                )'
      - '                    else:'
      - '                        line_loc.append('
      - '                            ('
      - '                                relevant_function[0]["start_line"],'
      - '                                relevant_function[0]["end_line"],'
      - '                            )'
      - '                        )'
      - '            elif loc.startswith("line: "):'
      - '                if remove_line:'
      - '                    # TODO: can recover the corresponding function instead
        of throwing it away'
      - '                    continue'
      - '                loc = loc[len("line: ") :].strip().split()[0]'
      - '                try:'
      - '                    # line_loc.append(int(loc))'
      - '                    line_loc.append((int(loc), int(loc)))'
      - '                except:'
      - '                    continue'
      - '            elif loc.startswith("variable:"):'
      - '                vars = loc[len("variable:") :].strip().split()'
      - '                for v in vars:'
      - '                    if v in global_vars:'
      - '                        line_loc.append('
      - '                            (global_vars[v]["start_line"], global_vars[v]["end_line"])'
      - '                        )'
      - '            else:'
      - '                if loc.strip():'
      - '                    print(f"loc {loc} not recognised")'
      - '                # assert False'
      - ''
      - '    # Fine-grained-only loc: Remove intervals that are supersets of another.'
      - '    if fine_grain_only:'
      - '        filtered_line_loc = []'
      - '        for st, en in line_loc:'
      - '            if filtered_line_loc:'
      - '                last_st, last_en = filtered_line_loc[-1]'
      - '                # If the current interval is a more fine-grained loc, remove
        the superset.'
      - '                if last_st <= st and en <= last_en:'
      - '                    filtered_line_loc.pop()'
      - '            filtered_line_loc.append((st, en))'
      - '        line_loc = filtered_line_loc'
      - ''
      - '    # compute max min'
      - '    # TODO: think of strategies to do bunched up lines'
      - '    # TODO: e.g., we can have multiple code segments (right now, its just
        one)'
      - ''
      - '    for file_content in files:'
      - '        if file_content[0] == pred_file:'
      - '            content = file_content[1]'
      - '            break'
      - ''
      - '    if len(line_loc) == 0:'
      - '        return [], []'
      - ''
      - '    # max_line = min(max(line_loc) + context_window, len(content))'
      - '    # min_line = max(min(line_loc) - context_window, 0)'
      - '    #'
      - '    # return line_loc, max_line, min_line'
      - ''
      - '    # compute overlapping locations instead'
      - '    if loc_interval:'
      - '        contextual_line_loc = []'
      - '        for loc in line_loc:'
      - '            max_line = min(loc[1] + context_window, len(content))'
      - '            min_line = max(loc[0] - context_window, 0)'
      - '            contextual_line_loc.append((min_line, max_line))'
      - ''
      - '        return line_loc, merge_intervals(contextual_line_loc)'
      - '    else:'
      - '        # defaulting to max min'
      - '        max_line = min(max([loc[1] for loc in line_loc]) + context_window,
        len(content))'
      - '        min_line = max(min([loc[0] for loc in line_loc]) - context_window,
        0)'
      - ''
      - '        return line_loc, [(min_line, max_line)]'
    - end_line: 357
      name: compile_gt_locations
      start_line: 324
      text:
      - 'def compile_gt_locations(gt_location: dict) -> tuple[list, set, set, set]:'
      - '    """mostly serves a way to check what are the gt locations in gt patch"""'
      - '    edits = gt_location["edits"]'
      - ''
      - '    lines, classes, methods, functions = [], set(), set(), set()'
      - ''
      - '    adds = set()'
      - ''
      - '    for edit in edits:'
      - '        for clazz in edit["class_names"]:'
      - '            classes.add(clazz)'
      - ''
      - '        for method in edit["method_names"]:'
      - '            methods.add(method)'
      - ''
      - '        for function in edit["function_names"]:'
      - '            functions.add(function)'
      - ''
      - '        if edit["type"] == "add":'
      - '            adds.add(edit["line"])'
      - '        else:'
      - '            lines.append(edit["line"])'
      - ''
      - '    # handle the added lines'
      - '    add_intervals = [(i, i + 1) for i in adds]'
      - '    add_intervals = merge_intervals(add_intervals)'
      - '    for st, en in add_intervals:'
      - '        lines.append(st)'
      - '    lines = list(set(lines))'
      - ''
      - '    # sort the lines'
      - '    lines = sorted(lines)'
      - ''
      - '    return lines, classes, methods, functions'
    - end_line: 375
      name: show_project_structure
      start_line: 360
      text:
      - 'def show_project_structure(structure, spacing=0) -> str:'
      - '    """pprint the project structure"""'
      - ''
      - '    pp_string = ""'
      - ''
      - '    for key, value in structure.items():'
      - '        if "." in key and ".py" not in key:'
      - '            continue  # skip none python files'
      - '        if "." in key:'
      - '            pp_string += " " * spacing + str(key) + "\n"'
      - '        else:'
      - '            pp_string += " " * spacing + str(key) + "/" + "\n"'
      - '        if "classes" not in value:'
      - '            pp_string += show_project_structure(value, spacing + 4)'
      - ''
      - '    return pp_string'
    - end_line: 384
      name: filter_out_test_files
      start_line: 378
      text:
      - 'def filter_out_test_files(structure):'
      - '    """filter out test files from the project structure"""'
      - '    for key, value in list(structure.items()):'
      - '        if key.startswith("test"):'
      - '            del structure[key]'
      - '        elif isinstance(value, dict):'
      - '            filter_out_test_files(value)'
    - end_line: 400
      name: filter_none_python
      start_line: 387
      text:
      - 'def filter_none_python(structure):'
      - '    for key, value in list(structure.items()):'
      - '        if ('
      - '            not "functions" in value.keys()'
      - '            and not "classes" in value.keys()'
      - '            and not "text" in value.keys()'
      - '        ) or not len(value.keys()) == 3:'
      - '            filter_none_python(value)'
      - ''
      - '            if structure[key] == {}:'
      - '                del structure[key]'
      - '        else:'
      - '            if not key.endswith(".py"):'
      - '                del structure[key]'
    - end_line: 436
      name: filter_proposed_files
      start_line: 403
      text:
      - 'def filter_proposed_files(proposed_files, repo_structure):'
      - '    """'
      - '    Filter proposed files against a given repository structure.'
      - ''
      - '    Arguments:'
      - '    proposed_files -- list of proposed files with instance IDs'
      - '    repo_structure -- list of repository structures with instance IDs'
      - ''
      - '    Returns:'
      - '    A list of dictionaries with instance IDs and valid files matching the
        repository structure.'
      - '    """'
      - '    instance_to_files = {'
      - '        entry["instance_id"]: entry["files"] for entry in proposed_files'
      - '    }'
      - '    instance_to_structure = {'
      - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
      - '    }'
      - '    filtered_files = []'
      - '    for instance_id, files in instance_to_files.items():'
      - '        if instance_id in instance_to_structure:'
      - '            repo_files, _, _ = get_full_file_paths_and_classes_and_functions('
      - '                instance_to_structure[instance_id]'
      - '            )'
      - '            repo_files_set = set(repo_files)'
      - '            valid_files = []'
      - '            for repo_file in repo_files_set:'
      - '                for proposed_file in files:'
      - '                    if proposed_file == repo_file.split("/")[-1]:'
      - '                        valid_files.append(repo_file)'
      - '            if valid_files:'
      - '                filtered_files.append('
      - '                    {"instance_id": instance_id, "files": valid_files}'
      - '                )'
      - '    return filtered_files'
    - end_line: 476
      name: filter_proposed_classes
      start_line: 439
      text:
      - 'def filter_proposed_classes(proposed_classes, repo_structure):'
      - '    """'
      - '    Filter proposed classes against a given repository structure.'
      - ''
      - '    Arguments:'
      - '    proposed_classes -- list of proposed classes with instance IDs'
      - '    repo_structure -- list of repository structures with instance IDs'
      - ''
      - '    Returns:'
      - '    A list of dictionaries with instance IDs and valid classes matching the
        repository structure.'
      - '    """'
      - '    instance_to_classes = {'
      - '        entry["instance_id"]: entry["classes"] for entry in proposed_classes'
      - '    }'
      - '    instance_to_structure = {'
      - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
      - '    }'
      - '    filtered_classes = []'
      - '    for instance_id, classes in instance_to_classes.items():'
      - '        if instance_id in instance_to_structure:'
      - '            _, repo_classes, _ = get_full_file_paths_and_classes_and_functions('
      - '                instance_to_structure[instance_id]'
      - '            )'
      - '            repo_classes_set = {clazz["name"]: clazz["file"] for clazz in
        repo_classes}'
      - '            valid_classes = []'
      - '            for proposed_class in classes:'
      - '                if proposed_class in repo_classes_set:'
      - '                    valid_classes.append('
      - '                        {'
      - '                            "name": proposed_class,'
      - '                            "file": repo_classes_set[proposed_class],'
      - '                        }'
      - '                    )'
      - '            if valid_classes:'
      - '                filtered_classes.append('
      - '                    {"instance_id": instance_id, "classes": valid_classes}'
      - '                )'
      - '    return filtered_classes'
    - end_line: 517
      name: filter_proposed_methods
      start_line: 479
      text:
      - 'def filter_proposed_methods(proposed_methods, repo_structure):'
      - '    """'
      - '    Filter proposed methods against a given repository structure.'
      - ''
      - '    Arguments:'
      - '    proposed_methods -- list of proposed methods with instance IDs'
      - '    repo_structure -- list of repository structures with instance IDs'
      - ''
      - '    Returns:'
      - '    A list of dictionaries with instance IDs and valid methods matching the
        repository structure.'
      - '    """'
      - '    instance_to_methods = {'
      - '        entry["instance_id"]: entry["methods"] for entry in proposed_methods'
      - '    }'
      - '    instance_to_structure = {'
      - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
      - '    }'
      - '    filtered_methods = []'
      - '    for instance_id, methods in instance_to_methods.items():'
      - '        if instance_id in instance_to_structure:'
      - '            _, repo_classes, _ = get_full_file_paths_and_classes_and_functions('
      - '                instance_to_structure[instance_id]'
      - '            )'
      - '            valid_methods = []'
      - '            for repo_class in repo_classes:'
      - '                for method in methods:'
      - '                    if method in repo_class["methods"]:'
      - '                        valid_methods.append('
      - '                            {'
      - '                                "class": repo_class["name"],'
      - '                                "method": method,'
      - '                                "file": repo_class["file"],'
      - '                            }'
      - '                        )'
      - '            if valid_methods:'
      - '                filtered_methods.append('
      - '                    {"instance_id": instance_id, "methods": valid_methods}'
      - '                )'
      - '    return filtered_methods'
    - end_line: 557
      name: filter_proposed_functions
      start_line: 520
      text:
      - 'def filter_proposed_functions(proposed_functions, repo_structure):'
      - '    """'
      - '    Filter proposed functions against a given repository structure.'
      - ''
      - '    Arguments:'
      - '    proposed_functions -- list of proposed functions with instance IDs'
      - '    repo_structure -- list of repository structures with instance IDs'
      - ''
      - '    Returns:'
      - '    A list of dictionaries with instance IDs and valid functions matching
        the repository structure.'
      - '    """'
      - '    instance_to_functions = {'
      - '        entry["instance_id"]: entry["functions"] for entry in proposed_functions'
      - '    }'
      - '    instance_to_structure = {'
      - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
      - '    }'
      - '    filtered_functions = []'
      - '    for instance_id, functions in instance_to_functions.items():'
      - '        if instance_id in instance_to_structure:'
      - '            _, _, repo_functions = get_full_file_paths_and_classes_and_functions('
      - '                instance_to_structure[instance_id]'
      - '            )'
      - '            valid_functions = []'
      - '            for repo_function in repo_functions:'
      - '                for function in functions:'
      - '                    if isinstance('
      - '                        repo_function["name"], dict'
      - '                    ):  # Why are there cases where this is not a dict?'
      - '                        if function == repo_function["name"].get("name",
        []):'
      - '                            valid_functions.append('
      - '                                {"function": function, "file": repo_function["file"]}'
      - '                            )'
      - '            if valid_functions:'
      - '                filtered_functions.append('
      - '                    {"instance_id": instance_id, "functions": valid_functions}'
      - '                )'
      - '    return filtered_functions'
    - end_line: 622
      name: get_full_file_paths_and_classes_and_functions
      start_line: 560
      text:
      - 'def get_full_file_paths_and_classes_and_functions(structure, current_path=""):'
      - '    """'
      - '    Recursively retrieve all file paths, classes, and functions within a
        directory structure.'
      - ''
      - '    Arguments:'
      - '    structure -- a dictionary representing the directory structure'
      - '    current_path -- the path accumulated so far, used during recursion (default="")'
      - ''
      - '    Returns:'
      - '    A tuple containing:'
      - '    - files: list of full file paths'
      - '    - classes: list of class details with file paths'
      - '    - functions: list of function details with file paths'
      - '    """'
      - '    files = []'
      - '    classes = []'
      - '    functions = []'
      - '    for name, content in structure.items():'
      - '        if isinstance(content, dict):'
      - '            if ('
      - '                not "functions" in content.keys()'
      - '                and not "classes" in content.keys()'
      - '                and not "text" in content.keys()'
      - '            ) or not len(content.keys()) == 3:'
      - '                # or guards against case where functions and classes are
        somehow part of the structure.'
      - '                next_path = f"{current_path}/{name}" if current_path else
        name'
      - '                ('
      - '                    sub_files,'
      - '                    sub_classes,'
      - '                    sub_functions,'
      - '                ) = get_full_file_paths_and_classes_and_functions(content,
        next_path)'
      - '                files.extend(sub_files)'
      - '                classes.extend(sub_classes)'
      - '                functions.extend(sub_functions)'
      - '            else:'
      - '                next_path = f"{current_path}/{name}" if current_path else
        name'
      - '                files.append((next_path, content["text"]))'
      - '                if "classes" in content:'
      - '                    for clazz in content["classes"]:'
      - '                        classes.append('
      - '                            {'
      - '                                "file": next_path,'
      - '                                "name": clazz["name"],'
      - '                                "start_line": clazz["start_line"],'
      - '                                "end_line": clazz["end_line"],'
      - '                                "methods": ['
      - '                                    {'
      - '                                        "name": method["name"],'
      - '                                        "start_line": method["start_line"],'
      - '                                        "end_line": method["end_line"],'
      - '                                    }'
      - '                                    for method in clazz.get("methods", [])'
      - '                                ],'
      - '                            }'
      - '                        )'
      - '                if "functions" in content:'
      - '                    for function in content["functions"]:'
      - '                        function["file"] = next_path'
      - '                        functions.append(function)'
      - '        else:'
      - '            next_path = f"{current_path}/{name}" if current_path else name'
      - '            files.append(next_path)'
      - '    return files, classes, functions'
    - end_line: 640
      name: get_repo_structure
      start_line: 628
      text:
      - 'def get_repo_structure(instance_id: str, repo_name, base_commit, playground):'
      - ''
      - '    if PROJECT_FILE_LOC is not None:'
      - '        with open(PROJECT_FILE_LOC + "/" + instance_id + ".json") as f:'
      - '            d = json.load(f)'
      - '        repo_structure = d["structure"]'
      - '    else:'
      - '        d = get_project_structure_from_scratch('
      - '            repo_name, base_commit, instance_id, playground'
      - '        )'
      - '        repo_structure = d["structure"]'
      - ''
      - '    return repo_structure'
    - end_line: 656
      name: get_repo_files
      start_line: 643
      text:
      - 'def get_repo_files(structure, filepaths: list[str]):'
      - '    files, classes, functions = get_full_file_paths_and_classes_and_functions(structure)'
      - '    file_contents = dict()'
      - '    for filepath in filepaths:'
      - '        content = None'
      - ''
      - '        for file_content in files:'
      - '            if file_content[0] == filepath:'
      - '                content = "\n".join(file_content[1])'
      - '                file_contents[filepath] = content'
      - '                break'
      - ''
      - '        assert content is not None, "file not found"'
      - '    return file_contents'
    - end_line: 675
      name: test_merge
      start_line: 659
      text:
      - 'def test_merge():'
      - '    # Example usage:'
      - '    input_tuples = [(1, 3), (2, 4), (5, 7), (6, 8)]'
      - '    merged_tuples = merge_intervals(input_tuples)'
      - '    assert merged_tuples == [(1, 4), (5, 8)]'
      - ''
      - '    input_tuples = [(1, 5), (2, 3)]'
      - '    merged_tuples = merge_intervals(input_tuples)'
      - '    assert merged_tuples == [(1, 5)]'
      - ''
      - '    input_tuples = [(1, 1)]'
      - '    merged_tuples = merge_intervals(input_tuples)'
      - '    assert merged_tuples == [(1, 1)]'
      - ''
      - '    input_tuples = [(1, 1), (2, 3)]'
      - '    merged_tuples = merge_intervals(input_tuples)'
      - '    assert merged_tuples == [(1, 1), (2, 3)]'
    - end_line: 697
      name: test_interval_display
      start_line: 678
      text:
      - 'def test_interval_display():'
      - ''
      - '    content = """'
      - one
      - two
      - three
      - four
      - five
      - six
      - seven
      - eight
      - '""".strip()'
      - ''
      - '    x = line_wrap_content(content, [])'
      - '    print(x)'
      - ''
      - '    print("============")'
      - ''
      - '    x = line_wrap_content(content, [(1, 2), (4, 6), (7, 8)])'
      - '    print(x)'
    - end_line: 22
      name: is_scope
      start_line: 20
      text:
      - '    def is_scope(line):'
      - '        # TODO: this might not be precise, can improve with syntax parsing'
      - '        return line.startswith("class ") or line.strip().startswith("def
        ")'
    text:
    - import json
    - import os
    - ''
    - from agentless.util.parse_global_var import parse_global_var_from_code
    - from get_repo_structure.get_repo_structure import (
    - '    get_project_structure_from_scratch,'
    - '    parse_python_file,'
    - )
    - ''
    - ''
    - def line_wrap_content(
    - '    content: str,'
    - '    context_intervals=None,'
    - '    add_space=False,'
    - '    no_line_number=False,'
    - '    sticky_scroll=False,'
    - '):'
    - '    """add n| to each line, where n increases"""'
    - ''
    - '    def is_scope(line):'
    - '        # TODO: this might not be precise, can improve with syntax parsing'
    - '        return line.startswith("class ") or line.strip().startswith("def ")'
    - ''
    - '    lines = content.split("\n")'
    - '    new_lines = []'
    - '    if context_intervals is None or context_intervals == []:'
    - '        context_intervals = [(0, len(lines))]'
    - ''
    - '    prev_scopes = []'
    - '    line_format = "{line}"'
    - '    if not no_line_number:'
    - '        line_format = ('
    - '            "{line_number}|{line}" if not add_space else "{line_number}| {line}
      "'
    - '        )'
    - '    for interval in context_intervals:'
    - '        min_line, max_line = interval'
    - ''
    - '        if min_line != 0:'
    - '            new_lines.append("...")'
    - ''
    - '        scopes = []'
    - '        for i, line in enumerate(lines):'
    - '            if sticky_scroll:'
    - '                # add current line to scope if necessary'
    - '                if is_scope(line):'
    - '                    indent_level = len(line) - len(line.lstrip())'
    - '                    while scopes and scopes[-1]["indent_level"] >= indent_level:'
    - '                        scopes.pop()'
    - '                    scopes.append('
    - '                        {"line": line, "line_number": i, "indent_level": indent_level}'
    - '                    )'
    - ''
    - '            if min_line != -1 and i < min_line - 1:'
    - '                continue'
    - '            if sticky_scroll and i == min_line - 1:'
    - '                # add scope lines'
    - '                last_scope_line = None'
    - '                for j, scope_line in enumerate(scopes):'
    - '                    # don''t repeat previous scopes'
    - '                    if ('
    - '                        len(prev_scopes) > j'
    - '                        and prev_scopes[j]["line_number"] == scope_line["line_number"]'
    - '                    ):'
    - '                        continue'
    - '                    # don''t repeat current line'
    - '                    if i == scope_line["line_number"]:'
    - '                        continue'
    - '                    new_lines.append('
    - '                        line_format.format('
    - '                            line_number=scope_line["line_number"] + 1,'
    - '                            line=scope_line["line"],'
    - '                        )'
    - '                    )'
    - '                    last_scope_line = scope_line["line_number"]'
    - '                if last_scope_line is not None and last_scope_line < i - 1:'
    - '                    new_lines.append("...")'
    - ''
    - '            new_lines.append(line_format.format(line_number=i + 1, line=line))'
    - '            if max_line != -1 and i >= max_line - 1:'
    - '                break'
    - '        prev_scopes = scopes'
    - ''
    - '    if max_line != len(lines):'
    - '        new_lines.append("...")'
    - ''
    - '    return "\n".join(new_lines)'
    - ''
    - ''
    - 'def merge_intervals(intervals):'
    - '    # intervals inclusive'
    - '    if not intervals:'
    - '        return []'
    - ''
    - '    # Sort the intervals based on the starting value of each tuple'
    - '    intervals.sort(key=lambda interval: interval[0])'
    - ''
    - '    merged_intervals = [intervals[0]]'
    - ''
    - '    for current in intervals[1:]:'
    - '        last = merged_intervals[-1]'
    - ''
    - '        # Check if there is overlap'
    - '        if current[0] <= last[1]:'
    - '            # If there is overlap, merge the intervals'
    - '            merged_intervals[-1] = (last[0], max(last[1], current[1]))'
    - '        else:'
    - '            # If there is no overlap, just add the current interval to the
      result list'
    - '            merged_intervals.append(current)'
    - ''
    - '    return merged_intervals'
    - ''
    - ''
    - def transfer_arb_locs_to_locs(
    - '    locs,'
    - '    structure,'
    - '    pred_file,'
    - '    context_window=10,'
    - '    loc_interval=False,'
    - '    fine_grain_only=False,'
    - '    remove_line=False,'
    - '    file_content="",'
    - ') -> tuple[list, list]:'
    - '    if structure is None:'
    - '        class_info, function_names, file_lines = parse_python_file("", file_content)'
    - '        structure = {}'
    - '        structure[pred_file] = {'
    - '            "classes": class_info,'
    - '            "functions": function_names,'
    - '            "text": file_lines,'
    - '        }'
    - ''
    - '    files, classes, functions = get_full_file_paths_and_classes_and_functions(structure)'
    - ''
    - '    line_loc = []'
    - '    if isinstance(locs, str):'
    - '        # if its a single loc'
    - '        locs = [locs]'
    - '    # TODO: parse it in advance'
    - '    global_vars = parse_global_var_from_code(file_content)'
    - ''
    - '    for model_pred_locs in locs:'
    - '        current_class_name = ""'
    - '        for loc in model_pred_locs.splitlines():'
    - '            # handle cases like "class: MyClass.my_method"'
    - '            if loc.startswith("class: ") and "." not in loc:'
    - '                loc = loc[len("class: ") :].strip()'
    - '                relevant_class = ['
    - '                    clazz'
    - '                    for clazz in classes'
    - '                    if clazz["file"] == pred_file and clazz["name"] == loc'
    - '                ]'
    - ''
    - '                if len(relevant_class) == 0:'
    - '                    print(f"{loc} class could not be found")'
    - '                else:'
    - '                    line_loc.append('
    - '                        (relevant_class[0]["start_line"], relevant_class[0]["end_line"])'
    - '                    )'
    - '                    current_class_name = loc'
    - ''
    - '            elif loc.startswith("function: ") or "." in loc:'
    - '                full_loc = loc'
    - '                loc = loc.split(":", 1)[-1].strip()'
    - ''
    - '                if "." in loc:'
    - '                    # assume its a method within a class'
    - '                    method_name = loc.split(".")[1]'
    - '                    class_name = loc.split(".")[0]'
    - ''
    - '                    relevant_class = ['
    - '                        clazz'
    - '                        for clazz in classes'
    - '                        if clazz["file"] == pred_file and clazz["name"] ==
      class_name'
    - '                    ]'
    - '                    if len(relevant_class) == 0:'
    - '                        print(f"{class_name} class could not be found")'
    - '                    else:'
    - '                        relevant_method = ['
    - '                            method'
    - '                            for method in relevant_class[0]["methods"]'
    - '                            if method["name"] == method_name'
    - '                        ]'
    - '                        if len(relevant_method) == 0:'
    - '                            print(f"{full_loc} method could not be found")'
    - '                        else:'
    - '                            line_loc.append('
    - '                                ('
    - '                                    relevant_method[0]["start_line"],'
    - '                                    relevant_method[0]["end_line"],'
    - '                                )'
    - '                            )'
    - ''
    - '                else:'
    - '                    relevant_function = ['
    - '                        function'
    - '                        for function in functions'
    - '                        if function["file"] == pred_file and function["name"]
      == loc'
    - '                    ]'
    - '                    if len(relevant_function) == 0:'
    - '                        print(f"{loc} function could not be found")'
    - '                        if current_class_name != "":'
    - '                            # check if its a method'
    - '                            relevant_class = ['
    - '                                clazz'
    - '                                for clazz in classes'
    - '                                if clazz["file"] == pred_file'
    - '                                and clazz["name"] == current_class_name'
    - '                            ]'
    - '                            relevant_method = ['
    - '                                method'
    - '                                for method in relevant_class[0]["methods"]'
    - '                                if method["name"] == loc'
    - '                            ]'
    - '                            if len(relevant_method) == 0:'
    - '                                print(f"{loc} method could not be found")'
    - '                                # print([method for method in relevant_class[0][''methods'']])'
    - '                                #'
    - '                                # for file_content in files:'
    - '                                #     if file_content[0] == pred_file:'
    - '                                #         print("\n".join(file_content[1]))'
    - '                                #         exit()'
    - '                            else:'
    - '                                line_loc.append('
    - '                                    ('
    - '                                        relevant_method[0]["start_line"],'
    - '                                        relevant_method[0]["end_line"],'
    - '                                    )'
    - '                                )'
    - '                        else:'
    - '                            # look for it in any class'
    - '                            relevant_method = []'
    - '                            for clazz in classes:'
    - '                                if clazz["file"] == pred_file:'
    - '                                    relevant_method.extend('
    - '                                        ['
    - '                                            method'
    - '                                            for method in clazz["methods"]'
    - '                                            if method["name"] == loc'
    - '                                        ]'
    - '                                    )'
    - ''
    - '                            if len(relevant_method) == 1:'
    - '                                line_loc.append('
    - '                                    ('
    - '                                        relevant_method[0]["start_line"],'
    - '                                        relevant_method[0]["end_line"],'
    - '                                    )'
    - '                                )'
    - '                    else:'
    - '                        line_loc.append('
    - '                            ('
    - '                                relevant_function[0]["start_line"],'
    - '                                relevant_function[0]["end_line"],'
    - '                            )'
    - '                        )'
    - '            elif loc.startswith("line: "):'
    - '                if remove_line:'
    - '                    # TODO: can recover the corresponding function instead
      of throwing it away'
    - '                    continue'
    - '                loc = loc[len("line: ") :].strip().split()[0]'
    - '                try:'
    - '                    # line_loc.append(int(loc))'
    - '                    line_loc.append((int(loc), int(loc)))'
    - '                except:'
    - '                    continue'
    - '            elif loc.startswith("variable:"):'
    - '                vars = loc[len("variable:") :].strip().split()'
    - '                for v in vars:'
    - '                    if v in global_vars:'
    - '                        line_loc.append('
    - '                            (global_vars[v]["start_line"], global_vars[v]["end_line"])'
    - '                        )'
    - '            else:'
    - '                if loc.strip():'
    - '                    print(f"loc {loc} not recognised")'
    - '                # assert False'
    - ''
    - '    # Fine-grained-only loc: Remove intervals that are supersets of another.'
    - '    if fine_grain_only:'
    - '        filtered_line_loc = []'
    - '        for st, en in line_loc:'
    - '            if filtered_line_loc:'
    - '                last_st, last_en = filtered_line_loc[-1]'
    - '                # If the current interval is a more fine-grained loc, remove
      the superset.'
    - '                if last_st <= st and en <= last_en:'
    - '                    filtered_line_loc.pop()'
    - '            filtered_line_loc.append((st, en))'
    - '        line_loc = filtered_line_loc'
    - ''
    - '    # compute max min'
    - '    # TODO: think of strategies to do bunched up lines'
    - '    # TODO: e.g., we can have multiple code segments (right now, its just one)'
    - ''
    - '    for file_content in files:'
    - '        if file_content[0] == pred_file:'
    - '            content = file_content[1]'
    - '            break'
    - ''
    - '    if len(line_loc) == 0:'
    - '        return [], []'
    - ''
    - '    # max_line = min(max(line_loc) + context_window, len(content))'
    - '    # min_line = max(min(line_loc) - context_window, 0)'
    - '    #'
    - '    # return line_loc, max_line, min_line'
    - ''
    - '    # compute overlapping locations instead'
    - '    if loc_interval:'
    - '        contextual_line_loc = []'
    - '        for loc in line_loc:'
    - '            max_line = min(loc[1] + context_window, len(content))'
    - '            min_line = max(loc[0] - context_window, 0)'
    - '            contextual_line_loc.append((min_line, max_line))'
    - ''
    - '        return line_loc, merge_intervals(contextual_line_loc)'
    - '    else:'
    - '        # defaulting to max min'
    - '        max_line = min(max([loc[1] for loc in line_loc]) + context_window,
      len(content))'
    - '        min_line = max(min([loc[0] for loc in line_loc]) - context_window,
      0)'
    - ''
    - '        return line_loc, [(min_line, max_line)]'
    - ''
    - ''
    - 'def compile_gt_locations(gt_location: dict) -> tuple[list, set, set, set]:'
    - '    """mostly serves a way to check what are the gt locations in gt patch"""'
    - '    edits = gt_location["edits"]'
    - ''
    - '    lines, classes, methods, functions = [], set(), set(), set()'
    - ''
    - '    adds = set()'
    - ''
    - '    for edit in edits:'
    - '        for clazz in edit["class_names"]:'
    - '            classes.add(clazz)'
    - ''
    - '        for method in edit["method_names"]:'
    - '            methods.add(method)'
    - ''
    - '        for function in edit["function_names"]:'
    - '            functions.add(function)'
    - ''
    - '        if edit["type"] == "add":'
    - '            adds.add(edit["line"])'
    - '        else:'
    - '            lines.append(edit["line"])'
    - ''
    - '    # handle the added lines'
    - '    add_intervals = [(i, i + 1) for i in adds]'
    - '    add_intervals = merge_intervals(add_intervals)'
    - '    for st, en in add_intervals:'
    - '        lines.append(st)'
    - '    lines = list(set(lines))'
    - ''
    - '    # sort the lines'
    - '    lines = sorted(lines)'
    - ''
    - '    return lines, classes, methods, functions'
    - ''
    - ''
    - 'def show_project_structure(structure, spacing=0) -> str:'
    - '    """pprint the project structure"""'
    - ''
    - '    pp_string = ""'
    - ''
    - '    for key, value in structure.items():'
    - '        if "." in key and ".py" not in key:'
    - '            continue  # skip none python files'
    - '        if "." in key:'
    - '            pp_string += " " * spacing + str(key) + "\n"'
    - '        else:'
    - '            pp_string += " " * spacing + str(key) + "/" + "\n"'
    - '        if "classes" not in value:'
    - '            pp_string += show_project_structure(value, spacing + 4)'
    - ''
    - '    return pp_string'
    - ''
    - ''
    - 'def filter_out_test_files(structure):'
    - '    """filter out test files from the project structure"""'
    - '    for key, value in list(structure.items()):'
    - '        if key.startswith("test"):'
    - '            del structure[key]'
    - '        elif isinstance(value, dict):'
    - '            filter_out_test_files(value)'
    - ''
    - ''
    - 'def filter_none_python(structure):'
    - '    for key, value in list(structure.items()):'
    - '        if ('
    - '            not "functions" in value.keys()'
    - '            and not "classes" in value.keys()'
    - '            and not "text" in value.keys()'
    - '        ) or not len(value.keys()) == 3:'
    - '            filter_none_python(value)'
    - ''
    - '            if structure[key] == {}:'
    - '                del structure[key]'
    - '        else:'
    - '            if not key.endswith(".py"):'
    - '                del structure[key]'
    - ''
    - ''
    - 'def filter_proposed_files(proposed_files, repo_structure):'
    - '    """'
    - '    Filter proposed files against a given repository structure.'
    - ''
    - '    Arguments:'
    - '    proposed_files -- list of proposed files with instance IDs'
    - '    repo_structure -- list of repository structures with instance IDs'
    - ''
    - '    Returns:'
    - '    A list of dictionaries with instance IDs and valid files matching the repository
      structure.'
    - '    """'
    - '    instance_to_files = {'
    - '        entry["instance_id"]: entry["files"] for entry in proposed_files'
    - '    }'
    - '    instance_to_structure = {'
    - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
    - '    }'
    - '    filtered_files = []'
    - '    for instance_id, files in instance_to_files.items():'
    - '        if instance_id in instance_to_structure:'
    - '            repo_files, _, _ = get_full_file_paths_and_classes_and_functions('
    - '                instance_to_structure[instance_id]'
    - '            )'
    - '            repo_files_set = set(repo_files)'
    - '            valid_files = []'
    - '            for repo_file in repo_files_set:'
    - '                for proposed_file in files:'
    - '                    if proposed_file == repo_file.split("/")[-1]:'
    - '                        valid_files.append(repo_file)'
    - '            if valid_files:'
    - '                filtered_files.append('
    - '                    {"instance_id": instance_id, "files": valid_files}'
    - '                )'
    - '    return filtered_files'
    - ''
    - ''
    - 'def filter_proposed_classes(proposed_classes, repo_structure):'
    - '    """'
    - '    Filter proposed classes against a given repository structure.'
    - ''
    - '    Arguments:'
    - '    proposed_classes -- list of proposed classes with instance IDs'
    - '    repo_structure -- list of repository structures with instance IDs'
    - ''
    - '    Returns:'
    - '    A list of dictionaries with instance IDs and valid classes matching the
      repository structure.'
    - '    """'
    - '    instance_to_classes = {'
    - '        entry["instance_id"]: entry["classes"] for entry in proposed_classes'
    - '    }'
    - '    instance_to_structure = {'
    - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
    - '    }'
    - '    filtered_classes = []'
    - '    for instance_id, classes in instance_to_classes.items():'
    - '        if instance_id in instance_to_structure:'
    - '            _, repo_classes, _ = get_full_file_paths_and_classes_and_functions('
    - '                instance_to_structure[instance_id]'
    - '            )'
    - '            repo_classes_set = {clazz["name"]: clazz["file"] for clazz in repo_classes}'
    - '            valid_classes = []'
    - '            for proposed_class in classes:'
    - '                if proposed_class in repo_classes_set:'
    - '                    valid_classes.append('
    - '                        {'
    - '                            "name": proposed_class,'
    - '                            "file": repo_classes_set[proposed_class],'
    - '                        }'
    - '                    )'
    - '            if valid_classes:'
    - '                filtered_classes.append('
    - '                    {"instance_id": instance_id, "classes": valid_classes}'
    - '                )'
    - '    return filtered_classes'
    - ''
    - ''
    - 'def filter_proposed_methods(proposed_methods, repo_structure):'
    - '    """'
    - '    Filter proposed methods against a given repository structure.'
    - ''
    - '    Arguments:'
    - '    proposed_methods -- list of proposed methods with instance IDs'
    - '    repo_structure -- list of repository structures with instance IDs'
    - ''
    - '    Returns:'
    - '    A list of dictionaries with instance IDs and valid methods matching the
      repository structure.'
    - '    """'
    - '    instance_to_methods = {'
    - '        entry["instance_id"]: entry["methods"] for entry in proposed_methods'
    - '    }'
    - '    instance_to_structure = {'
    - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
    - '    }'
    - '    filtered_methods = []'
    - '    for instance_id, methods in instance_to_methods.items():'
    - '        if instance_id in instance_to_structure:'
    - '            _, repo_classes, _ = get_full_file_paths_and_classes_and_functions('
    - '                instance_to_structure[instance_id]'
    - '            )'
    - '            valid_methods = []'
    - '            for repo_class in repo_classes:'
    - '                for method in methods:'
    - '                    if method in repo_class["methods"]:'
    - '                        valid_methods.append('
    - '                            {'
    - '                                "class": repo_class["name"],'
    - '                                "method": method,'
    - '                                "file": repo_class["file"],'
    - '                            }'
    - '                        )'
    - '            if valid_methods:'
    - '                filtered_methods.append('
    - '                    {"instance_id": instance_id, "methods": valid_methods}'
    - '                )'
    - '    return filtered_methods'
    - ''
    - ''
    - 'def filter_proposed_functions(proposed_functions, repo_structure):'
    - '    """'
    - '    Filter proposed functions against a given repository structure.'
    - ''
    - '    Arguments:'
    - '    proposed_functions -- list of proposed functions with instance IDs'
    - '    repo_structure -- list of repository structures with instance IDs'
    - ''
    - '    Returns:'
    - '    A list of dictionaries with instance IDs and valid functions matching the
      repository structure.'
    - '    """'
    - '    instance_to_functions = {'
    - '        entry["instance_id"]: entry["functions"] for entry in proposed_functions'
    - '    }'
    - '    instance_to_structure = {'
    - '        entry["instance_id"]: entry["structure"] for entry in repo_structure'
    - '    }'
    - '    filtered_functions = []'
    - '    for instance_id, functions in instance_to_functions.items():'
    - '        if instance_id in instance_to_structure:'
    - '            _, _, repo_functions = get_full_file_paths_and_classes_and_functions('
    - '                instance_to_structure[instance_id]'
    - '            )'
    - '            valid_functions = []'
    - '            for repo_function in repo_functions:'
    - '                for function in functions:'
    - '                    if isinstance('
    - '                        repo_function["name"], dict'
    - '                    ):  # Why are there cases where this is not a dict?'
    - '                        if function == repo_function["name"].get("name", []):'
    - '                            valid_functions.append('
    - '                                {"function": function, "file": repo_function["file"]}'
    - '                            )'
    - '            if valid_functions:'
    - '                filtered_functions.append('
    - '                    {"instance_id": instance_id, "functions": valid_functions}'
    - '                )'
    - '    return filtered_functions'
    - ''
    - ''
    - 'def get_full_file_paths_and_classes_and_functions(structure, current_path=""):'
    - '    """'
    - '    Recursively retrieve all file paths, classes, and functions within a directory
      structure.'
    - ''
    - '    Arguments:'
    - '    structure -- a dictionary representing the directory structure'
    - '    current_path -- the path accumulated so far, used during recursion (default="")'
    - ''
    - '    Returns:'
    - '    A tuple containing:'
    - '    - files: list of full file paths'
    - '    - classes: list of class details with file paths'
    - '    - functions: list of function details with file paths'
    - '    """'
    - '    files = []'
    - '    classes = []'
    - '    functions = []'
    - '    for name, content in structure.items():'
    - '        if isinstance(content, dict):'
    - '            if ('
    - '                not "functions" in content.keys()'
    - '                and not "classes" in content.keys()'
    - '                and not "text" in content.keys()'
    - '            ) or not len(content.keys()) == 3:'
    - '                # or guards against case where functions and classes are somehow
      part of the structure.'
    - '                next_path = f"{current_path}/{name}" if current_path else name'
    - '                ('
    - '                    sub_files,'
    - '                    sub_classes,'
    - '                    sub_functions,'
    - '                ) = get_full_file_paths_and_classes_and_functions(content,
      next_path)'
    - '                files.extend(sub_files)'
    - '                classes.extend(sub_classes)'
    - '                functions.extend(sub_functions)'
    - '            else:'
    - '                next_path = f"{current_path}/{name}" if current_path else name'
    - '                files.append((next_path, content["text"]))'
    - '                if "classes" in content:'
    - '                    for clazz in content["classes"]:'
    - '                        classes.append('
    - '                            {'
    - '                                "file": next_path,'
    - '                                "name": clazz["name"],'
    - '                                "start_line": clazz["start_line"],'
    - '                                "end_line": clazz["end_line"],'
    - '                                "methods": ['
    - '                                    {'
    - '                                        "name": method["name"],'
    - '                                        "start_line": method["start_line"],'
    - '                                        "end_line": method["end_line"],'
    - '                                    }'
    - '                                    for method in clazz.get("methods", [])'
    - '                                ],'
    - '                            }'
    - '                        )'
    - '                if "functions" in content:'
    - '                    for function in content["functions"]:'
    - '                        function["file"] = next_path'
    - '                        functions.append(function)'
    - '        else:'
    - '            next_path = f"{current_path}/{name}" if current_path else name'
    - '            files.append(next_path)'
    - '    return files, classes, functions'
    - ''
    - ''
    - PROJECT_FILE_LOC = os.environ.get("PROJECT_FILE_LOC", None)
    - ''
    - ''
    - 'def get_repo_structure(instance_id: str, repo_name, base_commit, playground):'
    - ''
    - '    if PROJECT_FILE_LOC is not None:'
    - '        with open(PROJECT_FILE_LOC + "/" + instance_id + ".json") as f:'
    - '            d = json.load(f)'
    - '        repo_structure = d["structure"]'
    - '    else:'
    - '        d = get_project_structure_from_scratch('
    - '            repo_name, base_commit, instance_id, playground'
    - '        )'
    - '        repo_structure = d["structure"]'
    - ''
    - '    return repo_structure'
    - ''
    - ''
    - 'def get_repo_files(structure, filepaths: list[str]):'
    - '    files, classes, functions = get_full_file_paths_and_classes_and_functions(structure)'
    - '    file_contents = dict()'
    - '    for filepath in filepaths:'
    - '        content = None'
    - ''
    - '        for file_content in files:'
    - '            if file_content[0] == filepath:'
    - '                content = "\n".join(file_content[1])'
    - '                file_contents[filepath] = content'
    - '                break'
    - ''
    - '        assert content is not None, "file not found"'
    - '    return file_contents'
    - ''
    - ''
    - 'def test_merge():'
    - '    # Example usage:'
    - '    input_tuples = [(1, 3), (2, 4), (5, 7), (6, 8)]'
    - '    merged_tuples = merge_intervals(input_tuples)'
    - '    assert merged_tuples == [(1, 4), (5, 8)]'
    - ''
    - '    input_tuples = [(1, 5), (2, 3)]'
    - '    merged_tuples = merge_intervals(input_tuples)'
    - '    assert merged_tuples == [(1, 5)]'
    - ''
    - '    input_tuples = [(1, 1)]'
    - '    merged_tuples = merge_intervals(input_tuples)'
    - '    assert merged_tuples == [(1, 1)]'
    - ''
    - '    input_tuples = [(1, 1), (2, 3)]'
    - '    merged_tuples = merge_intervals(input_tuples)'
    - '    assert merged_tuples == [(1, 1), (2, 3)]'
    - ''
    - ''
    - 'def test_interval_display():'
    - ''
    - '    content = """'
    - one
    - two
    - three
    - four
    - five
    - six
    - seven
    - eight
    - '""".strip()'
    - ''
    - '    x = line_wrap_content(content, [])'
    - '    print(x)'
    - ''
    - '    print("============")'
    - ''
    - '    x = line_wrap_content(content, [(1, 2), (4, 6), (7, 8)])'
    - '    print(x)'
    - ''
    - ''
    - 'if __name__ == "__main__":'
    - '    test_merge()'
    - '    # test_interval_display()'
  utils.py:
    classes: []
    functions:
    - end_line: 17
      name: load_jsonl
      start_line: 6
      text:
      - 'def load_jsonl(filepath):'
      - '    """'
      - '    Load a JSONL file from the given filepath.'
      - ''
      - '    Arguments:'
      - '    filepath -- the path to the JSONL file to load'
      - ''
      - '    Returns:'
      - '    A list of dictionaries representing the data in each line of the JSONL
        file.'
      - '    """'
      - '    with open(filepath, "r") as file:'
      - '        return [json.loads(line) for line in file]'
    - end_line: 30
      name: write_jsonl
      start_line: 20
      text:
      - 'def write_jsonl(data, filepath):'
      - '    """'
      - '    Write data to a JSONL file at the given filepath.'
      - ''
      - '    Arguments:'
      - '    data -- a list of dictionaries to write to the JSONL file'
      - '    filepath -- the path to the JSONL file to write'
      - '    """'
      - '    with open(filepath, "w") as file:'
      - '        for entry in data:'
      - '            file.write(json.dumps(entry) + "\n")'
    - end_line: 34
      name: load_json
      start_line: 33
      text:
      - 'def load_json(filepath):'
      - '    return json.load(open(filepath, "r"))'
    - end_line: 59
      name: combine_by_instance_id
      start_line: 37
      text:
      - 'def combine_by_instance_id(data):'
      - '    """'
      - '    Combine data entries by their instance ID.'
      - ''
      - '    Arguments:'
      - '    data -- a list of dictionaries with instance IDs and other information'
      - ''
      - '    Returns:'
      - '    A list of combined dictionaries by instance ID with all associated data.'
      - '    """'
      - '    combined_data = defaultdict(lambda: defaultdict(list))'
      - '    for item in data:'
      - '        instance_id = item.get("instance_id")'
      - '        if not instance_id:'
      - '            continue'
      - '        for key, value in item.items():'
      - '            if key != "instance_id":'
      - '                combined_data[instance_id][key].extend('
      - '                    value if isinstance(value, list) else [value]'
      - '                )'
      - '    return ['
      - '        {**{"instance_id": iid}, **details} for iid, details in combined_data.items()'
      - '    ]'
    text:
    - import json
    - ''
    - import pandas as pd
    - ''
    - ''
    - 'def load_jsonl(filepath):'
    - '    """'
    - '    Load a JSONL file from the given filepath.'
    - ''
    - '    Arguments:'
    - '    filepath -- the path to the JSONL file to load'
    - ''
    - '    Returns:'
    - '    A list of dictionaries representing the data in each line of the JSONL
      file.'
    - '    """'
    - '    with open(filepath, "r") as file:'
    - '        return [json.loads(line) for line in file]'
    - ''
    - ''
    - 'def write_jsonl(data, filepath):'
    - '    """'
    - '    Write data to a JSONL file at the given filepath.'
    - ''
    - '    Arguments:'
    - '    data -- a list of dictionaries to write to the JSONL file'
    - '    filepath -- the path to the JSONL file to write'
    - '    """'
    - '    with open(filepath, "w") as file:'
    - '        for entry in data:'
    - '            file.write(json.dumps(entry) + "\n")'
    - ''
    - ''
    - 'def load_json(filepath):'
    - '    return json.load(open(filepath, "r"))'
    - ''
    - ''
    - 'def combine_by_instance_id(data):'
    - '    """'
    - '    Combine data entries by their instance ID.'
    - ''
    - '    Arguments:'
    - '    data -- a list of dictionaries with instance IDs and other information'
    - ''
    - '    Returns:'
    - '    A list of combined dictionaries by instance ID with all associated data.'
    - '    """'
    - '    combined_data = defaultdict(lambda: defaultdict(list))'
    - '    for item in data:'
    - '        instance_id = item.get("instance_id")'
    - '        if not instance_id:'
    - '            continue'
    - '        for key, value in item.items():'
    - '            if key != "instance_id":'
    - '                combined_data[instance_id][key].extend('
    - '                    value if isinstance(value, list) else [value]'
    - '                )'
    - '    return ['
    - '        {**{"instance_id": iid}, **details} for iid, details in combined_data.items()'
    - '    ]'
